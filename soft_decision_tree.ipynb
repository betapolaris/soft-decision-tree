{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "# Function to set random seed\n",
    "def set_random_seed(seed=42):\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (63000,) (7000, 784) (7000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(data_train, labels_train), (data_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten the dataset\n",
    "data_train = data_train.reshape(len(data_train), -1)\n",
    "data_test = data_test.reshape(len(data_test), -1)\n",
    "\n",
    "# Combine the dataset\n",
    "data = np.r_[data_train, data_test]\n",
    "labels = np.r_[labels_train, labels_test]\n",
    "\n",
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Create one-hot labels\n",
    "binarizer = LabelBinarizer()\n",
    "labels_one_hot = binarizer.fit_transform(labels)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_indices = np.random.permutation(len(data))\n",
    "data = data[shuffled_indices]\n",
    "labels = labels[shuffled_indices]\n",
    "labels_one_hot = labels_one_hot[shuffled_indices]\n",
    "\n",
    "# Split the dataset\n",
    "train_ratio = 0.9\n",
    "split_index = int(train_ratio * len(data))\n",
    "data_train = data[:split_index]\n",
    "labels_train = labels[:split_index]\n",
    "data_test = data[split_index:]\n",
    "labels_test = labels[split_index:]\n",
    "labels_train_one_hot = labels_one_hot[:split_index]\n",
    "labels_test_one_hot = labels_one_hot[split_index:]\n",
    "\n",
    "print(data_train.shape, labels_train.shape, data_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to fetch a portion of the dataset(\n",
    "def fetch_batch(X, y, batch_size):\n",
    "    shuffled_indices = np.random.permutation(len(X))\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[shuffled_indices[i:i+batch_size]]\n",
    "        y_batch = y[shuffled_indices[i:i+batch_size]]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to display one MNIST data\n",
    "def plot_digit(x):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    x = scaler.inverse_transform(x)\n",
    "    x = x.reshape(28, 28)\n",
    "    plt.imshow(x, cmap=matplotlib.cm.binary)\n",
    "    plt.show()\n",
    "    \n",
    "plot_digit(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic function\n",
    "def sigmoid(X):\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "# The softmax function\n",
    "def softmax(X, axis=-1):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5:\n",
    "    def __init__(self, learning_rate=0.1, momentum=0.5, dropout_rate=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        height = 28\n",
    "        width = 28\n",
    "        channels = 1\n",
    "        input_size = height * width * channels\n",
    "        output_size = 10\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            X_reshaped = tf.reshape(self.X_ph, shape=(-1, height, width, channels))\n",
    "            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "            self.train_ph = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "            \n",
    "            # First convolutional layer\n",
    "            X = tf.layers.conv2d(X_reshaped, filters=6, kernel_size=5, strides=1, \n",
    "                                 padding='SAME', activation=tf.nn.tanh)\n",
    "            \n",
    "            # First pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            X = tf.layers.conv2d(X, filters=16, kernel_size=5, strides=1,\n",
    "                                 padding='VALID', activation=tf.nn.tanh)\n",
    "            \n",
    "            # Second pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            X = tf.layers.conv2d(X, filters=120, kernel_size=5, strides=1,\n",
    "                                 padding='VALID', activation=tf.nn.tanh)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            X = tf.reshape(X, shape=(-1, 120))\n",
    "            X = tf.layers.dropout(X, rate=self.dropout_rate, training=self.train_ph)\n",
    "            X = tf.layers.dense(X, units=84, activation=tf.nn.tanh)\n",
    "            X = tf.layers.dropout(X, rate=self.dropout_rate, training=self.train_ph)\n",
    "            self.logits = tf.layers.dense(X, units=output_size)\n",
    "            \n",
    "            # Probabilities for each class\n",
    "            self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "            \n",
    "            # Use mean cross entropy as the loss function\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_ph)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            # Make optimizer and train op\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1, show_progress=False):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, \n",
    "                                                        self.y_ph: y_batch, \n",
    "                                                        self.train_ph: True})\n",
    "                \n",
    "            if show_progress:\n",
    "                prediction = self.predict(X[:1000])\n",
    "                accuracy = sum(prediction == y[:1000]) / 1000\n",
    "                print(\"Epoch: %d \\t Train accuracy: %.3f\" % (epoch, accuracy))\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return np.argmax(logits, axis=1)\n",
    "        \n",
    "    def get_probs(self, X):\n",
    "        probs = self.sess.run(self.probs, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return probs\n",
    "    \n",
    "    def reset_session(self):\n",
    "        self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build graph: 0.419 s\n",
      "Epoch: 0 \t Train accuracy: 0.955\n",
      "Epoch: 1 \t Train accuracy: 0.974\n",
      "Epoch: 2 \t Train accuracy: 0.979\n",
      "Epoch: 3 \t Train accuracy: 0.979\n",
      "Epoch: 4 \t Train accuracy: 0.982\n",
      "Epoch: 5 \t Train accuracy: 0.991\n",
      "Epoch: 6 \t Train accuracy: 0.987\n",
      "Epoch: 7 \t Train accuracy: 0.991\n",
      "Epoch: 8 \t Train accuracy: 0.985\n",
      "Epoch: 9 \t Train accuracy: 0.992\n",
      "Epoch: 10 \t Train accuracy: 0.987\n",
      "Epoch: 11 \t Train accuracy: 0.994\n",
      "Epoch: 12 \t Train accuracy: 0.990\n",
      "Epoch: 13 \t Train accuracy: 0.991\n",
      "Epoch: 14 \t Train accuracy: 0.995\n",
      "Epoch: 15 \t Train accuracy: 0.992\n",
      "Epoch: 16 \t Train accuracy: 0.994\n",
      "Epoch: 17 \t Train accuracy: 0.991\n",
      "Epoch: 18 \t Train accuracy: 0.994\n",
      "Epoch: 19 \t Train accuracy: 0.995\n",
      "Epoch: 20 \t Train accuracy: 0.995\n",
      "Epoch: 21 \t Train accuracy: 0.994\n",
      "Epoch: 22 \t Train accuracy: 0.996\n",
      "Epoch: 23 \t Train accuracy: 0.993\n",
      "Epoch: 24 \t Train accuracy: 0.996\n",
      "Time to train model: 408.161 s\n",
      "Test accuracy: 0.9897142857142858\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "lenet5 = LeNet5(learning_rate=0.1, momentum=0.5, dropout_rate=0.5)\n",
    "\n",
    "t = time.time()\n",
    "lenet5.build_graph()\n",
    "print(\"Time to build graph: %.3f s\" % (time.time() - t))\n",
    "\n",
    "t = time.time()\n",
    "lenet5.train(data_train, labels_train, n_epochs=25, show_progress=True)\n",
    "print(\"Time to train model: %.3f s\" % (time.time() - t))\n",
    "\n",
    "prediction = lenet5.predict(data_test)\n",
    "\n",
    "print(\"Test accuracy:\", sum(prediction==labels_test) / len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAADJdJREFUeJzt3WtsVGUaB/D/Iy4fKJeo1W5DgRohG5TE7jpBIuvGjbIiQgpGjYRsqjGoRBJW+QCaGAkJBm/rEjVGxabdcBEvu0gIWUoaAkuCyEiUy5oVIV1saFrqjaIkBHj2Q88k0HnO05k5Z679/xLTzsPbc94R/j0z75zzHFFVEJHtimJPgKiUMSBEDgaEyMGAEDkYECIHA0LkYECIHAwIkYMBIXJcGeWHRWQmgDUAhgFYq6qrvfHV1dVaX18fZZdEsejo6EBvb68MNi7ngIjIMABvApgBoBPAfhHZoqr/CfuZ+vp6JJPJXHdJFJtEIpHRuCgvsaYC+EZVj6vqOQDvA2iMsD2ikhMlIGMBfHvJ486gdhkReUxEkiKSPHXqVITdERVelIBYr9/STg1W1XdUNaGqiWuvvTbC7ogKL0pAOgGMu+RxHYCT0aZDVFqiBGQ/gEkicr2IDAfwEIAt8UyLqDTkvIqlqudFZDGA7ehf5m1W1SOxzYyoBET6HERVtwHYFtNciEoOP0kncjAgRA4GhMjBgBA5GBAiBwNC5GBAiBwMCJGDASFyMCBEDgaEyMGAEDkYECIHA0LkYECIHAwIkYMBIXIwIEQOBoTIEemadArX0tKSVuvq6srrPnfs2GHWZ8yYkdV4S11dnVn/9NNPzfojjzyS8bYBYNSoUWZ98eLFWW0nblGbV3cA6ANwAcB5Vc2s4SlRmYjjCPJHVe2NYTtEJYfvQYgcUQOiANpE5HMRecwawObVVM6iBmS6qv4OwD0AnhSRPwwcwObVVM5ENa0he24bElkB4IyqvhI2JpFIaKnfQOfgwYNmfffu3WZ9zZo1Zv3EiRNptfPnz+c+sQjC/o5FBr3BUsFccYX9u7qqqirjbfz4448Zj00kEkgmk4P+D8j5CCIiVSIyKvU9gD8BOJzr9ohKUZRVrBoA/wx+C10JYIOq/iuWWRGViCjd3Y8DuDnGuRCVHC7zEjkYECLHkD0X69ChQ2b97rvvNus9PT2R9xm2zD1r1iyzvmfPHrN+7Ngxs37LLbeY9WxWDidOnGjWwz7D+umnnzLetmf69OlmffLkyWm15557LpZ9ZoJHECIHA0LkYECIHAwIkYMBIXIM2VWsu+66y6z39sZzacttt92WVlu3bp05dsKECWY97ArE06dPm/Xq6mqzns1zGj16tFnfunWrWX/iiScy3jYAvP7662Z9wYIFZn3MmDFZbT9uPIIQORgQIgcDQuRgQIgcDAiRY8iuYt18s32m/q5du8x6tlcDHj6cfu3Y9u3bzbHz5s0z67W1tVnVw1xzzTUZjw1bOWttbc1qn2HGjh1r1ou9WhWGRxAiBwNC5GBAiBwMCJGDASFyDLqKJSLNAGYD6FHVKUHtagCbANQD6ADwoKr+kL9pxq+trc2sv/jii2b9vffeM+thV/dZ50stWrTIHPvRRx+Z9ZdeesmsNzQ0mPVsdXd3p9Xefvttc+zevXuz2rZ1JSBgn6NWyjI5grQAmDmgthxAu6pOAtAePCaqOIMGRFV3A/h+QLkRQGphvBXA3JjnRVQScn0PUqOqXQAQfL0ubCCbV1M5y/ubdDavpnKW66km3SJSq6pdIlILIHpPnBKxbNkysz5//nyz3tTUZNY7OjrSalZDawBob28367fffrtZnzNnjlkPuxgprHl1Y2NjWm3//v3m2DAjR44060uXLjXr5fZLMtcjyBYAqX8ZTQA+iWc6RKVl0ICIyEYAewH8RkQ6ReRRAKsBzBCRowBmBI+JKs6gL7FU1X5tAdwZ81yISg4/SSdyMCBEjiF7wVS2xo8fb9Z37txp1q0Lj1paWsyxL7zwgln/5ZdfzPqmTZvM+tGjR836xYsXzfoXX3xh1i0jRoww62G3oHv44Ycz3nYp4xGEyMGAEDkYECIHA0LkYECIHFzFyhOrNc8zzzxjjg27AGrlypVm/bPPPjPrBw4cMOth52IFt/C+zPDhw82xc+faVzRUympVGB5BiBwMCJGDASFyMCBEDgaEyMFVrBLw3XffmfWzZ88WeCbhV042NzcXeCalgUcQIgcDQuRgQIgcDAiRgwEhcuTavHoFgIUAUq0Sn1XVbfmaZDmy+mJt3LjRHLthwwazfuTIkVjmEnYuluXLL78062Erbdnc3q0c5dq8GgBeU9WG4D+GgypSrs2riYaEKO9BFovIQRFpFpGrwgaxeTWVs1wD8haAGwA0AOgC8GrYQDavpnKWU0BUtVtVL6jqRQDvApga77SISkNO52KlOrsHD+cBOBzflMrL8ePHzbrVL+qNN97IatvWFX+esNubXbhwwazv27cvrRbWK2v27NlmffPmzWa9pqbGrJebTJZ5NwK4A0C1iHQCeB7AHSLSAEDRf4/Cx/M4R6KiybV5tX1HS6IKw0/SiRwMCJGDASFy8IrCDB07dsysh63ufP3115H3WVVVZdYXLFhg1l9++WWzHnYuVl1dXVrtzJkz5tiwXlxh912slFUsHkGIHAwIkYMBIXIwIEQOvkkfYN26dWY9rPH0yZMnI+/z1ltvNetPP/20Wb///vsj7xMAbrrpprSadfrJUMYjCJGDASFyMCBEDgaEyMGAEDm4ijXA6tWrzXq2q1VWO5yw01JefdW+Yvmqq0Iv9Y/FtGnT0mpcxbocjyBEDgaEyMGAEDkYECIHA0LkyKSryTgAfwfwawAXAbyjqmtE5GoAmwDUo7+zyYOq+kP+phqvsFWpsAuAsmWtWBXrNmZhFzutXbu2wDMpP5kcQc4DWKqqkwFMA/CkiNwIYDmAdlWdBKA9eExUUTJpXt2lqgeC7/sAfAVgLIBGAK3BsFYAc/M1SaJiyeo9iIjUA/gtgH0AalLdFYOv14X8DJtXU9nKOCAiMhLAxwD+oqqnM/05Nq+mcpZRQETkV+gPx3pV/UdQ7haR2uDPawH05GeKRMWTySqWoL/V6Feq+tdL/mgLgCYAq4Ovn+RlhnnS2tpq1n/++edYtr9o0aKMx3Z2dpr1vr6+rPbZ1tZm1p966imznk1z7Dlz5pj1iRMnZryNcpTJyYrTAfwZwCERSbX+fhb9wfhARB4FcALAA/mZIlHxZNK8eg+AsF81d8Y7HaLSwk/SiRwMCJGDASFyDNkrCsNWZVatWmXWz549m9X2V65cmVazrjIEgF27dpn1uM4Ly8a9995r1sNW/caMGZPP6RQdjyBEDgaEyMGAEDkYECIHA0LkGLKrWFOmTDHr9913n1lfv359Vtvftm1b1nPKl9GjR5v1FStWpNUWLlxojg27HVyl4xGEyMGAEDkYECIHA0LkYECIHEN2FSvMkiVLzPq5c+fM+ocffpjP6ZgaGxvNeti9DpctW5bP6VQ0HkGIHAwIkYMBIXIwIEQOUVV/QHjz6hUAFgJItUt8VlXd8ysSiYQmk8nIkyaKKpFIIJlMDtr3KJNVrFTz6gMiMgrA5yKyI/iz11T1lSgTJSplmbT96QKQ6sHbJyKp5tVEFS9K82oAWCwiB0WkWUTMW7KyeTWVsyjNq98CcAOABvQfYcx7GbN5NZWznJtXq2q3ql5Q1YsA3gUwNX/TJCqOQQMS1rw61dk9MA/A4finR1RcUZpXzxeRBgCK/nsUPp6XGRIVUZTm1aVzTSlRnvCTdCIHA0LkYECIHAwIkYMBIXIwIEQOBoTIwYAQORgQIsegVxTGujORUwD+FzysBtBbsJ0XD59naZqgqoOeXl7QgFy2Y5GkqiaKsvMC4vMsb3yJReRgQIgcxQzIO0XcdyHxeZaxor0HISoHfIlF5GBAiBwFD4iIzBSR/4rINyKyvND7z6eg/VGPiBy+pHa1iOwQkaPBV7M9UjkRkXEislNEvhKRIyKyJKhX3HMtaEBEZBiANwHcA+BG9F/XfmMh55BnLQBmDqgtB9CuqpMAtAePy12q2+ZkANMAPBn8PVbccy30EWQqgG9U9biqngPwPgD7bjBlSFV3A/h+QLkRQGvwfSuAuQWdVB6oapeqHgi+7wOQ6rZZcc+10AEZC+DbSx53ovLbmNYE7VtTbVyvK/J8YjWg22bFPddCB8TqjsJ15jJldNusOIUOSCeAcZc8rgNwssBzKLTuVJO94GtPkecTC6vbJirwuRY6IPsBTBKR60VkOICHAGwp8BwKbQuApuD7JgCfFHEusQjrtolKfK6F/iRdRGYB+BuAYQCaVXVVQSeQRyKyEcAd6D/1uxvA8wA2A/gAwHgAJwA8oKoD38iXFRH5PYB/AziE/psqAf3dNveh0p4rTzUhCsdP0okcDAiRgwEhcjAgRA4GhMjBgBA5GBAix/8B5B3B9cAWsmkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAACldJREFUeJzt3V+IXOUZx/HvU9te2HqhRNOgpisSYpZC07gEwVIsosRSiIkoelFzIYmIQgu9ERH0RsxFbfRCgqYNRmz9AzUxF2oroWqEIm5C8U9jUCTRYNy4KDR4I+rTi52lMXvOu7Pzf2a/H5CZefbMnHeQX87Me955TmQmkqp9p98DkAaZAZEKDIhUYECkAgMiFRgQqcCASAUGRCowIFLBd9t5ckSsAx4CzgD+lJlbS9svWbIkx8bG2tml1BFHjhxheno65tuu5YBExBnAw8BVwDHgjYjYm5n/qXvO2NgYk5OTre5S6piJiYmmtmvnI9Za4P3M/CAzvwSeAta38XrSwGknIOcDH53y+Fij9i0RsSUiJiNi8tNPP21jd1LvtROQqs9vc5YGZ+ajmTmRmRPnnntuG7uTeq+dgBwDLjzl8QXAx+0NRxos7QTkDWBFRFwUEd8HbgT2dmZY0mBoeRYrM7+KiDuAvzMzzbszM9/p2MikAdDWeZDMfB54vkNjkQaOZ9KlAgMiFRgQqcCASAUGRCowIFKBAZEKDIhUYECkAgMiFRgQqcCASAUGRCowIFKBAZEKDIhUYECkAgMiFRgQqcCASAXtNq8+ApwEvga+yszmGp5KQ6KtgDT8MjOnO/A60sDxI5ZU0G5AEvhHRByIiC1VG9i8WsOs3YBcnplrgGuA2yPiF6dvYPNqDbN2Oyt+3Lg9ERG7mblmyKudGNigqbvwz9q1ayvrmXMa3bNq1arKbaenq7/Cbdy4scnRzdiwYUNlfc2aNZV1/8GaX8tHkIj4QUScNXsfuBp4u1MDkwZBO0eQpcDuiJh9nb9m5osdGZU0INrp7v4B8NMOjkUaOE7zSgUGRCroxJn0RaHxXavpepXDhw9X1qtmvAB27NjRke2XL19eWX/hhRfm1C655JLKbRcrjyBSgQGRCgyIVGBApAIDIhU4i9Wkupmgq6++urL+4ovNLyqom5Xq1PZHjhyprI+Pj8+pTU1NVW67WNdteQSRCgyIVGBApAIDIhUYEKnAWawm1c3iVK1nAjh48GDTr133W/09e/ZU1l955ZXKet1arzpV68h2795due2WLZUtB0aeRxCpwIBIBQZEKjAgUoEBkQpivnU9EbET+DVwIjN/0qidAzwNjAFHgBsy8/P5djYxMZF1/aXUPU888URl/eabb55TW7lyZeW2Bw4cqKyfeeaZrQ+sjyYmJpicnJz356DNHEEeA9adVrsT2JeZK4B9jcfSyJk3IJn5KvDZaeX1wK7G/V3AtR0elzQQWv0OsjQzjwM0bs+r29Dm1RpmXf+SbvNqDbNWl5pMRcSyzDweEcuAE50clDqrrml21VKTuuUq7777bmW9rjH2qGj1CLIX2NS4vwl4rjPDkQbLvAGJiCeBfwErI+JYRNwCbAWuioj3gKsaj6WRM+9HrMy8qeZPV3Z4LNLA8Uy6VGBApAJ/MLUI1J1/qlpmtNCWQqPOI4hUYECkAgMiFRgQqcCASAXOYi0Cde2DqtZi1V2CbbFems0jiFRgQKQCAyIVGBCpwIBIBc5iLQKPPPJIZb1qFmvdutMb2MwY1vY+7fIIIhUYEKnAgEgFBkQqMCBSwbyzWDXNq+8FNgOzP1W7KzOf79Yg1Zxnn322sl41W1VXX6xrruq02rwaYFtmrm78Zzg0klptXi0tCu18B7kjIt6MiJ0RcXbdRjav1jBrNSDbgYuB1cBx4IG6DW1erWHWUkAycyozv87Mb4AdwNrODksaDC2txZrt7N54uAF4u3ND0nyOHj1aWb/tttsq6wvpdbVkyZKWxjSqmpnmfRK4AlgSEceAe4ArImI1kMxco/DWLo5R6ptWm1f/uQtjkQaOZ9KlAgMiFRgQqcBfFA6h/fv3V9anp6cr63VrscbHx+fUNm7c2PrARpBHEKnAgEgFBkQqMCBSgV/SB1jd6uf77ruvsl63pKSu/vjjj7c2sEXEI4hUYECkAgMiFRgQqcCASAXOYg2w+++/v7J++PDhynrdkpK6nzr746j5eQSRCgyIVGBApAIDIhUYEKmgma4mFwKPAz8CvgEezcyHIuIc4GlgjJnOJjdk5ufdG+ro2rZtW2X9wQcfrKwvpI0PwMsvv1xZX758+YJeZzFq5gjyFfD7zFwFXAbcHhHjwJ3AvsxcAexrPJZGSjPNq49n5sHG/ZPAIeB8YD2wq7HZLuDabg1S6pcFfQeJiDHgZ8DrwNLZ7oqN2/NqnmPzag2tpgMSET8E/gb8LjP/2+zzbF6tYdZUQCLie8yE4y+ZOXsZo6mIWNb4+zLgRHeGKPVPM7NYwUyr0UOZ+cdT/rQX2ARsbdw+15URjpBDhw5V1rdu3VpZr1tbVee6666rrK9atWpBr6P/a2ax4uXAb4C3IuLfjdpdzATjmYi4BfgQuL47Q5T6p5nm1a8Bdf+UXdnZ4UiDxTPpUoEBkQoMiFTgLwq75IsvvphTq5tlOnGieoa8bharbg3V9u3bmxydmuURRCowIFKBAZEKDIhUYECkAmexumT37t1zagvtZ2Wfq/7zCCIVGBCpwIBIBQZEKjAgUoGzWF3y2muvzakt9BqCddavX9/SmLRwHkGkAgMiFRgQqcCASAXtNK++F9gMzLZLvCszn+/WQIfN5s2b59T27NlTuW3dD6buvvvuBdXVec3MYs02rz4YEWcBByLipcbftmXmH7o3PKm/mmn7cxyY7cF7MiJmm1dLI6+d5tUAd0TEmxGxMyLOrnmOzas1tNppXr0duBhYzcwR5oGq59m8WsOs5ebVmTmVmV9n5jfADmBt94Yp9UfLzasjYtns9UGADcDb3RnicLr00kvn1D755JM+jETtaKd59U0RsRpIZq5ReGtXRij1UTvNqz3noZHnmXSpwIBIBQZEKjAgUoEBkQoMiFRgQKQCAyIVGBCpIBbacqatnUV8ChxtPFwCTPds5/3j+xxMP87MeZeX9zQg39pxxGRmTvRl5z3k+xxufsSSCgyIVNDPgDzax333ku9ziPXtO4g0DPyIJRUYEKmg5wGJiHURcTgi3o+IO3u9/25qtD86ERFvn1I7JyJeioj3GreV7ZGGSURcGBH/jIhDEfFORPy2UR+599rTgETEGcDDwDXAODO/ax/v5Ri67DFg3Wm1O4F9mbkC2Nd4POxmu22uAi4Dbm/8fxy599rrI8ha4P3M/CAzvwSeAkbmajCZ+Srw2Wnl9cCuxv1dwLU9HVQXZObxzDzYuH8SmO22OXLvtdcBOR/46JTHxxj9NqZLZ9sjNW7P6/N4Ouq0bpsj9157HZCq7ijOMw+pim6bI6fXATkGXHjK4wuAj3s8hl6biohlMNNsD6i+1sGQqeq2yQi+114H5A1gRURcFBHfB24E9vZ4DL22F9jUuL8JeK6PY+mIum6bjOJ77fWZ9Ij4FfAgcAawMzPv6+kAuigingSuYGbp9xRwD7AHeAZYDnwIXJ+Zp3+RHyoR8XNgP/AWMxdVgplum68zau/VpSZSPc+kSwUGRCowIFKBAZEKDIhUYECkAgMiFfwPtL8h1N1sqNQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained CNN to generate target probabilities\n",
    "cnn_probs = lenet5.get_probs(data_train)\n",
    "\n",
    "# Show some examples to make sure everything's okay\n",
    "for i in range(5):\n",
    "    plot_digit(data_train[i])\n",
    "    print(np.around(cnn_probs[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDecisionTree:\n",
    "    def __init__(self, max_depth=1, learning_rate=0.1, momentum=0.1, inverse_temperature=1.0, reg_fn=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.inverse_temperature = inverse_temperature\n",
    "        if reg_fn: \n",
    "            self.reg_fn = reg_fn\n",
    "        else:\n",
    "            self.reg_fn = lambda d: 0  # no regularization\n",
    "    \n",
    "    def build_node(self, X, y, path_probs, current_depth=1, index=1):\n",
    "        if current_depth == self.max_depth:\n",
    "            # Build a leaf node\n",
    "            self.leaf_logits[index] = tf.Variable(initial_value=np.random.randn(y.shape[1], 1), dtype=tf.float32)\n",
    "            \n",
    "            probs = tf.nn.softmax(self.leaf_logits[index], axis=0)\n",
    "            \n",
    "            cross_entropy = tf.squeeze(tf.matmul(y, tf.log(probs)), axis=1)\n",
    "            weighted_cross_entropy = tf.multiply(path_probs, cross_entropy)\n",
    "            \n",
    "            self.loss = self.loss + tf.reduce_mean(weighted_cross_entropy)\n",
    "        else:\n",
    "            # Build an internal node\n",
    "            self.weights[index] = tf.Variable(initial_value=np.random.randn(X.shape[1], 1), dtype=tf.float32)\n",
    "            self.bias[index] = tf.Variable(initial_value=0.0)\n",
    "            \n",
    "            logits = tf.matmul(X, self.weights[index]) + self.bias[index]\n",
    "            \n",
    "            probs = tf.squeeze(tf.sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = tf.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = tf.multiply(path_probs, probs)\n",
    "            \n",
    "            # Add regularization term (balanced split)\n",
    "            epsilon = tf.constant(1e-8)\n",
    "            reg_term = self.reg_fn(current_depth)\n",
    "            left_ratio = tf.reduce_sum(path_probs_left) / (tf.reduce_sum(path_probs) + epsilon)\n",
    "            right_ratio = tf.reduce_sum(path_probs_right) / (tf.reduce_sum(path_probs) + epsilon)\n",
    "            \n",
    "            self.loss = self.loss + reg_term * (0.5 * tf.log(left_ratio + epsilon) + 0.5 * tf.log(right_ratio + epsilon))\n",
    "            \n",
    "            # Build left and right subtrees\n",
    "            self.build_node(X, y, path_probs_left, current_depth+1, 2*index)\n",
    "            self.build_node(X, y, path_probs_right, current_depth+1, 2*index+1)\n",
    "            \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        input_size = 28 * 28\n",
    "        output_size = 10\n",
    "        \n",
    "        # Create lists for storing parameters\n",
    "        n_nodes = 2 ** (self.max_depth + 1)\n",
    "        self.weights = [0] * n_nodes\n",
    "        self.bias = [0] * n_nodes\n",
    "        self.leaf_logits = [0] * n_nodes\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            self.y_ph = tf.placeholder(shape=(None, output_size), dtype=tf.float32)\n",
    "            \n",
    "            # Initialize the loss function\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Start building from the root node\n",
    "            batch_size = tf.shape(self.X_ph)[0]\n",
    "            self.build_node(self.X_ph, self.y_ph, tf.fill([batch_size], 1.0))\n",
    "            \n",
    "            # Finalize the loss function\n",
    "            self.loss = -self.loss\n",
    "            \n",
    "            # Make optimizer and train op\n",
    "            # optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1, show_progress=False, predict_method=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, self.y_ph: y_batch})\n",
    "                # print(\"Loss\", self.sess.run(self.loss, feed_dict={self.X_ph: X_batch, self.y_ph: y_batch}))\n",
    "                \n",
    "            if show_progress:\n",
    "                prediction = self.predict(X[:1000], method=predict_method)\n",
    "                accuracy = sum(prediction == np.argmax(y[:1000], axis=1)) / 1000\n",
    "                print(\"Epoch: %d \\t Train accuracy: %.3f\" % (epoch, accuracy))\n",
    "                   \n",
    "    def get_logits_local(self, X, current_depth=1, index=1):\n",
    "        # At each internal node, split the samples into two based on the \n",
    "        # computed probability at that node only\n",
    "        # print(\"Logits local\")\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: simply return the logits for every sample\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            logits = logits.ravel()\n",
    "            return [logits for _ in range(len(X))]\n",
    "        else:\n",
    "            # At internal node: split the dataset, get the logits of each, and then combine them\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            # print(\"Max logits\", np.max(logits))\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            mask = np.array(probs < 0.5)\n",
    "            \n",
    "            indices_left = np.nonzero(mask)\n",
    "            indices_right = np.nonzero(np.logical_not(mask))\n",
    "            \n",
    "            logits_left = self.get_logits_local(X[indices_left], current_depth+1, index*2)\n",
    "            logits_right = self.get_logits_local(X[indices_right], current_depth+1, index*2+1)\n",
    "            \n",
    "            logits = []\n",
    "            it_left, it_right = 0, 0\n",
    "            for m in mask:\n",
    "                if m:\n",
    "                    logits.append(logits_left[it_left])\n",
    "                    it_left += 1\n",
    "                else:\n",
    "                    logits.append(logits_right[it_right])\n",
    "                    it_right += 1\n",
    "            \n",
    "            return logits\n",
    "        \n",
    "    def get_logits_indices_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        # print(\"Indices global\")\n",
    "        # For every sample, compute the probability of reaching each leaf node and then\n",
    "        # return the index of the leaf with the highest path probability\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: return path probabilities and the leaf index\n",
    "            indices = np.full(len(X), index)\n",
    "            return path_probs, indices\n",
    "        else:\n",
    "            # At internal node: get the path_probs and indices from both child nodes, then\n",
    "            # combine them by taking the max prob for each sample\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left, indices_left = self.get_logits_indices_global(\n",
    "                X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right, indices_right = self.get_logits_indices_global(\n",
    "                X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            indices = np.where(probs_left > probs_right, indices_left, indices_right)\n",
    "            probs = np.maximum(probs_left, probs_right)\n",
    "            \n",
    "            return probs, indices\n",
    "        \n",
    "    def get_logits_global(self, X):\n",
    "        root_probs = np.full((len(X)), 1.0)\n",
    "        probs, indices = self.get_logits_indices_global(X, root_probs)\n",
    "        logits = []\n",
    "        for index in indices:\n",
    "            logits.append(self.sess.run(self.leaf_logits[index]).ravel())\n",
    "        return logits\n",
    "    \n",
    "    def get_probs_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        # print(\"Probs global\")\n",
    "        # For every sample, compute the probability of being every target at every\n",
    "        # leaf node, then add them all up\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: return target probabilities at this leaf node\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            probs = np.dot(path_probs.reshape(-1, 1), softmax(logits.reshape(1, -1)))\n",
    "            return probs\n",
    "        else:\n",
    "            # At internal node: get the probabilities from both child nodes and add them together\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left = self.get_probs_global(X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right = self.get_probs_global(X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            return probs_left + probs_right\n",
    "    \n",
    "    def predict(self, X, method=1):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits_local(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            logits = self.get_logits_global(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 3:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return np.argmax(probs, axis=1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_probs(self, X, current_depth=1, index=1):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits_local(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            logits = self.get_logits_global(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 3:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return probs\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def compute_loss(self, X, y):\n",
    "        return self.sess.run(self.loss, feed_dict={self.X_ph: X, self.y_ph: y})\n",
    "    \n",
    "    def visualize_parameters(self, index):\n",
    "        weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "        plot_digit(weights + bias)\n",
    "    \n",
    "    def reset_session(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "            \n",
    "    def __del__(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw data, depth = 5\n",
      "Time to train model: 78.489 s\n",
      "\n",
      "Train loss 1.1052345\n",
      "Method = 1\n",
      "Train accuracy: 0.872 \t Test accuracy: 0.868 \t Loss: 1.127\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.872 \t Test accuracy: 0.868 \t Loss: 1.127\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.872 \t Test accuracy: 0.868 \t Loss: 1.127\n",
      "\n",
      "Using raw data, depth = 6\n",
      "Time to train model: 111.821 s\n",
      "\n",
      "Train loss 1.029531\n",
      "Method = 1\n",
      "Train accuracy: 0.895 \t Test accuracy: 0.889 \t Loss: 1.068\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.895 \t Test accuracy: 0.889 \t Loss: 1.068\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.895 \t Test accuracy: 0.889 \t Loss: 1.068\n",
      "\n",
      "Using raw data, depth = 7\n",
      "Time to train model: 214.107 s\n",
      "\n",
      "Train loss 1.0298743\n",
      "Method = 1\n",
      "Train accuracy: 0.906 \t Test accuracy: 0.896 \t Loss: 1.083\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.906 \t Test accuracy: 0.896 \t Loss: 1.083\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.896 \t Loss: 1.083\n",
      "\n",
      "Using raw data, depth = 8\n",
      "Time to train model: 446.748 s\n",
      "\n",
      "Train loss 1.0087458\n",
      "Method = 1\n",
      "Train accuracy: 0.915 \t Test accuracy: 0.908 \t Loss: 1.061\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.915 \t Test accuracy: 0.908 \t Loss: 1.061\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.915 \t Test accuracy: 0.909 \t Loss: 1.061\n",
      "\n",
      "Using raw data, depth = 9\n",
      "Time to train model: 921.471 s\n",
      "\n",
      "Train loss 1.0522116\n",
      "Method = 1\n",
      "Train accuracy: 0.900 \t Test accuracy: 0.894 \t Loss: 1.110\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.900 \t Test accuracy: 0.894 \t Loss: 1.110\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.900 \t Test accuracy: 0.895 \t Loss: 1.110\n",
      "\n",
      "Using raw data, depth = 10\n",
      "Time to train model: 2076.442 s\n",
      "\n",
      "Train loss 1.0494918\n",
      "Method = 1\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.902 \t Loss: 1.136\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.902 \t Loss: 1.136\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.903 \t Loss: 1.136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using only the raw data\n",
    "\n",
    "for depth in [5, 6, 7, 8, 9, 10]:\n",
    "    print(\"Using raw data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    t = time.time()\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            learning_rate = 0.1,\n",
    "                            momentum = 0.9,\n",
    "                            inverse_temperature=1.0,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, labels_train_one_hot, batch_size=256, n_epochs=25)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, labels_train_one_hot))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, depth = 5\n",
      "Time to train model: 64.495 s\n",
      "\n",
      "Train loss 1.0773523\n",
      "Method = 1\n",
      "Train accuracy: 0.891 \t Test accuracy: 0.889 \t Loss: 1.099\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.891 \t Test accuracy: 0.889 \t Loss: 1.099\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.891 \t Test accuracy: 0.889 \t Loss: 1.099\n",
      "\n",
      "Using cnn data, depth = 6\n",
      "Time to train model: 110.941 s\n",
      "\n",
      "Train loss 1.0084119\n",
      "Method = 1\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.898 \t Loss: 1.046\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.898 \t Loss: 1.046\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.898 \t Loss: 1.046\n",
      "\n",
      "Using cnn data, depth = 7\n",
      "Time to train model: 221.412 s\n",
      "\n",
      "Train loss 1.0296985\n",
      "Method = 1\n",
      "Train accuracy: 0.899 \t Test accuracy: 0.893 \t Loss: 1.068\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.899 \t Test accuracy: 0.893 \t Loss: 1.068\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.900 \t Test accuracy: 0.893 \t Loss: 1.068\n",
      "\n",
      "Using cnn data, depth = 8\n",
      "Time to train model: 436.941 s\n",
      "\n",
      "Train loss 1.0292801\n",
      "Method = 1\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.901 \t Loss: 1.076\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.901 \t Loss: 1.076\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.901 \t Loss: 1.076\n",
      "\n",
      "Using cnn data, depth = 9\n",
      "Time to train model: 959.217 s\n",
      "\n",
      "Train loss 1.0372905\n",
      "Method = 1\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.900 \t Loss: 1.094\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.900 \t Loss: 1.094\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.901 \t Loss: 1.094\n",
      "\n",
      "Using cnn data, depth = 10\n",
      "Time to train model: 2151.193 s\n",
      "\n",
      "Train loss 1.0479537\n",
      "Method = 1\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.902 \t Loss: 1.124\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.902 \t Loss: 1.124\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.902 \t Loss: 1.124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using the data generated by the CNN\n",
    "\n",
    "for depth in [5, 6, 7, 8, 9, 10]:\n",
    "    print(\"Using cnn data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    t = time.time()\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            learning_rate = 0.1,\n",
    "                            momentum = 0.9,\n",
    "                            inverse_temperature=1.0,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=25)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, depth = 5\n",
      "Time to train model: 67.899 s\n",
      "\n",
      "Train loss 1.010978\n",
      "Method = 1\n",
      "Train accuracy: 0.906 \t Test accuracy: 0.892 \t Loss: 1.083\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.906 \t Test accuracy: 0.892 \t Loss: 1.083\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.906 \t Test accuracy: 0.892 \t Loss: 1.083\n",
      "\n",
      "Using cnn data, depth = 6\n",
      "Time to train model: 112.680 s\n",
      "\n",
      "Train loss 1.0217314\n",
      "Method = 1\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.895 \t Loss: 1.052\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.895 \t Loss: 1.052\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.899 \t Test accuracy: 0.895 \t Loss: 1.052\n",
      "\n",
      "Using cnn data, depth = 7\n",
      "Time to train model: 218.269 s\n",
      "\n",
      "Train loss 1.0446178\n",
      "Method = 1\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.897 \t Loss: 1.093\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.897 \t Loss: 1.093\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.897 \t Loss: 1.093\n",
      "\n",
      "Using cnn data, depth = 8\n",
      "Time to train model: 463.426 s\n",
      "\n",
      "Train loss 1.0443943\n",
      "Method = 1\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.902 \t Loss: 1.082\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.902 \t Loss: 1.082\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.903 \t Loss: 1.082\n",
      "\n",
      "Using cnn data, depth = 9\n",
      "Time to train model: 947.923 s\n",
      "\n",
      "Train loss 1.0596128\n",
      "Method = 1\n",
      "Train accuracy: 0.901 \t Test accuracy: 0.899 \t Loss: 1.103\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.901 \t Test accuracy: 0.899 \t Loss: 1.103\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.902 \t Test accuracy: 0.899 \t Loss: 1.103\n",
      "\n",
      "Using cnn data, depth = 10\n",
      "Time to train model: 2066.057 s\n",
      "\n",
      "Train loss 1.0360575\n",
      "Method = 1\n",
      "Train accuracy: 0.912 \t Test accuracy: 0.906 \t Loss: 1.133\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.912 \t Test accuracy: 0.906 \t Loss: 1.133\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.912 \t Test accuracy: 0.907 \t Loss: 1.133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using a combination of both\n",
    "distill_proportion = 0.9\n",
    "combined_labels = (1 - distill_proportion) * labels_train_one_hot + distill_proportion * cnn_probs\n",
    "\n",
    "for depth in [5, 6, 7, 8, 9, 10]:\n",
    "    print(\"Using combined data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    t = time.time()\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            learning_rate = 0.1,\n",
    "                            momentum = 0.9,\n",
    "                            inverse_temperature=1.0,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, combined_labels, batch_size=256, n_epochs=25)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, combined_labels))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
