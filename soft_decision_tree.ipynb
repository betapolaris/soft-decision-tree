{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "# Function to set random seed\n",
    "def set_random_seed(seed=42):\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (63000,) (7000, 784) (7000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(data_train, labels_train), (data_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten the dataset\n",
    "data_train = data_train.reshape(len(data_train), -1)\n",
    "data_test = data_test.reshape(len(data_test), -1)\n",
    "\n",
    "# Combine the dataset\n",
    "data = np.r_[data_train, data_test]\n",
    "labels = np.r_[labels_train, labels_test]\n",
    "\n",
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Create one-hot labels\n",
    "binarizer = LabelBinarizer()\n",
    "labels_one_hot = binarizer.fit_transform(labels)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_indices = np.random.permutation(len(data))\n",
    "data = data[shuffled_indices]\n",
    "labels = labels[shuffled_indices]\n",
    "labels_one_hot = labels_one_hot[shuffled_indices]\n",
    "\n",
    "# Split the dataset\n",
    "train_ratio = 0.9\n",
    "split_index = int(train_ratio * len(data))\n",
    "data_train = data[:split_index]\n",
    "labels_train = labels[:split_index]\n",
    "data_test = data[split_index:]\n",
    "labels_test = labels[split_index:]\n",
    "labels_train_one_hot = labels_one_hot[:split_index]\n",
    "labels_test_one_hot = labels_one_hot[split_index:]\n",
    "\n",
    "print(data_train.shape, labels_train.shape, data_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to fetch a portion of the dataset(\n",
    "def fetch_batch(X, y, batch_size):\n",
    "    shuffled_indices = np.random.permutation(len(X))\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[shuffled_indices[i:i+batch_size]]\n",
    "        y_batch = y[shuffled_indices[i:i+batch_size]]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to display one MNIST data\n",
    "def plot_digit(x):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    x = scaler.inverse_transform(x)\n",
    "    x = x.reshape(28, 28)\n",
    "    plt.imshow(x, cmap=matplotlib.cm.binary)\n",
    "    plt.show()\n",
    "    \n",
    "plot_digit(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic function\n",
    "def sigmoid(X):\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "# The softmax function\n",
    "def softmax(X, axis=-1):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5:\n",
    "    def __init__(self, learning_rate=0.1, momentum=0.5, dropout_rate=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        height = 28\n",
    "        width = 28\n",
    "        channels = 1\n",
    "        input_size = height * width * channels\n",
    "        output_size = 10\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            X_reshaped = tf.reshape(self.X_ph, shape=(-1, height, width, channels))\n",
    "            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "            self.train_ph = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "            \n",
    "            # First convolutional layer\n",
    "            X = tf.layers.conv2d(X_reshaped, filters=6, kernel_size=5, strides=1, \n",
    "                                 padding='SAME', activation=tf.nn.tanh)\n",
    "            \n",
    "            # First pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            X = tf.layers.conv2d(X, filters=16, kernel_size=5, strides=1,\n",
    "                                 padding='VALID', activation=tf.nn.tanh)\n",
    "            \n",
    "            # Second pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            X = tf.layers.conv2d(X, filters=120, kernel_size=5, strides=1,\n",
    "                                 padding='VALID', activation=tf.nn.tanh)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            X = tf.reshape(X, shape=(-1, 120))\n",
    "            X = tf.layers.dropout(X, rate=self.dropout_rate, training=self.train_ph)\n",
    "            X = tf.layers.dense(X, units=84, activation=tf.nn.tanh)\n",
    "            X = tf.layers.dropout(X, rate=self.dropout_rate, training=self.train_ph)\n",
    "            self.logits = tf.layers.dense(X, units=output_size)\n",
    "            \n",
    "            # Probabilities for each class\n",
    "            self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "            \n",
    "            # Use mean cross entropy as the loss function\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_ph)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            # Make optimizer and train op\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1, show_progress=False):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, \n",
    "                                                        self.y_ph: y_batch, \n",
    "                                                        self.train_ph: True})\n",
    "                \n",
    "            if show_progress:\n",
    "                prediction = self.predict(X[:1000])\n",
    "                accuracy = sum(prediction == y[:1000]) / 1000\n",
    "                print(\"Epoch: %d \\t Train accuracy: %.3f\" % (epoch, accuracy))\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return np.argmax(logits, axis=1)\n",
    "        \n",
    "    def get_probs(self, X):\n",
    "        probs = self.sess.run(self.probs, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return probs\n",
    "    \n",
    "    def reset_session(self):\n",
    "        self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build graph: 0.407 s\n",
      "Epoch: 0 \t Train accuracy: 0.968\n",
      "Epoch: 1 \t Train accuracy: 0.974\n",
      "Epoch: 2 \t Train accuracy: 0.974\n",
      "Epoch: 3 \t Train accuracy: 0.981\n",
      "Epoch: 4 \t Train accuracy: 0.985\n",
      "Epoch: 5 \t Train accuracy: 0.987\n",
      "Epoch: 6 \t Train accuracy: 0.985\n",
      "Epoch: 7 \t Train accuracy: 0.985\n",
      "Epoch: 8 \t Train accuracy: 0.990\n",
      "Epoch: 9 \t Train accuracy: 0.980\n",
      "Epoch: 10 \t Train accuracy: 0.988\n",
      "Epoch: 11 \t Train accuracy: 0.992\n",
      "Epoch: 12 \t Train accuracy: 0.987\n",
      "Epoch: 13 \t Train accuracy: 0.990\n",
      "Epoch: 14 \t Train accuracy: 0.994\n",
      "Epoch: 15 \t Train accuracy: 0.994\n",
      "Epoch: 16 \t Train accuracy: 0.991\n",
      "Epoch: 17 \t Train accuracy: 0.987\n",
      "Epoch: 18 \t Train accuracy: 0.994\n",
      "Epoch: 19 \t Train accuracy: 0.996\n",
      "Time to train model: 302.561 s\n",
      "Test accuracy: 0.989\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "lenet5 = LeNet5(learning_rate=0.1, momentum=0.5, dropout_rate=0.5)\n",
    "\n",
    "t = time.time()\n",
    "lenet5.build_graph()\n",
    "print(\"Time to build graph: %.3f s\" % (time.time() - t))\n",
    "\n",
    "t = time.time()\n",
    "lenet5.train(data_train, labels_train, n_epochs=20, show_progress=True)\n",
    "print(\"Time to train model: %.3f s\" % (time.time() - t))\n",
    "\n",
    "prediction = lenet5.predict(data_test)\n",
    "\n",
    "print(\"Test accuracy:\", sum(prediction==labels_test) / len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained CNN to generate target probabilities\n",
    "cnn_probs = lenet5.get_probs(data_train)\n",
    "\n",
    "# Show some examples to make sure everything's okay\n",
    "for i in range(5):\n",
    "    plot_digit(data_train[i])\n",
    "    print(np.around(cnn_probs[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDecisionTree:\n",
    "    def __init__(self, max_depth=1, learning_rate=0.1, momentum=0.1, inverse_temperature=1.0, reg_fn=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.inverse_temperature = inverse_temperature\n",
    "        if reg_fn: \n",
    "            self.reg_fn = reg_fn\n",
    "        else:\n",
    "            self.reg_fn = lambda d: 0  # no regularization\n",
    "    \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        input_size = 28 * 28\n",
    "        output_size = 10\n",
    "        \n",
    "        # Create lists for storing parameters\n",
    "        n_nodes = 2 ** (self.max_depth + 1)\n",
    "        self.weights = [0] * n_nodes\n",
    "        self.bias = [0] * n_nodes\n",
    "        self.leaf_logits = [0] * n_nodes\n",
    "        self.path_probs = [0] * n_nodes\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            self.y_ph = tf.placeholder(shape=(None, output_size), dtype=tf.float32)\n",
    "            \n",
    "            # Initialize the loss function\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Start building from the root node\n",
    "            batch_size = tf.shape(self.X_ph)[0]\n",
    "            self.path_probs[1] = tf.fill([batch_size], 1.0)\n",
    "            self.build_node(self.X_ph, self.y_ph)\n",
    "            \n",
    "            # Finalize the loss function\n",
    "            self.loss = -self.loss\n",
    "            \n",
    "            # Make optimizer and train op\n",
    "            # optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "    \n",
    "    def build_node(self, X, y, current_depth=1, index=1):\n",
    "        if current_depth == self.max_depth:\n",
    "            # Build a leaf node\n",
    "            self.leaf_logits[index] = tf.Variable(initial_value=np.random.randn(y.shape[1], 1), dtype=tf.float32)\n",
    "            \n",
    "            probs = tf.nn.softmax(self.leaf_logits[index], axis=0)\n",
    "            \n",
    "            cross_entropy = tf.squeeze(tf.matmul(y, tf.log(probs)), axis=1)\n",
    "            weighted_cross_entropy = tf.multiply(self.path_probs[index], cross_entropy)\n",
    "            \n",
    "            self.loss = self.loss + tf.reduce_mean(weighted_cross_entropy)\n",
    "        else:\n",
    "            # Build an internal node\n",
    "            self.weights[index] = tf.Variable(initial_value=np.random.randn(X.shape[1], 1), dtype=tf.float32)\n",
    "            self.bias[index] = tf.Variable(initial_value=0.0)\n",
    "            \n",
    "            logits = tf.matmul(X, self.weights[index]) + self.bias[index]\n",
    "            \n",
    "            probs = tf.squeeze(tf.sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            left_index = 2 * index\n",
    "            right_index = 2 * index + 1\n",
    "            self.path_probs[left_index] = tf.multiply(self.path_probs[index], 1-probs)\n",
    "            self.path_probs[right_index] = tf.multiply(self.path_probs[index], probs)\n",
    "            \n",
    "            # Add regularization term (balanced split)\n",
    "            epsilon = tf.constant(1e-10)  # small constant to prevent division by zero/log of zero\n",
    "            reg_term = self.reg_fn(current_depth)\n",
    "            left_ratio = tf.reduce_sum(self.path_probs[left_index]) / (tf.reduce_sum(self.path_probs[index]) + epsilon)\n",
    "            right_ratio = tf.reduce_sum(self.path_probs[right_index]) / (tf.reduce_sum(self.path_probs[index]) + epsilon)\n",
    "            \n",
    "            self.loss = self.loss + reg_term * (0.5 * tf.log(left_ratio + epsilon) + 0.5 * tf.log(right_ratio + epsilon))\n",
    "            \n",
    "            # Build left and right subtrees\n",
    "            self.build_node(X, y, current_depth+1, left_index)\n",
    "            self.build_node(X, y, current_depth+1, right_index)\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1, show_progress=False):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                # Skip last batch if it's small\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, self.y_ph: y_batch})\n",
    "                \n",
    "            if show_progress:\n",
    "                loss = self.sess.run(self.loss, feed_dict={self.X_ph: X, self.y_ph: y})\n",
    "                print(\"Epoch: %d \\t Loss: %.3f\" % (epoch, loss))\n",
    "                   \n",
    "    def get_logits_local(self, X, current_depth=1, index=1):\n",
    "        # At each internal node, split the samples into two based on the \n",
    "        # computed probability at that node only\n",
    "        # print(\"Logits local\")\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: simply return the logits for every sample\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            logits = logits.ravel()\n",
    "            return [logits for _ in range(len(X))]\n",
    "        else:\n",
    "            # At internal node: split the dataset, get the logits of each, and then combine them\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            # print(\"Max logits\", np.max(logits))\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            mask = np.array(probs < 0.5)\n",
    "            \n",
    "            indices_left = np.nonzero(mask)\n",
    "            indices_right = np.nonzero(np.logical_not(mask))\n",
    "            \n",
    "            logits_left = self.get_logits_local(X[indices_left], current_depth+1, index*2)\n",
    "            logits_right = self.get_logits_local(X[indices_right], current_depth+1, index*2+1)\n",
    "            \n",
    "            logits = []\n",
    "            it_left, it_right = 0, 0\n",
    "            for m in mask:\n",
    "                if m:\n",
    "                    logits.append(logits_left[it_left])\n",
    "                    it_left += 1\n",
    "                else:\n",
    "                    logits.append(logits_right[it_right])\n",
    "                    it_right += 1\n",
    "            \n",
    "            return logits\n",
    "        \n",
    "    def get_logits_indices_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        # print(\"Indices global\")\n",
    "        # For every sample, compute the probability of reaching each leaf node and then\n",
    "        # return the index of the leaf with the highest path probability\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: return path probabilities and the leaf index\n",
    "            indices = np.full(len(X), index)\n",
    "            return path_probs, indices\n",
    "        else:\n",
    "            # At internal node: get the path_probs and indices from both child nodes, then\n",
    "            # combine them by taking the max prob for each sample\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left, indices_left = self.get_logits_indices_global(\n",
    "                X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right, indices_right = self.get_logits_indices_global(\n",
    "                X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            indices = np.where(probs_left > probs_right, indices_left, indices_right)\n",
    "            probs = np.maximum(probs_left, probs_right)\n",
    "            \n",
    "            return probs, indices\n",
    "        \n",
    "    def get_logits_global(self, X):\n",
    "        root_probs = np.full((len(X)), 1.0)\n",
    "        probs, indices = self.get_logits_indices_global(X, root_probs)\n",
    "        logits = []\n",
    "        for index in indices:\n",
    "            logits.append(self.sess.run(self.leaf_logits[index]).ravel())\n",
    "        return logits\n",
    "    \n",
    "    def get_probs_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        # print(\"Probs global\")\n",
    "        # For every sample, compute the probability of being every target at every\n",
    "        # leaf node, then add them all up\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: return target probabilities at this leaf node\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            probs = np.dot(path_probs.reshape(-1, 1), softmax(logits.reshape(1, -1)))\n",
    "            return probs\n",
    "        else:\n",
    "            # At internal node: get the probabilities from both child nodes and add them together\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left = self.get_probs_global(X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right = self.get_probs_global(X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            return probs_left + probs_right\n",
    "    \n",
    "    def predict(self, X, method=1):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits_local(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            logits = self.get_logits_global(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 3:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return np.argmax(probs, axis=1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_probs(self, X, current_depth=1, index=1):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits_local(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            logits = self.get_logits_global(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 3:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return probs\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def compute_loss(self, X, y):\n",
    "        return self.sess.run(self.loss, feed_dict={self.X_ph: X, self.y_ph: y})\n",
    "    \n",
    "    def visualize_parameters(self, index):\n",
    "        weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "        plot_digit(weights + bias)\n",
    "    \n",
    "    def reset_session(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "            \n",
    "    def __del__(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw data, depth = 5\n",
      "Time to train model: 62.168 s\n",
      "\n",
      "Train loss 1.0366297\n",
      "Method = 1\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.893 \t Loss: 1.072\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.893 \t Loss: 1.072\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.893 \t Loss: 1.072\n",
      "\n",
      "Using raw data, depth = 6\n",
      "Time to train model: 112.161 s\n",
      "\n",
      "Train loss 1.0182103\n",
      "Method = 1\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.903 \t Loss: 1.055\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.903 \t Loss: 1.055\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.903 \t Loss: 1.055\n",
      "\n",
      "Using raw data, depth = 7\n",
      "Time to train model: 223.368 s\n",
      "\n",
      "Train loss 1.0212076\n",
      "Method = 1\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.897 \t Loss: 1.074\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.897 \t Loss: 1.074\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.898 \t Loss: 1.074\n",
      "\n",
      "Using raw data, depth = 8\n",
      "Time to train model: 443.623 s\n",
      "\n",
      "Train loss 1.0274262\n",
      "Method = 1\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.900 \t Loss: 1.087\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.900 \t Loss: 1.087\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.900 \t Loss: 1.087\n",
      "\n",
      "Using raw data, depth = 9\n",
      "Time to train model: 948.366 s\n",
      "\n",
      "Train loss 1.0478029\n",
      "Method = 1\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.901 \t Loss: 1.108\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.901 \t Loss: 1.108\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.906 \t Test accuracy: 0.901 \t Loss: 1.108\n",
      "\n",
      "Using raw data, depth = 10\n",
      "Time to train model: 2122.693 s\n",
      "\n",
      "Train loss 1.0806774\n",
      "Method = 1\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.892 \t Loss: 1.179\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.898 \t Test accuracy: 0.892 \t Loss: 1.179\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.899 \t Test accuracy: 0.893 \t Loss: 1.179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using only the raw data\n",
    "\n",
    "for depth in [5, 6, 7, 8, 9, 10]:\n",
    "    print(\"Using raw data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    t = time.time()\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            learning_rate = 0.1,\n",
    "                            inverse_temperature=1.0,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, labels_train_one_hot, batch_size=256, n_epochs=25)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, labels_train_one_hot))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, depth = 5\n",
      "Time to train model: 68.362 s\n",
      "\n",
      "Train loss 1.0749836\n",
      "Method = 1\n",
      "Train accuracy: 0.886 \t Test accuracy: 0.880 \t Loss: 1.109\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.886 \t Test accuracy: 0.880 \t Loss: 1.109\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.886 \t Test accuracy: 0.880 \t Loss: 1.109\n",
      "\n",
      "Using cnn data, depth = 6\n",
      "Time to train model: 114.091 s\n",
      "\n",
      "Train loss 1.0434128\n",
      "Method = 1\n",
      "Train accuracy: 0.890 \t Test accuracy: 0.887 \t Loss: 1.060\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.890 \t Test accuracy: 0.887 \t Loss: 1.060\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.891 \t Test accuracy: 0.888 \t Loss: 1.060\n",
      "\n",
      "Using cnn data, depth = 7\n",
      "Time to train model: 223.716 s\n",
      "\n",
      "Train loss 1.0172716\n",
      "Method = 1\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.904 \t Loss: 1.052\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.904 \t Loss: 1.052\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.904 \t Loss: 1.052\n",
      "\n",
      "Using cnn data, depth = 8\n",
      "Time to train model: 466.503 s\n",
      "\n",
      "Train loss 1.0268661\n",
      "Method = 1\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.909 \t Loss: 1.062\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.909 \t Loss: 1.062\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.910 \t Test accuracy: 0.910 \t Loss: 1.062\n",
      "\n",
      "Using cnn data, depth = 9\n",
      "Time to train model: 966.680 s\n",
      "\n",
      "Train loss 1.0458685\n",
      "Method = 1\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.901 \t Loss: 1.104\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.901 \t Loss: 1.104\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.907 \t Test accuracy: 0.901 \t Loss: 1.104\n",
      "\n",
      "Using cnn data, depth = 10\n",
      "Time to train model: 2060.674 s\n",
      "\n",
      "Train loss 1.0456349\n",
      "Method = 1\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.900 \t Loss: 1.141\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.908 \t Test accuracy: 0.900 \t Loss: 1.141\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.909 \t Test accuracy: 0.900 \t Loss: 1.141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using the data generated by the CNN\n",
    "\n",
    "for depth in [5, 6, 7, 8, 9, 10]:\n",
    "    print(\"Using cnn data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    t = time.time()\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            learning_rate = 0.1,\n",
    "                            inverse_temperature = 1.0,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=25)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, learning rate = 0.05\n",
      "Epoch: 0 \t Loss: 1.230\n",
      "Epoch: 1 \t Loss: 1.158\n",
      "Epoch: 2 \t Loss: 1.090\n",
      "Epoch: 3 \t Loss: 1.075\n",
      "Epoch: 4 \t Loss: 1.055\n",
      "Epoch: 5 \t Loss: 1.036\n",
      "Epoch: 6 \t Loss: 1.009\n",
      "Epoch: 7 \t Loss: 1.008\n",
      "Epoch: 8 \t Loss: 0.992\n",
      "Epoch: 9 \t Loss: 1.006\n",
      "Epoch: 10 \t Loss: 0.984\n",
      "Epoch: 11 \t Loss: 0.978\n",
      "Epoch: 12 \t Loss: 0.988\n",
      "Epoch: 13 \t Loss: 0.980\n",
      "Epoch: 14 \t Loss: 0.964\n",
      "Epoch: 15 \t Loss: 0.960\n",
      "Epoch: 16 \t Loss: 0.967\n",
      "Epoch: 17 \t Loss: 0.960\n",
      "Epoch: 18 \t Loss: 0.954\n",
      "Epoch: 19 \t Loss: 0.956\n",
      "Epoch: 20 \t Loss: 0.945\n",
      "Epoch: 21 \t Loss: 0.955\n",
      "Epoch: 22 \t Loss: 0.949\n",
      "Epoch: 23 \t Loss: 0.941\n",
      "Epoch: 24 \t Loss: 0.941\n",
      "Epoch: 25 \t Loss: 0.939\n",
      "Epoch: 26 \t Loss: 0.934\n",
      "Epoch: 27 \t Loss: 0.932\n",
      "Epoch: 28 \t Loss: 0.931\n",
      "Epoch: 29 \t Loss: 0.934\n",
      "Epoch: 30 \t Loss: 0.938\n",
      "Epoch: 31 \t Loss: 0.934\n",
      "Epoch: 32 \t Loss: 0.932\n",
      "Epoch: 33 \t Loss: 0.930\n",
      "Epoch: 34 \t Loss: 0.937\n",
      "Epoch: 35 \t Loss: 0.927\n",
      "Epoch: 36 \t Loss: 0.938\n",
      "Epoch: 37 \t Loss: 0.921\n",
      "Epoch: 38 \t Loss: 0.916\n",
      "Epoch: 39 \t Loss: 0.925\n",
      "Time to train model: 766.006 s\n",
      "\n",
      "Train loss 0.92524403\n",
      "Method = 1\n",
      "Train accuracy: 0.937 \t Test accuracy: 0.925 \t Loss: 0.987\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.937 \t Test accuracy: 0.925 \t Loss: 0.987\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.937 \t Test accuracy: 0.926 \t Loss: 0.987\n",
      "\n",
      "Using cnn data, learning rate = 0.10\n",
      "Epoch: 0 \t Loss: 1.286\n",
      "Epoch: 1 \t Loss: 1.186\n",
      "Epoch: 2 \t Loss: 1.170\n",
      "Epoch: 3 \t Loss: 1.140\n",
      "Epoch: 4 \t Loss: 1.124\n",
      "Epoch: 5 \t Loss: 1.114\n",
      "Epoch: 6 \t Loss: 1.092\n",
      "Epoch: 7 \t Loss: 1.075\n",
      "Epoch: 8 \t Loss: 1.071\n",
      "Epoch: 9 \t Loss: 1.086\n",
      "Epoch: 10 \t Loss: 1.066\n",
      "Epoch: 11 \t Loss: 1.060\n",
      "Epoch: 12 \t Loss: 1.046\n",
      "Epoch: 13 \t Loss: 1.043\n",
      "Epoch: 14 \t Loss: 1.051\n",
      "Epoch: 15 \t Loss: 1.054\n",
      "Epoch: 16 \t Loss: 1.030\n",
      "Epoch: 17 \t Loss: 1.028\n",
      "Epoch: 18 \t Loss: 1.028\n",
      "Epoch: 19 \t Loss: 1.024\n",
      "Epoch: 20 \t Loss: 1.036\n",
      "Epoch: 21 \t Loss: 1.007\n",
      "Epoch: 22 \t Loss: 1.022\n",
      "Epoch: 23 \t Loss: 1.017\n",
      "Epoch: 24 \t Loss: 1.027\n",
      "Epoch: 25 \t Loss: 1.007\n",
      "Epoch: 26 \t Loss: 1.005\n",
      "Epoch: 27 \t Loss: 1.015\n",
      "Epoch: 28 \t Loss: 1.001\n",
      "Epoch: 29 \t Loss: 0.990\n",
      "Epoch: 30 \t Loss: 1.005\n",
      "Epoch: 31 \t Loss: 0.998\n",
      "Epoch: 32 \t Loss: 1.001\n",
      "Epoch: 33 \t Loss: 1.001\n",
      "Epoch: 34 \t Loss: 0.989\n",
      "Epoch: 35 \t Loss: 0.987\n",
      "Epoch: 36 \t Loss: 0.986\n",
      "Epoch: 37 \t Loss: 0.994\n",
      "Epoch: 38 \t Loss: 0.994\n",
      "Epoch: 39 \t Loss: 0.998\n",
      "Time to train model: 768.491 s\n",
      "\n",
      "Train loss 0.99786437\n",
      "Method = 1\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Using cnn data, learning rate = 0.50\n",
      "Epoch: 0 \t Loss: 1.669\n",
      "Epoch: 1 \t Loss: 1.644\n",
      "Epoch: 2 \t Loss: 1.598\n",
      "Epoch: 3 \t Loss: 1.536\n",
      "Epoch: 4 \t Loss: 1.538\n",
      "Epoch: 5 \t Loss: 1.523\n",
      "Epoch: 6 \t Loss: 1.437\n",
      "Epoch: 7 \t Loss: 1.458\n",
      "Epoch: 8 \t Loss: 1.461\n",
      "Epoch: 9 \t Loss: 1.468\n",
      "Epoch: 10 \t Loss: 1.438\n",
      "Epoch: 11 \t Loss: 1.457\n",
      "Epoch: 12 \t Loss: 1.404\n",
      "Epoch: 13 \t Loss: 1.417\n",
      "Epoch: 14 \t Loss: 1.454\n",
      "Epoch: 15 \t Loss: 1.453\n",
      "Epoch: 16 \t Loss: 1.368\n",
      "Epoch: 17 \t Loss: 1.383\n",
      "Epoch: 18 \t Loss: 1.397\n",
      "Epoch: 19 \t Loss: 1.428\n",
      "Epoch: 20 \t Loss: 1.411\n",
      "Epoch: 21 \t Loss: 1.383\n",
      "Epoch: 22 \t Loss: 1.322\n",
      "Epoch: 23 \t Loss: 1.327\n",
      "Epoch: 24 \t Loss: 1.340\n",
      "Epoch: 25 \t Loss: 1.379\n",
      "Epoch: 26 \t Loss: 1.359\n",
      "Epoch: 27 \t Loss: 1.349\n",
      "Epoch: 28 \t Loss: 1.338\n",
      "Epoch: 29 \t Loss: 1.337\n",
      "Epoch: 30 \t Loss: 1.333\n",
      "Epoch: 31 \t Loss: 1.335\n",
      "Epoch: 32 \t Loss: 1.332\n",
      "Epoch: 33 \t Loss: 1.336\n",
      "Epoch: 34 \t Loss: 1.307\n",
      "Epoch: 35 \t Loss: 1.293\n",
      "Epoch: 36 \t Loss: 1.313\n",
      "Epoch: 37 \t Loss: 1.304\n",
      "Epoch: 38 \t Loss: 1.309\n",
      "Epoch: 39 \t Loss: 1.322\n",
      "Time to train model: 746.166 s\n",
      "\n",
      "Train loss 1.3223945\n",
      "Method = 1\n",
      "Train accuracy: 0.810 \t Test accuracy: 0.815 \t Loss: 1.338\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.810 \t Test accuracy: 0.815 \t Loss: 1.338\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.810 \t Test accuracy: 0.815 \t Loss: 1.338\n",
      "\n",
      "Using cnn data, learning rate = 1.00\n",
      "Epoch: 0 \t Loss: 1.982\n",
      "Epoch: 1 \t Loss: 2.002\n",
      "Epoch: 2 \t Loss: 1.869\n",
      "Epoch: 3 \t Loss: 1.828\n",
      "Epoch: 4 \t Loss: 1.822\n",
      "Epoch: 5 \t Loss: 1.732\n",
      "Epoch: 6 \t Loss: 1.798\n",
      "Epoch: 7 \t Loss: 1.756\n",
      "Epoch: 8 \t Loss: 1.676\n",
      "Epoch: 9 \t Loss: 1.697\n",
      "Epoch: 10 \t Loss: 1.682\n",
      "Epoch: 11 \t Loss: 1.686\n",
      "Epoch: 12 \t Loss: 1.679\n",
      "Epoch: 13 \t Loss: 1.774\n",
      "Epoch: 14 \t Loss: 1.755\n",
      "Epoch: 15 \t Loss: 1.814\n",
      "Epoch: 16 \t Loss: 1.687\n",
      "Epoch: 17 \t Loss: 1.707\n",
      "Epoch: 18 \t Loss: 1.709\n",
      "Epoch: 19 \t Loss: 1.686\n",
      "Epoch: 20 \t Loss: 1.655\n",
      "Epoch: 21 \t Loss: 1.695\n",
      "Epoch: 22 \t Loss: 1.627\n",
      "Epoch: 23 \t Loss: 1.614\n",
      "Epoch: 24 \t Loss: 1.590\n",
      "Epoch: 25 \t Loss: 1.614\n",
      "Epoch: 26 \t Loss: 1.588\n",
      "Epoch: 27 \t Loss: 1.604\n",
      "Epoch: 28 \t Loss: 1.669\n",
      "Epoch: 29 \t Loss: 1.603\n",
      "Epoch: 30 \t Loss: 1.581\n",
      "Epoch: 31 \t Loss: 1.651\n",
      "Epoch: 32 \t Loss: 1.698\n",
      "Epoch: 33 \t Loss: 1.642\n",
      "Epoch: 34 \t Loss: 1.574\n",
      "Epoch: 35 \t Loss: 1.566\n",
      "Epoch: 36 \t Loss: 1.549\n",
      "Epoch: 37 \t Loss: 1.525\n",
      "Epoch: 38 \t Loss: 1.540\n",
      "Epoch: 39 \t Loss: 1.546\n",
      "Time to train model: 769.672 s\n",
      "\n",
      "Train loss 1.5457113\n",
      "Method = 1\n",
      "Train accuracy: 0.785 \t Test accuracy: 0.783 \t Loss: 1.580\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.785 \t Test accuracy: 0.783 \t Loss: 1.580\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.785 \t Test accuracy: 0.783 \t Loss: 1.580\n",
      "\n",
      "Using cnn data, learning rate = 1.50\n",
      "Epoch: 0 \t Loss: 2.761\n",
      "Epoch: 1 \t Loss: 2.360\n",
      "Epoch: 2 \t Loss: 2.278\n",
      "Epoch: 3 \t Loss: 2.123\n",
      "Epoch: 4 \t Loss: 2.205\n",
      "Epoch: 5 \t Loss: 2.093\n",
      "Epoch: 6 \t Loss: 2.060\n",
      "Epoch: 7 \t Loss: 2.101\n",
      "Epoch: 8 \t Loss: 2.030\n",
      "Epoch: 9 \t Loss: 2.094\n",
      "Epoch: 10 \t Loss: 2.072\n",
      "Epoch: 11 \t Loss: 2.134\n",
      "Epoch: 12 \t Loss: 2.148\n",
      "Epoch: 13 \t Loss: 2.210\n",
      "Epoch: 14 \t Loss: 2.069\n",
      "Epoch: 15 \t Loss: 2.153\n",
      "Epoch: 16 \t Loss: 2.066\n",
      "Epoch: 17 \t Loss: 2.053\n",
      "Epoch: 18 \t Loss: 2.089\n",
      "Epoch: 19 \t Loss: 2.102\n",
      "Epoch: 20 \t Loss: 2.047\n",
      "Epoch: 21 \t Loss: 2.029\n",
      "Epoch: 22 \t Loss: 2.063\n",
      "Epoch: 23 \t Loss: 1.959\n",
      "Epoch: 24 \t Loss: 1.961\n",
      "Epoch: 25 \t Loss: 1.963\n",
      "Epoch: 26 \t Loss: 1.910\n",
      "Epoch: 27 \t Loss: 1.931\n",
      "Epoch: 28 \t Loss: 1.987\n",
      "Epoch: 29 \t Loss: 2.069\n",
      "Epoch: 30 \t Loss: 1.975\n",
      "Epoch: 31 \t Loss: 1.917\n",
      "Epoch: 32 \t Loss: 1.854\n",
      "Epoch: 33 \t Loss: 2.007\n",
      "Epoch: 34 \t Loss: 1.904\n",
      "Epoch: 35 \t Loss: 1.864\n",
      "Epoch: 36 \t Loss: 1.961\n",
      "Epoch: 37 \t Loss: 1.911\n",
      "Epoch: 38 \t Loss: 1.971\n",
      "Epoch: 39 \t Loss: 1.948\n",
      "Time to train model: 751.856 s\n",
      "\n",
      "Train loss 1.9477206\n",
      "Method = 1\n",
      "Train accuracy: 0.714 \t Test accuracy: 0.718 \t Loss: 1.968\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.714 \t Test accuracy: 0.718 \t Loss: 1.968\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.714 \t Test accuracy: 0.718 \t Loss: 1.968\n",
      "\n",
      "Using cnn data, learning rate = 2.00\n",
      "Epoch: 0 \t Loss: 2.881\n",
      "Epoch: 1 \t Loss: 2.587\n",
      "Epoch: 2 \t Loss: 2.782\n",
      "Epoch: 3 \t Loss: 2.557\n",
      "Epoch: 4 \t Loss: 2.612\n",
      "Epoch: 5 \t Loss: 2.486\n",
      "Epoch: 6 \t Loss: 2.414\n",
      "Epoch: 7 \t Loss: 2.355\n",
      "Epoch: 8 \t Loss: 2.180\n",
      "Epoch: 9 \t Loss: 2.219\n",
      "Epoch: 10 \t Loss: nan\n",
      "Epoch: 11 \t Loss: nan\n",
      "Epoch: 12 \t Loss: nan\n",
      "Epoch: 13 \t Loss: nan\n",
      "Epoch: 14 \t Loss: nan\n",
      "Epoch: 15 \t Loss: nan\n",
      "Epoch: 16 \t Loss: nan\n",
      "Epoch: 17 \t Loss: nan\n",
      "Epoch: 18 \t Loss: nan\n",
      "Epoch: 19 \t Loss: nan\n",
      "Epoch: 20 \t Loss: nan\n",
      "Epoch: 21 \t Loss: nan\n",
      "Epoch: 22 \t Loss: nan\n",
      "Epoch: 23 \t Loss: nan\n",
      "Epoch: 24 \t Loss: nan\n",
      "Epoch: 25 \t Loss: nan\n",
      "Epoch: 26 \t Loss: nan\n",
      "Epoch: 27 \t Loss: nan\n",
      "Epoch: 28 \t Loss: nan\n",
      "Epoch: 29 \t Loss: nan\n",
      "Epoch: 30 \t Loss: nan\n",
      "Epoch: 31 \t Loss: nan\n",
      "Epoch: 32 \t Loss: nan\n",
      "Epoch: 33 \t Loss: nan\n",
      "Epoch: 34 \t Loss: nan\n",
      "Epoch: 35 \t Loss: nan\n",
      "Epoch: 36 \t Loss: nan\n",
      "Epoch: 37 \t Loss: nan\n",
      "Epoch: 38 \t Loss: nan\n",
      "Epoch: 39 \t Loss: nan\n",
      "Time to train model: 773.437 s\n",
      "\n",
      "Train loss nan\n",
      "Method = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/ipykernel_launcher.py:121: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.099 \t Test accuracy: 0.099 \t Loss: nan\n",
      "\n",
      "Method = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/ipykernel_launcher.py:167: RuntimeWarning: invalid value encountered in greater\n",
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/ipykernel_launcher.py:168: RuntimeWarning: invalid value encountered in maximum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.099 \t Test accuracy: 0.099 \t Loss: nan\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.099 \t Test accuracy: 0.099 \t Loss: nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various learning rates\n",
    "\n",
    "for learning_rate in [0.05, 0.1, 0.5, 1.0, 1.5, 2.0]:\n",
    "    print(\"Using cnn data, learning rate = %.2f\" % (learning_rate))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            learning_rate = learning_rate,\n",
    "                            inverse_temperature = 1.0,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=40, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, inverse temperature = 0.1\n",
      "Epoch: 0 \t Loss: 0.992\n",
      "Epoch: 1 \t Loss: 0.933\n",
      "Epoch: 2 \t Loss: 0.893\n",
      "Epoch: 3 \t Loss: 0.885\n",
      "Epoch: 4 \t Loss: 0.866\n",
      "Epoch: 5 \t Loss: 0.850\n",
      "Epoch: 6 \t Loss: 0.848\n",
      "Epoch: 7 \t Loss: 0.838\n",
      "Epoch: 8 \t Loss: 0.828\n",
      "Epoch: 9 \t Loss: 0.824\n",
      "Epoch: 10 \t Loss: 0.820\n",
      "Epoch: 11 \t Loss: 0.814\n",
      "Epoch: 12 \t Loss: 0.810\n",
      "Epoch: 13 \t Loss: 0.808\n",
      "Epoch: 14 \t Loss: 0.808\n",
      "Epoch: 15 \t Loss: 0.804\n",
      "Epoch: 16 \t Loss: 0.805\n",
      "Epoch: 17 \t Loss: 0.799\n",
      "Epoch: 18 \t Loss: 0.798\n",
      "Epoch: 19 \t Loss: 0.796\n",
      "Epoch: 20 \t Loss: 0.791\n",
      "Epoch: 21 \t Loss: 0.794\n",
      "Epoch: 22 \t Loss: 0.788\n",
      "Epoch: 23 \t Loss: 0.788\n",
      "Epoch: 24 \t Loss: 0.784\n",
      "Epoch: 25 \t Loss: 0.788\n",
      "Epoch: 26 \t Loss: 0.785\n",
      "Epoch: 27 \t Loss: 0.783\n",
      "Epoch: 28 \t Loss: 0.783\n",
      "Epoch: 29 \t Loss: 0.781\n",
      "Epoch: 30 \t Loss: 0.782\n",
      "Epoch: 31 \t Loss: 0.782\n",
      "Epoch: 32 \t Loss: 0.783\n",
      "Epoch: 33 \t Loss: 0.772\n",
      "Epoch: 34 \t Loss: 0.773\n",
      "Epoch: 35 \t Loss: 0.773\n",
      "Epoch: 36 \t Loss: 0.776\n",
      "Epoch: 37 \t Loss: 0.770\n",
      "Epoch: 38 \t Loss: 0.771\n",
      "Epoch: 39 \t Loss: 0.772\n",
      "Time to train model: 787.287 s\n",
      "\n",
      "Train loss 0.77184993\n",
      "Method = 1\n",
      "Train accuracy: 0.978 \t Test accuracy: 0.946 \t Loss: 0.976\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.978 \t Test accuracy: 0.945 \t Loss: 0.976\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.980 \t Test accuracy: 0.947 \t Loss: 0.976\n",
      "\n",
      "Using cnn data, inverse temperature = 0.5\n",
      "Epoch: 0 \t Loss: 1.153\n",
      "Epoch: 1 \t Loss: 1.086\n",
      "Epoch: 2 \t Loss: 1.055\n",
      "Epoch: 3 \t Loss: 1.046\n",
      "Epoch: 4 \t Loss: 1.019\n",
      "Epoch: 5 \t Loss: 1.007\n",
      "Epoch: 6 \t Loss: 0.997\n",
      "Epoch: 7 \t Loss: 0.985\n",
      "Epoch: 8 \t Loss: 0.971\n",
      "Epoch: 9 \t Loss: 0.978\n",
      "Epoch: 10 \t Loss: 0.963\n",
      "Epoch: 11 \t Loss: 0.960\n",
      "Epoch: 12 \t Loss: 0.955\n",
      "Epoch: 13 \t Loss: 0.961\n",
      "Epoch: 14 \t Loss: 0.945\n",
      "Epoch: 15 \t Loss: 0.956\n",
      "Epoch: 16 \t Loss: 0.950\n",
      "Epoch: 17 \t Loss: 0.937\n",
      "Epoch: 18 \t Loss: 0.956\n",
      "Epoch: 19 \t Loss: 0.941\n",
      "Epoch: 20 \t Loss: 0.940\n",
      "Epoch: 21 \t Loss: 0.937\n",
      "Epoch: 22 \t Loss: 0.935\n",
      "Epoch: 23 \t Loss: 0.925\n",
      "Epoch: 24 \t Loss: 0.926\n",
      "Epoch: 25 \t Loss: 0.934\n",
      "Epoch: 26 \t Loss: 0.923\n",
      "Epoch: 27 \t Loss: 0.928\n",
      "Epoch: 28 \t Loss: 0.930\n",
      "Epoch: 29 \t Loss: 0.931\n",
      "Epoch: 30 \t Loss: 0.918\n",
      "Epoch: 31 \t Loss: 0.927\n",
      "Epoch: 32 \t Loss: 0.915\n",
      "Epoch: 33 \t Loss: 0.910\n",
      "Epoch: 34 \t Loss: 0.904\n",
      "Epoch: 35 \t Loss: 0.906\n",
      "Epoch: 36 \t Loss: 0.904\n",
      "Epoch: 37 \t Loss: 0.912\n",
      "Epoch: 38 \t Loss: 0.911\n",
      "Epoch: 39 \t Loss: 0.915\n",
      "Time to train model: 786.886 s\n",
      "\n",
      "Train loss 0.91485506\n",
      "Method = 1\n",
      "Train accuracy: 0.941 \t Test accuracy: 0.931 \t Loss: 0.981\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.941 \t Test accuracy: 0.931 \t Loss: 0.981\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.941 \t Test accuracy: 0.931 \t Loss: 0.981\n",
      "\n",
      "Using cnn data, inverse temperature = 1.0\n",
      "Epoch: 0 \t Loss: 1.286\n",
      "Epoch: 1 \t Loss: 1.186\n",
      "Epoch: 2 \t Loss: 1.170\n",
      "Epoch: 3 \t Loss: 1.140\n",
      "Epoch: 4 \t Loss: 1.124\n",
      "Epoch: 5 \t Loss: 1.114\n",
      "Epoch: 6 \t Loss: 1.092\n",
      "Epoch: 7 \t Loss: 1.075\n",
      "Epoch: 8 \t Loss: 1.071\n",
      "Epoch: 9 \t Loss: 1.086\n",
      "Epoch: 10 \t Loss: 1.066\n",
      "Epoch: 11 \t Loss: 1.060\n",
      "Epoch: 12 \t Loss: 1.046\n",
      "Epoch: 13 \t Loss: 1.043\n",
      "Epoch: 14 \t Loss: 1.051\n",
      "Epoch: 15 \t Loss: 1.054\n",
      "Epoch: 16 \t Loss: 1.030\n",
      "Epoch: 17 \t Loss: 1.028\n",
      "Epoch: 18 \t Loss: 1.028\n",
      "Epoch: 19 \t Loss: 1.024\n",
      "Epoch: 20 \t Loss: 1.036\n",
      "Epoch: 21 \t Loss: 1.007\n",
      "Epoch: 22 \t Loss: 1.022\n",
      "Epoch: 23 \t Loss: 1.017\n",
      "Epoch: 24 \t Loss: 1.027\n",
      "Epoch: 25 \t Loss: 1.007\n",
      "Epoch: 26 \t Loss: 1.005\n",
      "Epoch: 27 \t Loss: 1.015\n",
      "Epoch: 28 \t Loss: 1.001\n",
      "Epoch: 29 \t Loss: 0.990\n",
      "Epoch: 30 \t Loss: 1.005\n",
      "Epoch: 31 \t Loss: 0.998\n",
      "Epoch: 32 \t Loss: 1.001\n",
      "Epoch: 33 \t Loss: 1.001\n",
      "Epoch: 34 \t Loss: 0.989\n",
      "Epoch: 35 \t Loss: 0.987\n",
      "Epoch: 36 \t Loss: 0.986\n",
      "Epoch: 37 \t Loss: 0.994\n",
      "Epoch: 38 \t Loss: 0.994\n",
      "Epoch: 39 \t Loss: 0.998\n",
      "Time to train model: 761.735 s\n",
      "\n",
      "Train loss 0.99786437\n",
      "Method = 1\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Using cnn data, inverse temperature = 2.0\n",
      "Epoch: 0 \t Loss: 1.419\n",
      "Epoch: 1 \t Loss: 1.327\n",
      "Epoch: 2 \t Loss: 1.292\n",
      "Epoch: 3 \t Loss: 1.254\n",
      "Epoch: 4 \t Loss: 1.239\n",
      "Epoch: 5 \t Loss: 1.252\n",
      "Epoch: 6 \t Loss: 1.190\n",
      "Epoch: 7 \t Loss: 1.169\n",
      "Epoch: 8 \t Loss: 1.156\n",
      "Epoch: 9 \t Loss: 1.159\n",
      "Epoch: 10 \t Loss: 1.153\n",
      "Epoch: 11 \t Loss: 1.153\n",
      "Epoch: 12 \t Loss: 1.140\n",
      "Epoch: 13 \t Loss: 1.144\n",
      "Epoch: 14 \t Loss: 1.135\n",
      "Epoch: 15 \t Loss: 1.145\n",
      "Epoch: 16 \t Loss: 1.118\n",
      "Epoch: 17 \t Loss: 1.139\n",
      "Epoch: 18 \t Loss: 1.132\n",
      "Epoch: 19 \t Loss: 1.113\n",
      "Epoch: 20 \t Loss: 1.094\n",
      "Epoch: 21 \t Loss: 1.105\n",
      "Epoch: 22 \t Loss: 1.107\n",
      "Epoch: 23 \t Loss: 1.118\n",
      "Epoch: 24 \t Loss: 1.114\n",
      "Epoch: 25 \t Loss: 1.111\n",
      "Epoch: 26 \t Loss: 1.104\n",
      "Epoch: 27 \t Loss: 1.110\n",
      "Epoch: 28 \t Loss: 1.083\n",
      "Epoch: 29 \t Loss: 1.104\n",
      "Epoch: 30 \t Loss: 1.099\n",
      "Epoch: 31 \t Loss: 1.091\n",
      "Epoch: 32 \t Loss: 1.079\n",
      "Epoch: 33 \t Loss: 1.090\n",
      "Epoch: 34 \t Loss: 1.083\n",
      "Epoch: 35 \t Loss: 1.076\n",
      "Epoch: 36 \t Loss: 1.080\n",
      "Epoch: 37 \t Loss: 1.103\n",
      "Epoch: 38 \t Loss: 1.098\n",
      "Epoch: 39 \t Loss: 1.077\n",
      "Time to train model: 784.692 s\n",
      "\n",
      "Train loss 1.0774214\n",
      "Method = 1\n",
      "Train accuracy: 0.894 \t Test accuracy: 0.896 \t Loss: 1.100\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.894 \t Test accuracy: 0.896 \t Loss: 1.100\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.895 \t Test accuracy: 0.896 \t Loss: 1.100\n",
      "\n",
      "Using cnn data, inverse temperature = 5.0\n",
      "Epoch: 0 \t Loss: 1.636\n",
      "Epoch: 1 \t Loss: 1.548\n",
      "Epoch: 2 \t Loss: 1.451\n",
      "Epoch: 3 \t Loss: 1.432\n",
      "Epoch: 4 \t Loss: 1.396\n",
      "Epoch: 5 \t Loss: 1.376\n",
      "Epoch: 6 \t Loss: 1.364\n",
      "Epoch: 7 \t Loss: 1.361\n",
      "Epoch: 8 \t Loss: 1.380\n",
      "Epoch: 9 \t Loss: 1.335\n",
      "Epoch: 10 \t Loss: 1.320\n",
      "Epoch: 11 \t Loss: 1.322\n",
      "Epoch: 12 \t Loss: 1.315\n",
      "Epoch: 13 \t Loss: 1.285\n",
      "Epoch: 14 \t Loss: 1.289\n",
      "Epoch: 15 \t Loss: 1.284\n",
      "Epoch: 16 \t Loss: 1.294\n",
      "Epoch: 17 \t Loss: 1.275\n",
      "Epoch: 18 \t Loss: 1.266\n",
      "Epoch: 19 \t Loss: 1.265\n",
      "Epoch: 20 \t Loss: 1.292\n",
      "Epoch: 21 \t Loss: 1.273\n",
      "Epoch: 22 \t Loss: 1.251\n",
      "Epoch: 23 \t Loss: 1.272\n",
      "Epoch: 24 \t Loss: 1.254\n",
      "Epoch: 25 \t Loss: 1.247\n",
      "Epoch: 26 \t Loss: 1.248\n",
      "Epoch: 27 \t Loss: 1.248\n",
      "Epoch: 28 \t Loss: 1.247\n",
      "Epoch: 29 \t Loss: 1.249\n",
      "Epoch: 30 \t Loss: 1.228\n",
      "Epoch: 31 \t Loss: 1.229\n",
      "Epoch: 32 \t Loss: 1.247\n",
      "Epoch: 33 \t Loss: 1.224\n",
      "Epoch: 34 \t Loss: 1.223\n",
      "Epoch: 35 \t Loss: 1.253\n",
      "Epoch: 36 \t Loss: 1.234\n",
      "Epoch: 37 \t Loss: 1.222\n",
      "Epoch: 38 \t Loss: 1.238\n",
      "Epoch: 39 \t Loss: 1.199\n",
      "Time to train model: 771.746 s\n",
      "\n",
      "Train loss 1.1990596\n",
      "Method = 1\n",
      "Train accuracy: 0.851 \t Test accuracy: 0.849 \t Loss: 1.228\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.851 \t Test accuracy: 0.849 \t Loss: 1.228\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.851 \t Test accuracy: 0.849 \t Loss: 1.228\n",
      "\n",
      "Using cnn data, inverse temperature = 10.0\n",
      "Epoch: 0 \t Loss: 1.775\n",
      "Epoch: 1 \t Loss: 1.666\n",
      "Epoch: 2 \t Loss: 1.654\n",
      "Epoch: 3 \t Loss: 1.642\n",
      "Epoch: 4 \t Loss: 1.579\n",
      "Epoch: 5 \t Loss: 1.580\n",
      "Epoch: 6 \t Loss: 1.586\n",
      "Epoch: 7 \t Loss: 1.555\n",
      "Epoch: 8 \t Loss: 1.533\n",
      "Epoch: 9 \t Loss: 1.539\n",
      "Epoch: 10 \t Loss: 1.506\n",
      "Epoch: 11 \t Loss: 1.506\n",
      "Epoch: 12 \t Loss: 1.508\n",
      "Epoch: 13 \t Loss: 1.475\n",
      "Epoch: 14 \t Loss: 1.491\n",
      "Epoch: 15 \t Loss: 1.503\n",
      "Epoch: 16 \t Loss: 1.474\n",
      "Epoch: 17 \t Loss: 1.474\n",
      "Epoch: 18 \t Loss: 1.461\n",
      "Epoch: 19 \t Loss: 1.474\n",
      "Epoch: 20 \t Loss: 1.466\n",
      "Epoch: 21 \t Loss: 1.444\n",
      "Epoch: 22 \t Loss: 1.434\n",
      "Epoch: 23 \t Loss: 1.463\n",
      "Epoch: 24 \t Loss: 1.474\n",
      "Epoch: 25 \t Loss: 1.425\n",
      "Epoch: 26 \t Loss: 1.422\n",
      "Epoch: 27 \t Loss: 1.441\n",
      "Epoch: 28 \t Loss: 1.427\n",
      "Epoch: 29 \t Loss: 1.433\n",
      "Epoch: 30 \t Loss: 1.430\n",
      "Epoch: 31 \t Loss: 1.402\n",
      "Epoch: 32 \t Loss: 1.425\n",
      "Epoch: 33 \t Loss: 1.408\n",
      "Epoch: 34 \t Loss: 1.394\n",
      "Epoch: 35 \t Loss: 1.393\n",
      "Epoch: 36 \t Loss: 1.376\n",
      "Epoch: 37 \t Loss: 1.374\n",
      "Epoch: 38 \t Loss: 1.362\n",
      "Epoch: 39 \t Loss: 1.367\n",
      "Time to train model: 786.183 s\n",
      "\n",
      "Train loss 1.367179\n",
      "Method = 1\n",
      "Train accuracy: 0.797 \t Test accuracy: 0.789 \t Loss: 1.396\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.797 \t Test accuracy: 0.789 \t Loss: 1.396\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.797 \t Test accuracy: 0.789 \t Loss: 1.396\n",
      "\n",
      "Using cnn data, inverse temperature = 20.0\n",
      "Epoch: 0 \t Loss: 1.870\n",
      "Epoch: 1 \t Loss: 1.753\n",
      "Epoch: 2 \t Loss: 1.659\n",
      "Epoch: 3 \t Loss: 1.679\n",
      "Epoch: 4 \t Loss: 1.731\n",
      "Epoch: 5 \t Loss: 1.606\n",
      "Epoch: 6 \t Loss: 1.566\n",
      "Epoch: 7 \t Loss: 1.525\n",
      "Epoch: 8 \t Loss: 1.534\n",
      "Epoch: 9 \t Loss: 1.517\n",
      "Epoch: 10 \t Loss: 1.549\n",
      "Epoch: 11 \t Loss: 1.523\n",
      "Epoch: 12 \t Loss: 1.516\n",
      "Epoch: 13 \t Loss: 1.532\n",
      "Epoch: 14 \t Loss: 1.518\n",
      "Epoch: 15 \t Loss: 1.511\n",
      "Epoch: 16 \t Loss: 1.476\n",
      "Epoch: 17 \t Loss: 1.483\n",
      "Epoch: 18 \t Loss: 1.487\n",
      "Epoch: 19 \t Loss: 1.477\n",
      "Epoch: 20 \t Loss: 1.483\n",
      "Epoch: 21 \t Loss: 1.506\n",
      "Epoch: 22 \t Loss: 1.482\n",
      "Epoch: 23 \t Loss: 1.490\n",
      "Epoch: 24 \t Loss: 1.477\n",
      "Epoch: 25 \t Loss: 1.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \t Loss: 1.469\n",
      "Epoch: 27 \t Loss: 1.491\n",
      "Epoch: 28 \t Loss: 1.465\n",
      "Epoch: 29 \t Loss: 1.468\n",
      "Epoch: 30 \t Loss: 1.448\n",
      "Epoch: 31 \t Loss: 1.468\n",
      "Epoch: 32 \t Loss: 1.453\n",
      "Epoch: 33 \t Loss: 1.495\n",
      "Epoch: 34 \t Loss: 1.480\n",
      "Epoch: 35 \t Loss: 1.474\n",
      "Epoch: 36 \t Loss: 1.462\n",
      "Epoch: 37 \t Loss: 1.434\n",
      "Epoch: 38 \t Loss: 1.430\n",
      "Epoch: 39 \t Loss: 1.446\n",
      "Time to train model: 786.338 s\n",
      "\n",
      "Train loss 1.446005\n",
      "Method = 1\n",
      "Train accuracy: 0.763 \t Test accuracy: 0.762 \t Loss: 1.472\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.763 \t Test accuracy: 0.762 \t Loss: 1.472\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.763 \t Test accuracy: 0.762 \t Loss: 1.472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various inverse temperatures\n",
    "\n",
    "for inverse_temperature in [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]:\n",
    "    print(\"Using cnn data, inverse temperature = %.1f\" % (inverse_temperature))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            learning_rate = 0.1,\n",
    "                            inverse_temperature = inverse_temperature,\n",
    "                            reg_fn = lambda d: 3 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=40, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, regularization strength = 1.0\n",
      "Epoch: 0 \t Loss: 93.141\n",
      "Epoch: 1 \t Loss: 92.473\n",
      "Epoch: 2 \t Loss: 92.873\n",
      "Epoch: 3 \t Loss: 92.627\n",
      "Epoch: 4 \t Loss: 92.148\n",
      "Epoch: 5 \t Loss: 92.498\n",
      "Epoch: 6 \t Loss: 91.494\n",
      "Epoch: 7 \t Loss: 91.626\n",
      "Epoch: 8 \t Loss: 91.761\n",
      "Epoch: 9 \t Loss: 91.677\n",
      "Epoch: 10 \t Loss: 91.153\n",
      "Epoch: 11 \t Loss: 91.117\n",
      "Epoch: 12 \t Loss: 91.746\n",
      "Epoch: 13 \t Loss: 91.113\n",
      "Epoch: 14 \t Loss: 91.231\n",
      "Epoch: 15 \t Loss: 91.484\n",
      "Epoch: 16 \t Loss: 91.337\n",
      "Epoch: 17 \t Loss: 91.106\n",
      "Epoch: 18 \t Loss: 91.220\n",
      "Epoch: 19 \t Loss: 91.429\n",
      "Epoch: 20 \t Loss: 90.936\n",
      "Epoch: 21 \t Loss: 90.990\n",
      "Epoch: 22 \t Loss: 91.002\n",
      "Epoch: 23 \t Loss: 91.730\n",
      "Epoch: 24 \t Loss: 90.972\n",
      "Epoch: 25 \t Loss: 91.193\n",
      "Epoch: 26 \t Loss: 91.512\n",
      "Epoch: 27 \t Loss: 91.110\n",
      "Epoch: 28 \t Loss: 91.396\n",
      "Epoch: 29 \t Loss: 91.174\n",
      "Epoch: 30 \t Loss: 91.181\n",
      "Epoch: 31 \t Loss: 91.477\n",
      "Epoch: 32 \t Loss: 91.168\n",
      "Epoch: 33 \t Loss: 91.428\n",
      "Epoch: 34 \t Loss: 91.086\n",
      "Epoch: 35 \t Loss: 91.538\n",
      "Epoch: 36 \t Loss: 91.728\n",
      "Epoch: 37 \t Loss: 91.092\n",
      "Epoch: 38 \t Loss: 91.622\n",
      "Epoch: 39 \t Loss: 91.478\n",
      "Time to train model: 786.314 s\n",
      "\n",
      "Train loss 91.477684\n",
      "Method = 1\n",
      "Train accuracy: 0.429 \t Test accuracy: 0.441 \t Loss: 92.068\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.429 \t Test accuracy: 0.441 \t Loss: 92.068\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.436 \t Test accuracy: 0.448 \t Loss: 92.068\n",
      "\n",
      "Using cnn data, regularization strength = 1.5\n",
      "Epoch: 0 \t Loss: 10.837\n",
      "Epoch: 1 \t Loss: 10.722\n",
      "Epoch: 2 \t Loss: 10.751\n",
      "Epoch: 3 \t Loss: 10.683\n",
      "Epoch: 4 \t Loss: 10.684\n",
      "Epoch: 5 \t Loss: 10.610\n",
      "Epoch: 6 \t Loss: 10.698\n",
      "Epoch: 7 \t Loss: 10.632\n",
      "Epoch: 8 \t Loss: 10.550\n",
      "Epoch: 9 \t Loss: 10.670\n",
      "Epoch: 10 \t Loss: 10.654\n",
      "Epoch: 11 \t Loss: 10.623\n",
      "Epoch: 12 \t Loss: 10.649\n",
      "Epoch: 13 \t Loss: 10.624\n",
      "Epoch: 14 \t Loss: 10.656\n",
      "Epoch: 15 \t Loss: 10.711\n",
      "Epoch: 16 \t Loss: 10.672\n",
      "Epoch: 17 \t Loss: 10.641\n",
      "Epoch: 18 \t Loss: 10.679\n",
      "Epoch: 19 \t Loss: 10.684\n",
      "Epoch: 20 \t Loss: 10.697\n",
      "Epoch: 21 \t Loss: 10.672\n",
      "Epoch: 22 \t Loss: 10.642\n",
      "Epoch: 23 \t Loss: 10.634\n",
      "Epoch: 24 \t Loss: 10.642\n",
      "Epoch: 25 \t Loss: 10.708\n",
      "Epoch: 26 \t Loss: 10.670\n",
      "Epoch: 27 \t Loss: 10.589\n",
      "Epoch: 28 \t Loss: 10.611\n",
      "Epoch: 29 \t Loss: 10.600\n",
      "Epoch: 30 \t Loss: 10.620\n",
      "Epoch: 31 \t Loss: 10.637\n",
      "Epoch: 32 \t Loss: 10.711\n",
      "Epoch: 33 \t Loss: 10.529\n",
      "Epoch: 34 \t Loss: 10.672\n",
      "Epoch: 35 \t Loss: 10.599\n",
      "Epoch: 36 \t Loss: 10.658\n",
      "Epoch: 37 \t Loss: 10.575\n",
      "Epoch: 38 \t Loss: 10.549\n",
      "Epoch: 39 \t Loss: 10.511\n",
      "Time to train model: 771.052 s\n",
      "\n",
      "Train loss 10.510703\n",
      "Method = 1\n",
      "Train accuracy: 0.510 \t Test accuracy: 0.520 \t Loss: 10.540\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.510 \t Test accuracy: 0.520 \t Loss: 10.540\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.528 \t Test accuracy: 0.535 \t Loss: 10.540\n",
      "\n",
      "Using cnn data, regularization strength = 2.0\n",
      "Epoch: 0 \t Loss: 3.587\n",
      "Epoch: 1 \t Loss: 3.474\n",
      "Epoch: 2 \t Loss: 3.486\n",
      "Epoch: 3 \t Loss: 3.478\n",
      "Epoch: 4 \t Loss: 3.429\n",
      "Epoch: 5 \t Loss: 3.393\n",
      "Epoch: 6 \t Loss: 3.350\n",
      "Epoch: 7 \t Loss: 3.357\n",
      "Epoch: 8 \t Loss: 3.377\n",
      "Epoch: 9 \t Loss: 3.371\n",
      "Epoch: 10 \t Loss: 3.401\n",
      "Epoch: 11 \t Loss: 3.358\n",
      "Epoch: 12 \t Loss: 3.387\n",
      "Epoch: 13 \t Loss: 3.347\n",
      "Epoch: 14 \t Loss: 3.346\n",
      "Epoch: 15 \t Loss: 3.351\n",
      "Epoch: 16 \t Loss: 3.329\n",
      "Epoch: 17 \t Loss: 3.314\n",
      "Epoch: 18 \t Loss: 3.310\n",
      "Epoch: 19 \t Loss: 3.338\n",
      "Epoch: 20 \t Loss: 3.313\n",
      "Epoch: 21 \t Loss: 3.292\n",
      "Epoch: 22 \t Loss: 3.270\n",
      "Epoch: 23 \t Loss: 3.276\n",
      "Epoch: 24 \t Loss: 3.302\n",
      "Epoch: 25 \t Loss: 3.267\n",
      "Epoch: 26 \t Loss: 3.256\n",
      "Epoch: 27 \t Loss: 3.296\n",
      "Epoch: 28 \t Loss: 3.250\n",
      "Epoch: 29 \t Loss: 3.275\n",
      "Epoch: 30 \t Loss: 3.242\n",
      "Epoch: 31 \t Loss: 3.231\n",
      "Epoch: 32 \t Loss: 3.263\n",
      "Epoch: 33 \t Loss: 3.214\n",
      "Epoch: 34 \t Loss: 3.219\n",
      "Epoch: 35 \t Loss: 3.256\n",
      "Epoch: 36 \t Loss: 3.208\n",
      "Epoch: 37 \t Loss: 3.212\n",
      "Epoch: 38 \t Loss: 3.185\n",
      "Epoch: 39 \t Loss: 3.194\n",
      "Time to train model: 754.112 s\n",
      "\n",
      "Train loss 3.1939611\n",
      "Method = 1\n",
      "Train accuracy: 0.779 \t Test accuracy: 0.773 \t Loss: 3.250\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.779 \t Test accuracy: 0.773 \t Loss: 3.250\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.787 \t Test accuracy: 0.782 \t Loss: 3.250\n",
      "\n",
      "Using cnn data, regularization strength = 2.5\n",
      "Epoch: 0 \t Loss: 1.837\n",
      "Epoch: 1 \t Loss: 1.748\n",
      "Epoch: 2 \t Loss: 1.680\n",
      "Epoch: 3 \t Loss: 1.686\n",
      "Epoch: 4 \t Loss: 1.654\n",
      "Epoch: 5 \t Loss: 1.640\n",
      "Epoch: 6 \t Loss: 1.617\n",
      "Epoch: 7 \t Loss: 1.625\n",
      "Epoch: 8 \t Loss: 1.599\n",
      "Epoch: 9 \t Loss: 1.623\n",
      "Epoch: 10 \t Loss: 1.630\n",
      "Epoch: 11 \t Loss: 1.596\n",
      "Epoch: 12 \t Loss: 1.567\n",
      "Epoch: 13 \t Loss: 1.574\n",
      "Epoch: 14 \t Loss: 1.567\n",
      "Epoch: 15 \t Loss: 1.571\n",
      "Epoch: 16 \t Loss: 1.585\n",
      "Epoch: 17 \t Loss: 1.551\n",
      "Epoch: 18 \t Loss: 1.549\n",
      "Epoch: 19 \t Loss: 1.552\n",
      "Epoch: 20 \t Loss: 1.531\n",
      "Epoch: 21 \t Loss: 1.555\n",
      "Epoch: 22 \t Loss: 1.531\n",
      "Epoch: 23 \t Loss: 1.541\n",
      "Epoch: 24 \t Loss: 1.550\n",
      "Epoch: 25 \t Loss: 1.534\n",
      "Epoch: 26 \t Loss: 1.527\n",
      "Epoch: 27 \t Loss: 1.526\n",
      "Epoch: 28 \t Loss: 1.531\n",
      "Epoch: 29 \t Loss: 1.529\n",
      "Epoch: 30 \t Loss: 1.517\n",
      "Epoch: 31 \t Loss: 1.514\n",
      "Epoch: 32 \t Loss: 1.514\n",
      "Epoch: 33 \t Loss: 1.518\n",
      "Epoch: 34 \t Loss: 1.508\n",
      "Epoch: 35 \t Loss: 1.518\n",
      "Epoch: 36 \t Loss: 1.526\n",
      "Epoch: 37 \t Loss: 1.521\n",
      "Epoch: 38 \t Loss: 1.509\n",
      "Epoch: 39 \t Loss: 1.501\n",
      "Time to train model: 768.445 s\n",
      "\n",
      "Train loss 1.5011204\n",
      "Method = 1\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.899 \t Loss: 1.551\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.899 \t Loss: 1.551\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.899 \t Loss: 1.551\n",
      "\n",
      "Using cnn data, regularization strength = 3.0\n",
      "Epoch: 0 \t Loss: 1.286\n",
      "Epoch: 1 \t Loss: 1.186\n",
      "Epoch: 2 \t Loss: 1.170\n",
      "Epoch: 3 \t Loss: 1.140\n",
      "Epoch: 4 \t Loss: 1.124\n",
      "Epoch: 5 \t Loss: 1.114\n",
      "Epoch: 6 \t Loss: 1.092\n",
      "Epoch: 7 \t Loss: 1.075\n",
      "Epoch: 8 \t Loss: 1.071\n",
      "Epoch: 9 \t Loss: 1.086\n",
      "Epoch: 10 \t Loss: 1.066\n",
      "Epoch: 11 \t Loss: 1.060\n",
      "Epoch: 12 \t Loss: 1.046\n",
      "Epoch: 13 \t Loss: 1.043\n",
      "Epoch: 14 \t Loss: 1.051\n",
      "Epoch: 15 \t Loss: 1.054\n",
      "Epoch: 16 \t Loss: 1.030\n",
      "Epoch: 17 \t Loss: 1.028\n",
      "Epoch: 18 \t Loss: 1.028\n",
      "Epoch: 19 \t Loss: 1.024\n",
      "Epoch: 20 \t Loss: 1.036\n",
      "Epoch: 21 \t Loss: 1.007\n",
      "Epoch: 22 \t Loss: 1.022\n",
      "Epoch: 23 \t Loss: 1.017\n",
      "Epoch: 24 \t Loss: 1.027\n",
      "Epoch: 25 \t Loss: 1.007\n",
      "Epoch: 26 \t Loss: 1.005\n",
      "Epoch: 27 \t Loss: 1.015\n",
      "Epoch: 28 \t Loss: 1.001\n",
      "Epoch: 29 \t Loss: 0.990\n",
      "Epoch: 30 \t Loss: 1.005\n",
      "Epoch: 31 \t Loss: 0.998\n",
      "Epoch: 32 \t Loss: 1.001\n",
      "Epoch: 33 \t Loss: 1.001\n",
      "Epoch: 34 \t Loss: 0.989\n",
      "Epoch: 35 \t Loss: 0.987\n",
      "Epoch: 36 \t Loss: 0.986\n",
      "Epoch: 37 \t Loss: 0.994\n",
      "Epoch: 38 \t Loss: 0.994\n",
      "Epoch: 39 \t Loss: 0.998\n",
      "Time to train model: 777.244 s\n",
      "\n",
      "Train loss 0.99786437\n",
      "Method = 1\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.916 \t Loss: 1.039\n",
      "\n",
      "Using cnn data, regularization strength = 3.5\n",
      "Epoch: 0 \t Loss: 1.035\n",
      "Epoch: 1 \t Loss: 0.984\n",
      "Epoch: 2 \t Loss: 0.951\n",
      "Epoch: 3 \t Loss: 0.926\n",
      "Epoch: 4 \t Loss: 0.895\n",
      "Epoch: 5 \t Loss: 0.894\n",
      "Epoch: 6 \t Loss: 0.873\n",
      "Epoch: 7 \t Loss: 0.864\n",
      "Epoch: 8 \t Loss: 0.861\n",
      "Epoch: 9 \t Loss: 0.841\n",
      "Epoch: 10 \t Loss: 0.838\n",
      "Epoch: 11 \t Loss: 0.850\n",
      "Epoch: 12 \t Loss: 0.820\n",
      "Epoch: 13 \t Loss: 0.854\n",
      "Epoch: 14 \t Loss: 0.820\n",
      "Epoch: 15 \t Loss: 0.809\n",
      "Epoch: 16 \t Loss: 0.815\n",
      "Epoch: 17 \t Loss: 0.814\n",
      "Epoch: 18 \t Loss: 0.798\n",
      "Epoch: 19 \t Loss: 0.792\n",
      "Epoch: 20 \t Loss: 0.799\n",
      "Epoch: 21 \t Loss: 0.812\n",
      "Epoch: 22 \t Loss: 0.798\n",
      "Epoch: 23 \t Loss: 0.791\n",
      "Epoch: 24 \t Loss: 0.797\n",
      "Epoch: 25 \t Loss: 0.799\n",
      "Epoch: 26 \t Loss: 0.786\n",
      "Epoch: 27 \t Loss: 0.792\n",
      "Epoch: 28 \t Loss: 0.776\n",
      "Epoch: 29 \t Loss: 0.792\n",
      "Epoch: 30 \t Loss: 0.779\n",
      "Epoch: 31 \t Loss: 0.777\n",
      "Epoch: 32 \t Loss: 0.778\n",
      "Epoch: 33 \t Loss: 0.778\n",
      "Epoch: 34 \t Loss: 0.778\n",
      "Epoch: 35 \t Loss: 0.776\n",
      "Epoch: 36 \t Loss: 0.771\n",
      "Epoch: 37 \t Loss: 0.772\n",
      "Epoch: 38 \t Loss: 0.774\n",
      "Epoch: 39 \t Loss: 0.769\n",
      "Time to train model: 752.039 s\n",
      "\n",
      "Train loss 0.76868993\n",
      "Method = 1\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.911 \t Loss: 0.838\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.911 \t Loss: 0.838\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.922 \t Test accuracy: 0.911 \t Loss: 0.838\n",
      "\n",
      "Using cnn data, regularization strength = 4.0\n",
      "Epoch: 0 \t Loss: 0.914\n",
      "Epoch: 1 \t Loss: 0.838\n",
      "Epoch: 2 \t Loss: 0.808\n",
      "Epoch: 3 \t Loss: 0.782\n",
      "Epoch: 4 \t Loss: 0.764\n",
      "Epoch: 5 \t Loss: 0.759\n",
      "Epoch: 6 \t Loss: 0.751\n",
      "Epoch: 7 \t Loss: 0.752\n",
      "Epoch: 8 \t Loss: 0.721\n",
      "Epoch: 9 \t Loss: 0.719\n",
      "Epoch: 10 \t Loss: 0.720\n",
      "Epoch: 11 \t Loss: 0.707\n",
      "Epoch: 12 \t Loss: 0.720\n",
      "Epoch: 13 \t Loss: 0.704\n",
      "Epoch: 14 \t Loss: 0.694\n",
      "Epoch: 15 \t Loss: 0.699\n",
      "Epoch: 16 \t Loss: 0.696\n",
      "Epoch: 17 \t Loss: 0.684\n",
      "Epoch: 18 \t Loss: 0.688\n",
      "Epoch: 19 \t Loss: 0.683\n",
      "Epoch: 20 \t Loss: 0.677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \t Loss: 0.676\n",
      "Epoch: 22 \t Loss: 0.673\n",
      "Epoch: 23 \t Loss: 0.668\n",
      "Epoch: 24 \t Loss: 0.669\n",
      "Epoch: 25 \t Loss: 0.661\n",
      "Epoch: 26 \t Loss: 0.654\n",
      "Epoch: 27 \t Loss: 0.666\n",
      "Epoch: 28 \t Loss: 0.670\n",
      "Epoch: 29 \t Loss: 0.658\n",
      "Epoch: 30 \t Loss: 0.655\n",
      "Epoch: 31 \t Loss: 0.653\n",
      "Epoch: 32 \t Loss: 0.656\n",
      "Epoch: 33 \t Loss: 0.662\n",
      "Epoch: 34 \t Loss: 0.640\n",
      "Epoch: 35 \t Loss: 0.655\n",
      "Epoch: 36 \t Loss: 0.642\n",
      "Epoch: 37 \t Loss: 0.646\n",
      "Epoch: 38 \t Loss: 0.651\n",
      "Epoch: 39 \t Loss: 0.647\n",
      "Time to train model: 750.090 s\n",
      "\n",
      "Train loss 0.6473125\n",
      "Method = 1\n",
      "Train accuracy: 0.925 \t Test accuracy: 0.916 \t Loss: 0.702\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.925 \t Test accuracy: 0.916 \t Loss: 0.702\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.925 \t Test accuracy: 0.917 \t Loss: 0.702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various regularization strength\n",
    "\n",
    "for r in [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]:\n",
    "    print(\"Using cnn data, regularization strength = %.1f\" % (r))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            learning_rate = 0.1,\n",
    "                            inverse_temperature = 1.0,\n",
    "                            reg_fn = lambda d: r ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=40, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 874.670 s\n",
      "\n",
      "Train loss 0.41164118\n",
      "Method = 1\n",
      "Train accuracy: 0.989 \t Test accuracy: 0.947 \t Loss: 0.684\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.989 \t Test accuracy: 0.948 \t Loss: 0.684\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.989 \t Test accuracy: 0.951 \t Loss: 0.684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "tree.train(data_train, labels_train_one_hot, batch_size=256, n_epochs=50)\n",
    "\n",
    "print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "\n",
    "print(\"Train loss\", tree.compute_loss(data_train, labels_train_one_hot))\n",
    "\n",
    "for predict_method in [1, 2, 3]:\n",
    "    print(\"Method = %d\" % (predict_method))\n",
    "\n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=predict_method)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=predict_method)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 872.349 s\n",
      "\n",
      "Train loss 0.42515978\n",
      "Method = 1\n",
      "Train accuracy: 0.986 \t Test accuracy: 0.950 \t Loss: 0.643\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.986 \t Test accuracy: 0.952 \t Loss: 0.643\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "tree.train(data_train, cnn_probs, batch_size=256, n_epochs=50)\n",
    "\n",
    "print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "\n",
    "print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "\n",
    "for predict_method in [1, 2, 3]:\n",
    "    print(\"Method = %d\" % (predict_method))\n",
    "\n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=predict_method)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=predict_method)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (train_accuracy, test_accuracy, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
