{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "# Function to set random seed\n",
    "def set_random_seed(seed=42):\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (63000,) (7000, 784) (7000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(data_train, labels_train), (data_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten the dataset\n",
    "data_train = data_train.reshape(len(data_train), -1)\n",
    "data_test = data_test.reshape(len(data_test), -1)\n",
    "\n",
    "# Combine the dataset\n",
    "data = np.r_[data_train, data_test]\n",
    "labels = np.r_[labels_train, labels_test]\n",
    "\n",
    "# Scale the inputs\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Create one-hot labels\n",
    "binarizer = LabelBinarizer()\n",
    "labels_one_hot = binarizer.fit_transform(labels)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_indices = np.random.permutation(len(data))\n",
    "data = data[shuffled_indices]\n",
    "labels = labels[shuffled_indices]\n",
    "labels_one_hot = labels_one_hot[shuffled_indices]\n",
    "\n",
    "# Split the dataset\n",
    "train_ratio = 0.9\n",
    "split_index = int(train_ratio * len(data))\n",
    "data_train = data[:split_index]\n",
    "labels_train = labels[:split_index]\n",
    "data_test = data[split_index:]\n",
    "labels_test = labels[split_index:]\n",
    "labels_train_one_hot = labels_one_hot[:split_index]\n",
    "labels_test_one_hot = labels_one_hot[split_index:]\n",
    "\n",
    "print(data_train.shape, labels_train.shape, data_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to fetch a portion of the dataset(\n",
    "def fetch_batch(X, y, batch_size):\n",
    "    shuffled_indices = np.random.permutation(len(X))\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[shuffled_indices[i:i+batch_size]]\n",
    "        y_batch = y[shuffled_indices[i:i+batch_size]]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Function to display one MNIST data\n",
    "def plot_digit(x):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    x = scaler.inverse_transform(x)\n",
    "    x = x.reshape(28, 28)\n",
    "    plt.imshow(x, cmap=matplotlib.cm.binary)\n",
    "    plt.show()\n",
    "    \n",
    "plot_digit(data_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic function\n",
    "def sigmoid(X):\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "# The softmax function\n",
    "def softmax(X, axis=-1):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5:\n",
    "    def __init__(self, learning_rate=0.1, momentum=0.5, dropout_rate=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        height = 28\n",
    "        width = 28\n",
    "        channels = 1\n",
    "        input_size = height * width * channels\n",
    "        output_size = 10\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            X_reshaped = tf.reshape(self.X_ph, shape=(-1, height, width, channels))\n",
    "            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "            self.train_ph = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "            \n",
    "            # First convolutional layer\n",
    "            X = tf.layers.conv2d(X_reshaped, filters=6, kernel_size=5, strides=1, \n",
    "                                 padding='SAME', activation=tf.nn.tanh)\n",
    "            \n",
    "            # First pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            X = tf.layers.conv2d(X, filters=16, kernel_size=5, strides=1,\n",
    "                                 padding='VALID', activation=tf.nn.tanh)\n",
    "            \n",
    "            # Second pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            X = tf.layers.conv2d(X, filters=120, kernel_size=5, strides=1,\n",
    "                                 padding='VALID', activation=tf.nn.tanh)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            X = tf.reshape(X, shape=(-1, 120))\n",
    "            X = tf.layers.dropout(X, rate=self.dropout_rate, training=self.train_ph)\n",
    "            X = tf.layers.dense(X, units=84, activation=tf.nn.tanh)\n",
    "            X = tf.layers.dropout(X, rate=self.dropout_rate, training=self.train_ph)\n",
    "            self.logits = tf.layers.dense(X, units=output_size)\n",
    "            \n",
    "            # Probabilities for each class\n",
    "            self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "            \n",
    "            # Use mean cross entropy as the loss function\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_ph)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            # Make optimizer and train op\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1, show_progress=False):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, \n",
    "                                                        self.y_ph: y_batch, \n",
    "                                                        self.train_ph: True})\n",
    "                \n",
    "            if show_progress:\n",
    "                prediction = self.predict(X[:1000])\n",
    "                accuracy = sum(prediction == y[:1000]) / 1000\n",
    "                print(\"Epoch: %d \\t Train accuracy: %.3f\" % (epoch, accuracy))\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return np.argmax(logits, axis=1)\n",
    "        \n",
    "    def get_probs(self, X):\n",
    "        probs = self.sess.run(self.probs, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return probs\n",
    "    \n",
    "    def reset_session(self):\n",
    "        self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-cc22cd845523>:26: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-cc22cd845523>:44: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-6-cc22cd845523>:45: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Epoch: 0 \t Train accuracy: 0.966\n",
      "Epoch: 1 \t Train accuracy: 0.979\n",
      "Epoch: 2 \t Train accuracy: 0.981\n",
      "Epoch: 3 \t Train accuracy: 0.984\n",
      "Epoch: 4 \t Train accuracy: 0.982\n",
      "Epoch: 5 \t Train accuracy: 0.984\n",
      "Epoch: 6 \t Train accuracy: 0.986\n",
      "Epoch: 7 \t Train accuracy: 0.990\n",
      "Epoch: 8 \t Train accuracy: 0.990\n",
      "Epoch: 9 \t Train accuracy: 0.990\n",
      "Epoch: 10 \t Train accuracy: 0.991\n",
      "Epoch: 11 \t Train accuracy: 0.992\n",
      "Epoch: 12 \t Train accuracy: 0.987\n",
      "Epoch: 13 \t Train accuracy: 0.989\n",
      "Epoch: 14 \t Train accuracy: 0.992\n",
      "Epoch: 15 \t Train accuracy: 0.995\n",
      "Epoch: 16 \t Train accuracy: 0.992\n",
      "Epoch: 17 \t Train accuracy: 0.994\n",
      "Epoch: 18 \t Train accuracy: 0.996\n",
      "Epoch: 19 \t Train accuracy: 0.993\n",
      "Epoch: 20 \t Train accuracy: 0.995\n",
      "Epoch: 21 \t Train accuracy: 0.996\n",
      "Epoch: 22 \t Train accuracy: 0.995\n",
      "Epoch: 23 \t Train accuracy: 0.993\n",
      "Epoch: 24 \t Train accuracy: 0.996\n",
      "Test accuracy: 0.9892857142857143\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "lenet5 = LeNet5(learning_rate=0.1, momentum=0.5, dropout_rate=0.5)\n",
    "\n",
    "lenet5.build_graph()\n",
    "\n",
    "lenet5.train(data_train, labels_train, n_epochs=25, show_progress=True)\n",
    "\n",
    "prediction = lenet5.predict(data_test)\n",
    "\n",
    "print(\"Test accuracy:\", sum(prediction==labels_test) / len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained CNN to generate target probabilities\n",
    "cnn_probs = lenet5.get_probs(data_train)\n",
    "\n",
    "# Show some examples to make sure everything's okay\n",
    "for i in range(5):\n",
    "    plot_digit(data_train[i])\n",
    "    print(np.around(cnn_probs[i],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDecisionTree:\n",
    "    def __init__(self, max_depth=8, learning_rate=0.1, inverse_temperature=1.0, \n",
    "                 reg_fn=lambda d: 2 ** -d, epsilon=1e-10):\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.inverse_temperature = inverse_temperature\n",
    "        self.reg_fn = reg_fn\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        input_size = 28 * 28\n",
    "        output_size = 10\n",
    "        \n",
    "        # Create lists for storing parameters\n",
    "        n_nodes = 2 ** (self.max_depth + 1)\n",
    "        self.weights = [0] * n_nodes\n",
    "        self.bias = [0] * n_nodes\n",
    "        self.leaf_logits = [0] * n_nodes\n",
    "        self.path_probs = [0] * n_nodes\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            self.y_ph = tf.placeholder(shape=(None, output_size), dtype=tf.float32)\n",
    "            \n",
    "            # Initialize the loss function\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Start building from the root node\n",
    "            batch_size = tf.shape(self.X_ph)[0]\n",
    "            self.path_probs[1] = tf.fill([batch_size], 1.0)\n",
    "            self.build_node(self.X_ph, self.y_ph)\n",
    "            \n",
    "            # Finalize the loss function\n",
    "            self.loss = -self.loss\n",
    "            \n",
    "            # Make optimizer and train op\n",
    "            # optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "    \n",
    "    def build_node(self, X, y, current_depth=1, index=1):\n",
    "        if current_depth == self.max_depth:\n",
    "            # Build a leaf node\n",
    "            self.leaf_logits[index] = tf.Variable(initial_value=np.random.randn(y.shape[1], 1), dtype=tf.float32)\n",
    "            \n",
    "            probs = tf.nn.softmax(self.leaf_logits[index], axis=0)\n",
    "            \n",
    "            cross_entropy = tf.squeeze(tf.matmul(y, tf.log(probs)), axis=1)\n",
    "            weighted_cross_entropy = tf.multiply(self.path_probs[index], cross_entropy)\n",
    "            \n",
    "            self.loss = self.loss + tf.reduce_mean(weighted_cross_entropy)\n",
    "        else:\n",
    "            # Build an internal node\n",
    "            self.weights[index] = tf.Variable(initial_value=np.random.randn(X.shape[1], 1), dtype=tf.float32)\n",
    "            self.bias[index] = tf.Variable(initial_value=0.0)\n",
    "            \n",
    "            logits = tf.matmul(X, self.weights[index]) + self.bias[index]\n",
    "            \n",
    "            probs = tf.squeeze(tf.sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            left_index = 2 * index\n",
    "            right_index = 2 * index + 1\n",
    "            self.path_probs[left_index] = tf.multiply(self.path_probs[index], 1-probs)\n",
    "            self.path_probs[right_index] = tf.multiply(self.path_probs[index], probs)\n",
    "            \n",
    "            # Add regularization term (balanced split)\n",
    "            epsilon = tf.constant(self.epsilon)  # small constant to prevent division by zero/log of zero\n",
    "            reg_term = self.reg_fn(current_depth)\n",
    "            left_ratio = tf.reduce_sum(self.path_probs[left_index]) / (tf.reduce_sum(self.path_probs[index]) + epsilon)\n",
    "            right_ratio = tf.reduce_sum(self.path_probs[right_index]) / (tf.reduce_sum(self.path_probs[index]) + epsilon)\n",
    "            \n",
    "            self.loss = self.loss + reg_term * (0.5 * tf.log(left_ratio + epsilon) + 0.5 * tf.log(right_ratio + epsilon))\n",
    "            \n",
    "            # Build left and right subtrees\n",
    "            self.build_node(X, y, current_depth+1, left_index)\n",
    "            self.build_node(X, y, current_depth+1, right_index)\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1, show_progress=False):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                # Skip last batch if it's small\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, self.y_ph: y_batch})\n",
    "                \n",
    "            if show_progress:\n",
    "                loss = self.sess.run(self.loss, feed_dict={self.X_ph: X, self.y_ph: y})\n",
    "                print(\"Epoch: %d \\t Loss: %.3f\" % (epoch, loss))\n",
    "                   \n",
    "    def get_logits_local(self, X, current_depth=1, index=1):\n",
    "        # At each internal node, split the samples into two based on the \n",
    "        # computed probability at that node only\n",
    "        # print(\"Logits local\")\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: simply return the logits for every sample\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            logits = logits.ravel()\n",
    "            return [logits for _ in range(len(X))]\n",
    "        else:\n",
    "            # At internal node: split the dataset, get the logits of each, and then combine them\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            # print(\"Max logits\", np.max(logits))\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            mask = np.array(probs < 0.5)\n",
    "            \n",
    "            indices_left = np.nonzero(mask)\n",
    "            indices_right = np.nonzero(np.logical_not(mask))\n",
    "            \n",
    "            logits_left = self.get_logits_local(X[indices_left], current_depth+1, index*2)\n",
    "            logits_right = self.get_logits_local(X[indices_right], current_depth+1, index*2+1)\n",
    "            \n",
    "            logits = []\n",
    "            it_left, it_right = 0, 0\n",
    "            for m in mask:\n",
    "                if m:\n",
    "                    logits.append(logits_left[it_left])\n",
    "                    it_left += 1\n",
    "                else:\n",
    "                    logits.append(logits_right[it_right])\n",
    "                    it_right += 1\n",
    "            \n",
    "            return logits\n",
    "        \n",
    "    def get_logits_indices_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        # print(\"Indices global\")\n",
    "        # For every sample, compute the probability of reaching each leaf node and then\n",
    "        # return the index of the leaf with the highest path probability\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: return path probabilities and the leaf index\n",
    "            indices = np.full(len(X), index)\n",
    "            return path_probs, indices\n",
    "        else:\n",
    "            # At internal node: get the path_probs and indices from both child nodes, then\n",
    "            # combine them by taking the max prob for each sample\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left, indices_left = self.get_logits_indices_global(\n",
    "                X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right, indices_right = self.get_logits_indices_global(\n",
    "                X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            indices = np.where(probs_left > probs_right, indices_left, indices_right)\n",
    "            probs = np.maximum(probs_left, probs_right)\n",
    "            \n",
    "            return probs, indices\n",
    "        \n",
    "    def get_logits_global(self, X):\n",
    "        root_probs = np.full((len(X)), 1.0)\n",
    "        probs, indices = self.get_logits_indices_global(X, root_probs)\n",
    "        logits = []\n",
    "        for index in indices:\n",
    "            logits.append(self.sess.run(self.leaf_logits[index]).ravel())\n",
    "        return logits\n",
    "    \n",
    "    def get_probs_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        # print(\"Probs global\")\n",
    "        # For every sample, compute the probability of being every target at every\n",
    "        # leaf node, then add them all up\n",
    "        if len(X) == 0:\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: return target probabilities at this leaf node\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            probs = np.dot(path_probs.reshape(-1, 1), softmax(logits.reshape(1, -1)))\n",
    "            return probs\n",
    "        else:\n",
    "            # At internal node: get the probabilities from both child nodes and add them together\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            logits = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * logits), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left = self.get_probs_global(X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right = self.get_probs_global(X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            return probs_left + probs_right\n",
    "    \n",
    "    def predict(self, X, method=1):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits_local(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            logits = self.get_logits_global(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 3:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return np.argmax(probs, axis=1)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_probs(self, X, current_depth=1, index=1):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits_local(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            logits = self.get_logits_global(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 3:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return probs\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def compute_loss(self, X, y):\n",
    "        return self.sess.run(self.loss, feed_dict={self.X_ph: X, self.y_ph: y})\n",
    "    \n",
    "    def visualize_parameters(self, index):\n",
    "        weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "        plot_digit(weights + bias)\n",
    "    \n",
    "    def reset_session(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "            \n",
    "    def __del__(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 3.576\n",
      "Epoch: 1 \t Loss: 3.527\n",
      "Epoch: 2 \t Loss: 3.428\n",
      "Epoch: 3 \t Loss: 3.426\n",
      "Epoch: 4 \t Loss: 3.443\n",
      "Epoch: 5 \t Loss: 3.475\n",
      "Epoch: 6 \t Loss: 3.456\n",
      "Epoch: 7 \t Loss: 3.403\n",
      "Epoch: 8 \t Loss: 3.434\n",
      "Epoch: 9 \t Loss: 3.450\n",
      "Epoch: 10 \t Loss: 3.397\n",
      "Epoch: 11 \t Loss: 3.383\n",
      "Epoch: 12 \t Loss: 3.424\n",
      "Epoch: 13 \t Loss: 3.409\n",
      "Epoch: 14 \t Loss: 3.392\n",
      "Epoch: 15 \t Loss: 3.383\n",
      "Epoch: 16 \t Loss: 3.339\n",
      "Epoch: 17 \t Loss: 3.332\n",
      "Epoch: 18 \t Loss: 3.356\n",
      "Epoch: 19 \t Loss: 3.395\n",
      "Epoch: 20 \t Loss: 3.365\n",
      "Epoch: 21 \t Loss: 3.333\n",
      "Epoch: 22 \t Loss: 3.369\n",
      "Epoch: 23 \t Loss: 3.360\n",
      "Epoch: 24 \t Loss: 3.343\n",
      "Epoch: 25 \t Loss: 3.340\n",
      "Epoch: 26 \t Loss: 3.287\n",
      "Epoch: 27 \t Loss: 3.325\n",
      "Epoch: 28 \t Loss: 3.279\n",
      "Epoch: 29 \t Loss: 3.332\n",
      "Epoch: 30 \t Loss: 3.291\n",
      "Epoch: 31 \t Loss: 3.271\n",
      "Epoch: 32 \t Loss: 3.279\n",
      "Epoch: 33 \t Loss: 3.242\n",
      "Epoch: 34 \t Loss: 3.249\n",
      "Epoch: 35 \t Loss: 3.264\n",
      "Epoch: 36 \t Loss: 3.251\n",
      "Epoch: 37 \t Loss: 3.264\n",
      "Epoch: 38 \t Loss: 3.253\n",
      "Epoch: 39 \t Loss: 3.289\n",
      "Epoch: 40 \t Loss: 3.266\n",
      "Epoch: 41 \t Loss: 3.262\n",
      "Epoch: 42 \t Loss: 3.268\n",
      "Epoch: 43 \t Loss: 3.256\n",
      "Epoch: 44 \t Loss: 3.267\n",
      "Epoch: 45 \t Loss: 3.277\n",
      "Epoch: 46 \t Loss: 3.263\n",
      "Epoch: 47 \t Loss: 3.255\n",
      "Epoch: 48 \t Loss: 3.310\n",
      "Epoch: 49 \t Loss: 3.289\n",
      "Time to train model: 962.257 s\n",
      "\n",
      "Train loss 3.2885923\n",
      "Method = 1\n",
      "Train accuracy: 0.738 \t Test accuracy: 0.737 \t Loss: 3.323\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.738 \t Test accuracy: 0.737 \t Loss: 3.323\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.740 \t Test accuracy: 0.740 \t Loss: 3.323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using only the raw data\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree()\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "tree.train(data_train, labels_train_one_hot, batch_size=256, n_epochs=50, show_progress=True)\n",
    "\n",
    "print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "\n",
    "print(\"Train loss\", tree.compute_loss(data_train, labels_train_one_hot))\n",
    "\n",
    "for predict_method in [1, 2, 3]:\n",
    "    print(\"Method = %d\" % (predict_method))\n",
    "\n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=predict_method)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=predict_method)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 3.646\n",
      "Epoch: 1 \t Loss: 3.535\n",
      "Epoch: 2 \t Loss: 3.464\n",
      "Epoch: 3 \t Loss: 3.398\n",
      "Epoch: 4 \t Loss: 3.360\n",
      "Epoch: 5 \t Loss: 3.336\n",
      "Epoch: 6 \t Loss: 3.339\n",
      "Epoch: 7 \t Loss: 3.339\n",
      "Epoch: 8 \t Loss: 3.360\n",
      "Epoch: 9 \t Loss: 3.378\n",
      "Epoch: 10 \t Loss: 3.336\n",
      "Epoch: 11 \t Loss: 3.329\n",
      "Epoch: 12 \t Loss: 3.343\n",
      "Epoch: 13 \t Loss: 3.340\n",
      "Epoch: 14 \t Loss: 3.306\n",
      "Epoch: 15 \t Loss: 3.297\n",
      "Epoch: 16 \t Loss: 3.346\n",
      "Epoch: 17 \t Loss: 3.334\n",
      "Epoch: 18 \t Loss: 3.329\n",
      "Epoch: 19 \t Loss: 3.329\n",
      "Epoch: 20 \t Loss: 3.349\n",
      "Epoch: 21 \t Loss: 3.360\n",
      "Epoch: 22 \t Loss: 3.410\n",
      "Epoch: 23 \t Loss: 3.418\n",
      "Epoch: 24 \t Loss: 3.376\n",
      "Epoch: 25 \t Loss: 3.363\n",
      "Epoch: 26 \t Loss: 3.327\n",
      "Epoch: 27 \t Loss: 3.333\n",
      "Epoch: 28 \t Loss: 3.345\n",
      "Epoch: 29 \t Loss: 3.338\n",
      "Epoch: 30 \t Loss: 3.318\n",
      "Epoch: 31 \t Loss: 3.305\n",
      "Epoch: 32 \t Loss: 3.264\n",
      "Epoch: 33 \t Loss: 3.300\n",
      "Epoch: 34 \t Loss: 3.291\n",
      "Epoch: 35 \t Loss: 3.271\n",
      "Epoch: 36 \t Loss: 3.279\n",
      "Epoch: 37 \t Loss: 3.270\n",
      "Epoch: 38 \t Loss: 3.271\n",
      "Epoch: 39 \t Loss: 3.265\n",
      "Epoch: 40 \t Loss: 3.298\n",
      "Epoch: 41 \t Loss: 3.285\n",
      "Epoch: 42 \t Loss: 3.265\n",
      "Epoch: 43 \t Loss: 3.253\n",
      "Epoch: 44 \t Loss: 3.253\n",
      "Epoch: 45 \t Loss: 3.258\n",
      "Epoch: 46 \t Loss: 3.230\n",
      "Epoch: 47 \t Loss: 3.230\n",
      "Epoch: 48 \t Loss: 3.257\n",
      "Epoch: 49 \t Loss: 3.263\n",
      "Time to train model: 932.396 s\n",
      "\n",
      "Train loss 3.2626982\n",
      "Method = 1\n",
      "Train accuracy: 0.751 \t Test accuracy: 0.745 \t Loss: 3.301\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.751 \t Test accuracy: 0.745 \t Loss: 3.301\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.758 \t Test accuracy: 0.752 \t Loss: 3.301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using the data generated by the CNN\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree()\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "tree.train(data_train, cnn_probs, batch_size=256, n_epochs=50, show_progress=True)\n",
    "\n",
    "print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "\n",
    "print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "\n",
    "for predict_method in [1, 2, 3]:\n",
    "    print(\"Method = %d\" % (predict_method))\n",
    "\n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=predict_method)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=predict_method)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, depth = 5\n",
      "Epoch: 0 \t Loss: 2.178\n",
      "Epoch: 1 \t Loss: 2.117\n",
      "Epoch: 2 \t Loss: 2.088\n",
      "Epoch: 3 \t Loss: 2.031\n",
      "Epoch: 4 \t Loss: 2.028\n",
      "Epoch: 5 \t Loss: 2.027\n",
      "Epoch: 6 \t Loss: 2.000\n",
      "Epoch: 7 \t Loss: 2.009\n",
      "Epoch: 8 \t Loss: 2.006\n",
      "Epoch: 9 \t Loss: 1.974\n",
      "Epoch: 10 \t Loss: 1.961\n",
      "Epoch: 11 \t Loss: 1.967\n",
      "Epoch: 12 \t Loss: 1.959\n",
      "Epoch: 13 \t Loss: 1.957\n",
      "Epoch: 14 \t Loss: 1.958\n",
      "Epoch: 15 \t Loss: 1.959\n",
      "Epoch: 16 \t Loss: 1.949\n",
      "Epoch: 17 \t Loss: 1.956\n",
      "Epoch: 18 \t Loss: 1.948\n",
      "Epoch: 19 \t Loss: 1.945\n",
      "Epoch: 20 \t Loss: 1.949\n",
      "Epoch: 21 \t Loss: 1.951\n",
      "Epoch: 22 \t Loss: 1.939\n",
      "Epoch: 23 \t Loss: 1.936\n",
      "Epoch: 24 \t Loss: 1.933\n",
      "Time to train model: 73.084 s\n",
      "\n",
      "Train loss 1.9325297\n",
      "Method = 1\n",
      "Train accuracy: 0.882 \t Test accuracy: 0.876 \t Loss: 1.970\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.882 \t Test accuracy: 0.876 \t Loss: 1.970\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.882 \t Test accuracy: 0.876 \t Loss: 1.970\n",
      "\n",
      "Using cnn data, depth = 6\n",
      "Epoch: 0 \t Loss: 2.502\n",
      "Epoch: 1 \t Loss: 2.426\n",
      "Epoch: 2 \t Loss: 2.367\n",
      "Epoch: 3 \t Loss: 2.368\n",
      "Epoch: 4 \t Loss: 2.341\n",
      "Epoch: 5 \t Loss: 2.314\n",
      "Epoch: 6 \t Loss: 2.323\n",
      "Epoch: 7 \t Loss: 2.313\n",
      "Epoch: 8 \t Loss: 2.317\n",
      "Epoch: 9 \t Loss: 2.293\n",
      "Epoch: 10 \t Loss: 2.260\n",
      "Epoch: 11 \t Loss: 2.253\n",
      "Epoch: 12 \t Loss: 2.256\n",
      "Epoch: 13 \t Loss: 2.245\n",
      "Epoch: 14 \t Loss: 2.226\n",
      "Epoch: 15 \t Loss: 2.242\n",
      "Epoch: 16 \t Loss: 2.235\n",
      "Epoch: 17 \t Loss: 2.224\n",
      "Epoch: 18 \t Loss: 2.217\n",
      "Epoch: 19 \t Loss: 2.222\n",
      "Epoch: 20 \t Loss: 2.208\n",
      "Epoch: 21 \t Loss: 2.201\n",
      "Epoch: 22 \t Loss: 2.217\n",
      "Epoch: 23 \t Loss: 2.226\n",
      "Epoch: 24 \t Loss: 2.228\n",
      "Time to train model: 135.229 s\n",
      "\n",
      "Train loss 2.2276258\n",
      "Method = 1\n",
      "Train accuracy: 0.894 \t Test accuracy: 0.888 \t Loss: 2.269\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.894 \t Test accuracy: 0.888 \t Loss: 2.269\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.894 \t Test accuracy: 0.888 \t Loss: 2.269\n",
      "\n",
      "Using cnn data, depth = 7\n",
      "Epoch: 0 \t Loss: 3.009\n",
      "Epoch: 1 \t Loss: 2.915\n",
      "Epoch: 2 \t Loss: 2.871\n",
      "Epoch: 3 \t Loss: 2.853\n",
      "Epoch: 4 \t Loss: 2.838\n",
      "Epoch: 5 \t Loss: 2.841\n",
      "Epoch: 6 \t Loss: 2.782\n",
      "Epoch: 7 \t Loss: 2.739\n",
      "Epoch: 8 \t Loss: 2.754\n",
      "Epoch: 9 \t Loss: 2.737\n",
      "Epoch: 10 \t Loss: 2.736\n",
      "Epoch: 11 \t Loss: 2.728\n",
      "Epoch: 12 \t Loss: 2.761\n",
      "Epoch: 13 \t Loss: 2.740\n",
      "Epoch: 14 \t Loss: 2.715\n",
      "Epoch: 15 \t Loss: 2.727\n",
      "Epoch: 16 \t Loss: 2.756\n",
      "Epoch: 17 \t Loss: 2.747\n",
      "Epoch: 18 \t Loss: 2.724\n",
      "Epoch: 19 \t Loss: 2.739\n",
      "Epoch: 20 \t Loss: 2.716\n",
      "Epoch: 21 \t Loss: 2.701\n",
      "Epoch: 22 \t Loss: 2.704\n",
      "Epoch: 23 \t Loss: 2.682\n",
      "Epoch: 24 \t Loss: 2.669\n",
      "Time to train model: 247.827 s\n",
      "\n",
      "Train loss 2.6694133\n",
      "Method = 1\n",
      "Train accuracy: 0.843 \t Test accuracy: 0.837 \t Loss: 2.726\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.843 \t Test accuracy: 0.837 \t Loss: 2.726\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.844 \t Test accuracy: 0.837 \t Loss: 2.726\n",
      "\n",
      "Using cnn data, depth = 8\n",
      "Epoch: 0 \t Loss: 3.646\n",
      "Epoch: 1 \t Loss: 3.535\n",
      "Epoch: 2 \t Loss: 3.464\n",
      "Epoch: 3 \t Loss: 3.398\n",
      "Epoch: 4 \t Loss: 3.360\n",
      "Epoch: 5 \t Loss: 3.336\n",
      "Epoch: 6 \t Loss: 3.339\n",
      "Epoch: 7 \t Loss: 3.339\n",
      "Epoch: 8 \t Loss: 3.360\n",
      "Epoch: 9 \t Loss: 3.378\n",
      "Epoch: 10 \t Loss: 3.336\n",
      "Epoch: 11 \t Loss: 3.329\n",
      "Epoch: 12 \t Loss: 3.343\n",
      "Epoch: 13 \t Loss: 3.340\n",
      "Epoch: 14 \t Loss: 3.306\n",
      "Epoch: 15 \t Loss: 3.297\n",
      "Epoch: 16 \t Loss: 3.346\n",
      "Epoch: 17 \t Loss: 3.334\n",
      "Epoch: 18 \t Loss: 3.329\n",
      "Epoch: 19 \t Loss: 3.329\n",
      "Epoch: 20 \t Loss: 3.349\n",
      "Epoch: 21 \t Loss: 3.360\n",
      "Epoch: 22 \t Loss: 3.410\n",
      "Epoch: 23 \t Loss: 3.418\n",
      "Epoch: 24 \t Loss: 3.376\n",
      "Time to train model: 506.186 s\n",
      "\n",
      "Train loss 3.3760889\n",
      "Method = 1\n",
      "Train accuracy: 0.700 \t Test accuracy: 0.693 \t Loss: 3.431\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.700 \t Test accuracy: 0.693 \t Loss: 3.431\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.703 \t Test accuracy: 0.696 \t Loss: 3.431\n",
      "\n",
      "Using cnn data, depth = 9\n",
      "Epoch: 0 \t Loss: 4.094\n",
      "Epoch: 1 \t Loss: 3.995\n",
      "Epoch: 2 \t Loss: 3.944\n",
      "Epoch: 3 \t Loss: 3.900\n",
      "Epoch: 4 \t Loss: 3.948\n",
      "Epoch: 5 \t Loss: 3.863\n",
      "Epoch: 6 \t Loss: 3.876\n",
      "Epoch: 7 \t Loss: 3.832\n",
      "Epoch: 8 \t Loss: 3.807\n",
      "Epoch: 9 \t Loss: 3.786\n",
      "Epoch: 10 \t Loss: 3.830\n",
      "Epoch: 11 \t Loss: 3.828\n",
      "Epoch: 12 \t Loss: 3.874\n",
      "Epoch: 13 \t Loss: 3.798\n",
      "Epoch: 14 \t Loss: 3.780\n",
      "Epoch: 15 \t Loss: 3.803\n",
      "Epoch: 16 \t Loss: 3.744\n",
      "Epoch: 17 \t Loss: 3.712\n",
      "Epoch: 18 \t Loss: 3.732\n",
      "Epoch: 19 \t Loss: 3.730\n",
      "Epoch: 20 \t Loss: 3.737\n",
      "Epoch: 21 \t Loss: 3.661\n",
      "Epoch: 22 \t Loss: 3.747\n",
      "Epoch: 23 \t Loss: 3.714\n",
      "Epoch: 24 \t Loss: 3.709\n",
      "Time to train model: 1074.622 s\n",
      "\n",
      "Train loss 3.7087402\n",
      "Method = 1\n",
      "Train accuracy: 0.737 \t Test accuracy: 0.740 \t Loss: 3.735\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.737 \t Test accuracy: 0.740 \t Loss: 3.735\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.759 \t Test accuracy: 0.764 \t Loss: 3.735\n",
      "\n",
      "Using cnn data, depth = 10\n",
      "Epoch: 0 \t Loss: 4.671\n",
      "Epoch: 1 \t Loss: 4.461\n",
      "Epoch: 2 \t Loss: 4.415\n",
      "Epoch: 3 \t Loss: 4.347\n",
      "Epoch: 4 \t Loss: 4.377\n",
      "Epoch: 5 \t Loss: 4.309\n",
      "Epoch: 6 \t Loss: 4.243\n",
      "Epoch: 7 \t Loss: 4.259\n",
      "Epoch: 8 \t Loss: 4.276\n",
      "Epoch: 9 \t Loss: 4.255\n",
      "Epoch: 10 \t Loss: 4.227\n",
      "Epoch: 11 \t Loss: 4.259\n",
      "Epoch: 12 \t Loss: 4.224\n",
      "Epoch: 13 \t Loss: 4.166\n",
      "Epoch: 14 \t Loss: 4.156\n",
      "Epoch: 15 \t Loss: 4.200\n",
      "Epoch: 16 \t Loss: 4.162\n",
      "Epoch: 17 \t Loss: 4.164\n",
      "Epoch: 18 \t Loss: 4.201\n",
      "Epoch: 19 \t Loss: 4.133\n",
      "Epoch: 20 \t Loss: 4.163\n",
      "Epoch: 21 \t Loss: 4.096\n",
      "Epoch: 22 \t Loss: 4.111\n",
      "Epoch: 23 \t Loss: 4.115\n",
      "Epoch: 24 \t Loss: 4.110\n",
      "Time to train model: 2274.197 s\n",
      "\n",
      "Train loss 4.1103077\n",
      "Method = 1\n",
      "Train accuracy: 0.740 \t Test accuracy: 0.733 \t Loss: 4.167\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.740 \t Test accuracy: 0.735 \t Loss: 4.167\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.790 \t Test accuracy: 0.782 \t Loss: 4.167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various depths\n",
    "\n",
    "for depth in [5, 6, 7, 8, 9, 10]:\n",
    "    print(\"Using cnn data, depth = %d\" % (depth))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth=depth)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=25, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, learning rate = 0.05\n",
      "Epoch: 0 \t Loss: 3.416\n",
      "Epoch: 1 \t Loss: 3.256\n",
      "Epoch: 2 \t Loss: 3.198\n",
      "Epoch: 3 \t Loss: 3.122\n",
      "Epoch: 4 \t Loss: 3.134\n",
      "Epoch: 5 \t Loss: 3.134\n",
      "Epoch: 6 \t Loss: 3.152\n",
      "Epoch: 7 \t Loss: 3.142\n",
      "Epoch: 8 \t Loss: 3.119\n",
      "Epoch: 9 \t Loss: 3.090\n",
      "Epoch: 10 \t Loss: 3.089\n",
      "Epoch: 11 \t Loss: 3.098\n",
      "Epoch: 12 \t Loss: 3.101\n",
      "Epoch: 13 \t Loss: 3.033\n",
      "Epoch: 14 \t Loss: 3.045\n",
      "Epoch: 15 \t Loss: 3.017\n",
      "Epoch: 16 \t Loss: 3.033\n",
      "Epoch: 17 \t Loss: 3.021\n",
      "Epoch: 18 \t Loss: 3.036\n",
      "Epoch: 19 \t Loss: 3.011\n",
      "Epoch: 20 \t Loss: 3.008\n",
      "Epoch: 21 \t Loss: 3.032\n",
      "Epoch: 22 \t Loss: 3.000\n",
      "Epoch: 23 \t Loss: 3.018\n",
      "Epoch: 24 \t Loss: 2.995\n",
      "Time to train model: 509.258 s\n",
      "\n",
      "Train loss 2.9954898\n",
      "Method = 1\n",
      "Train accuracy: 0.863 \t Test accuracy: 0.856 \t Loss: 3.043\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.863 \t Test accuracy: 0.856 \t Loss: 3.043\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.871 \t Test accuracy: 0.862 \t Loss: 3.043\n",
      "\n",
      "Using cnn data, learning rate = 0.10\n",
      "Epoch: 0 \t Loss: 3.646\n",
      "Epoch: 1 \t Loss: 3.535\n",
      "Epoch: 2 \t Loss: 3.464\n",
      "Epoch: 3 \t Loss: 3.398\n",
      "Epoch: 4 \t Loss: 3.360\n",
      "Epoch: 5 \t Loss: 3.336\n",
      "Epoch: 6 \t Loss: 3.339\n",
      "Epoch: 7 \t Loss: 3.339\n",
      "Epoch: 8 \t Loss: 3.360\n",
      "Epoch: 9 \t Loss: 3.378\n",
      "Epoch: 10 \t Loss: 3.336\n",
      "Epoch: 11 \t Loss: 3.329\n",
      "Epoch: 12 \t Loss: 3.343\n",
      "Epoch: 13 \t Loss: 3.340\n",
      "Epoch: 14 \t Loss: 3.306\n",
      "Epoch: 15 \t Loss: 3.297\n",
      "Epoch: 16 \t Loss: 3.346\n",
      "Epoch: 17 \t Loss: 3.334\n",
      "Epoch: 18 \t Loss: 3.329\n",
      "Epoch: 19 \t Loss: 3.329\n",
      "Epoch: 20 \t Loss: 3.349\n",
      "Epoch: 21 \t Loss: 3.360\n",
      "Epoch: 22 \t Loss: 3.410\n",
      "Epoch: 23 \t Loss: 3.418\n",
      "Epoch: 24 \t Loss: 3.376\n",
      "Time to train model: 507.608 s\n",
      "\n",
      "Train loss 3.3760889\n",
      "Method = 1\n",
      "Train accuracy: 0.700 \t Test accuracy: 0.693 \t Loss: 3.431\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.700 \t Test accuracy: 0.693 \t Loss: 3.431\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.703 \t Test accuracy: 0.696 \t Loss: 3.431\n",
      "\n",
      "Using cnn data, learning rate = 0.50\n",
      "Epoch: 0 \t Loss: 3.963\n",
      "Epoch: 1 \t Loss: 3.911\n",
      "Epoch: 2 \t Loss: 3.927\n",
      "Epoch: 3 \t Loss: 3.875\n",
      "Epoch: 4 \t Loss: 3.909\n",
      "Epoch: 5 \t Loss: 3.801\n",
      "Epoch: 6 \t Loss: 3.857\n",
      "Epoch: 7 \t Loss: 3.865\n",
      "Epoch: 8 \t Loss: 3.870\n",
      "Epoch: 9 \t Loss: 3.833\n",
      "Epoch: 10 \t Loss: 3.815\n",
      "Epoch: 11 \t Loss: 3.884\n",
      "Epoch: 12 \t Loss: 3.832\n",
      "Epoch: 13 \t Loss: 3.824\n",
      "Epoch: 14 \t Loss: 3.776\n",
      "Epoch: 15 \t Loss: 3.782\n",
      "Epoch: 16 \t Loss: 3.783\n",
      "Epoch: 17 \t Loss: 3.738\n",
      "Epoch: 18 \t Loss: 3.759\n",
      "Epoch: 19 \t Loss: 3.742\n",
      "Epoch: 20 \t Loss: 3.778\n",
      "Epoch: 21 \t Loss: 3.723\n",
      "Epoch: 22 \t Loss: 3.759\n",
      "Epoch: 23 \t Loss: 3.693\n",
      "Epoch: 24 \t Loss: 3.718\n",
      "Time to train model: 486.133 s\n",
      "\n",
      "Train loss 3.7183232\n",
      "Method = 1\n",
      "Train accuracy: 0.611 \t Test accuracy: 0.618 \t Loss: 3.725\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.611 \t Test accuracy: 0.618 \t Loss: 3.725\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.611 \t Test accuracy: 0.618 \t Loss: 3.725\n",
      "\n",
      "Using cnn data, learning rate = 1.00\n",
      "Epoch: 0 \t Loss: 4.287\n",
      "Epoch: 1 \t Loss: 4.193\n",
      "Epoch: 2 \t Loss: 4.183\n",
      "Epoch: 3 \t Loss: 4.186\n",
      "Epoch: 4 \t Loss: 4.193\n",
      "Epoch: 5 \t Loss: 4.265\n",
      "Epoch: 6 \t Loss: 4.210\n",
      "Epoch: 7 \t Loss: 4.162\n",
      "Epoch: 8 \t Loss: 4.184\n",
      "Epoch: 9 \t Loss: 4.080\n",
      "Epoch: 10 \t Loss: 4.070\n",
      "Epoch: 11 \t Loss: 4.022\n",
      "Epoch: 12 \t Loss: 4.062\n",
      "Epoch: 13 \t Loss: 3.963\n",
      "Epoch: 14 \t Loss: 4.028\n",
      "Epoch: 15 \t Loss: 3.915\n",
      "Epoch: 16 \t Loss: 3.979\n",
      "Epoch: 17 \t Loss: 3.998\n",
      "Epoch: 18 \t Loss: 4.023\n",
      "Epoch: 19 \t Loss: 4.016\n",
      "Epoch: 20 \t Loss: 4.013\n",
      "Epoch: 21 \t Loss: 3.956\n",
      "Epoch: 22 \t Loss: 3.973\n",
      "Epoch: 23 \t Loss: 4.046\n",
      "Epoch: 24 \t Loss: 4.007\n",
      "Time to train model: 489.639 s\n",
      "\n",
      "Train loss 4.0072927\n",
      "Method = 1\n",
      "Train accuracy: 0.558 \t Test accuracy: 0.551 \t Loss: 4.137\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.558 \t Test accuracy: 0.551 \t Loss: 4.137\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.558 \t Test accuracy: 0.551 \t Loss: 4.137\n",
      "\n",
      "Using cnn data, learning rate = 1.50\n",
      "Epoch: 0 \t Loss: 4.570\n",
      "Epoch: 1 \t Loss: 4.554\n",
      "Epoch: 2 \t Loss: 4.341\n",
      "Epoch: 3 \t Loss: 4.381\n",
      "Epoch: 4 \t Loss: 4.495\n",
      "Epoch: 5 \t Loss: 4.439\n",
      "Epoch: 6 \t Loss: 4.355\n",
      "Epoch: 7 \t Loss: 4.584\n",
      "Epoch: 8 \t Loss: 4.252\n",
      "Epoch: 9 \t Loss: 4.284\n",
      "Epoch: 10 \t Loss: 4.304\n",
      "Epoch: 11 \t Loss: 4.363\n",
      "Epoch: 12 \t Loss: 4.348\n",
      "Epoch: 13 \t Loss: 4.365\n",
      "Epoch: 14 \t Loss: 4.373\n",
      "Epoch: 15 \t Loss: 4.341\n",
      "Epoch: 16 \t Loss: 4.483\n",
      "Epoch: 17 \t Loss: 4.332\n",
      "Epoch: 18 \t Loss: 4.405\n",
      "Epoch: 19 \t Loss: 4.303\n",
      "Epoch: 20 \t Loss: 4.218\n",
      "Epoch: 21 \t Loss: 4.234\n",
      "Epoch: 22 \t Loss: 4.257\n",
      "Epoch: 23 \t Loss: 4.258\n",
      "Epoch: 24 \t Loss: 4.285\n",
      "Time to train model: 490.046 s\n",
      "\n",
      "Train loss 4.284831\n",
      "Method = 1\n",
      "Train accuracy: 0.537 \t Test accuracy: 0.535 \t Loss: 4.284\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.537 \t Test accuracy: 0.535 \t Loss: 4.284\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.538 \t Test accuracy: 0.535 \t Loss: 4.284\n",
      "\n",
      "Using cnn data, learning rate = 2.00\n",
      "Epoch: 0 \t Loss: 5.040\n",
      "Epoch: 1 \t Loss: 4.823\n",
      "Epoch: 2 \t Loss: 4.663\n",
      "Epoch: 3 \t Loss: 4.666\n",
      "Epoch: 4 \t Loss: 4.875\n",
      "Epoch: 5 \t Loss: 4.694\n",
      "Epoch: 6 \t Loss: 4.619\n",
      "Epoch: 7 \t Loss: 4.592\n",
      "Epoch: 8 \t Loss: 4.563\n",
      "Epoch: 9 \t Loss: 4.827\n",
      "Epoch: 10 \t Loss: 4.647\n",
      "Epoch: 11 \t Loss: 4.746\n",
      "Epoch: 12 \t Loss: 4.679\n",
      "Epoch: 13 \t Loss: 4.659\n",
      "Epoch: 14 \t Loss: 4.614\n",
      "Epoch: 15 \t Loss: 4.787\n",
      "Epoch: 16 \t Loss: 4.743\n",
      "Epoch: 17 \t Loss: 4.753\n",
      "Epoch: 18 \t Loss: 4.727\n",
      "Epoch: 19 \t Loss: 4.678\n",
      "Epoch: 20 \t Loss: 4.548\n",
      "Epoch: 21 \t Loss: 4.643\n",
      "Epoch: 22 \t Loss: 4.603\n",
      "Epoch: 23 \t Loss: 4.608\n",
      "Epoch: 24 \t Loss: 4.550\n",
      "Time to train model: 504.028 s\n",
      "\n",
      "Train loss 4.550109\n",
      "Method = 1\n",
      "Train accuracy: 0.521 \t Test accuracy: 0.525 \t Loss: 4.553\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.521 \t Test accuracy: 0.525 \t Loss: 4.553\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.521 \t Test accuracy: 0.525 \t Loss: 4.553\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various learning rates\n",
    "\n",
    "for learning_rate in [0.05, 0.1, 0.5, 1.0, 1.5, 2.0]:\n",
    "    print(\"Using cnn data, learning rate = %.2f\" % (learning_rate))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(learning_rate=learning_rate)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=25, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, inverse temperature = 0.1\n",
      "Epoch: 0 \t Loss: 2.802\n",
      "Epoch: 1 \t Loss: 2.725\n",
      "Epoch: 2 \t Loss: 2.697\n",
      "Epoch: 3 \t Loss: 2.677\n",
      "Epoch: 4 \t Loss: 2.667\n",
      "Epoch: 5 \t Loss: 2.650\n",
      "Epoch: 6 \t Loss: 2.642\n",
      "Epoch: 7 \t Loss: 2.634\n",
      "Epoch: 8 \t Loss: 2.625\n",
      "Epoch: 9 \t Loss: 2.626\n",
      "Epoch: 10 \t Loss: 2.616\n",
      "Epoch: 11 \t Loss: 2.613\n",
      "Epoch: 12 \t Loss: 2.610\n",
      "Epoch: 13 \t Loss: 2.611\n",
      "Epoch: 14 \t Loss: 2.603\n",
      "Epoch: 15 \t Loss: 2.605\n",
      "Epoch: 16 \t Loss: 2.599\n",
      "Epoch: 17 \t Loss: 2.598\n",
      "Epoch: 18 \t Loss: 2.596\n",
      "Epoch: 19 \t Loss: 2.597\n",
      "Epoch: 20 \t Loss: 2.590\n",
      "Epoch: 21 \t Loss: 2.592\n",
      "Epoch: 22 \t Loss: 2.587\n",
      "Epoch: 23 \t Loss: 2.591\n",
      "Epoch: 24 \t Loss: 2.588\n",
      "Time to train model: 509.428 s\n",
      "\n",
      "Train loss 2.5881126\n",
      "Method = 1\n",
      "Train accuracy: 0.974 \t Test accuracy: 0.944 \t Loss: 2.766\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.974 \t Test accuracy: 0.944 \t Loss: 2.766\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.976 \t Test accuracy: 0.947 \t Loss: 2.766\n",
      "\n",
      "Using cnn data, inverse temperature = 0.1\n",
      "Epoch: 0 \t Loss: 2.836\n",
      "Epoch: 1 \t Loss: 2.769\n",
      "Epoch: 2 \t Loss: 2.743\n",
      "Epoch: 3 \t Loss: 2.732\n",
      "Epoch: 4 \t Loss: 2.718\n",
      "Epoch: 5 \t Loss: 2.704\n",
      "Epoch: 6 \t Loss: 2.697\n",
      "Epoch: 7 \t Loss: 2.685\n",
      "Epoch: 8 \t Loss: 2.689\n",
      "Epoch: 9 \t Loss: 2.674\n",
      "Epoch: 10 \t Loss: 2.669\n",
      "Epoch: 11 \t Loss: 2.668\n",
      "Epoch: 12 \t Loss: 2.664\n",
      "Epoch: 13 \t Loss: 2.669\n",
      "Epoch: 14 \t Loss: 2.656\n",
      "Epoch: 15 \t Loss: 2.657\n",
      "Epoch: 16 \t Loss: 2.661\n",
      "Epoch: 17 \t Loss: 2.650\n",
      "Epoch: 18 \t Loss: 2.643\n",
      "Epoch: 19 \t Loss: 2.643\n",
      "Epoch: 20 \t Loss: 2.641\n",
      "Epoch: 21 \t Loss: 2.642\n",
      "Epoch: 22 \t Loss: 2.641\n",
      "Epoch: 23 \t Loss: 2.644\n",
      "Epoch: 24 \t Loss: 2.638\n",
      "Time to train model: 508.053 s\n",
      "\n",
      "Train loss 2.637598\n",
      "Method = 1\n",
      "Train accuracy: 0.963 \t Test accuracy: 0.936 \t Loss: 2.801\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.963 \t Test accuracy: 0.936 \t Loss: 2.801\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.967 \t Test accuracy: 0.939 \t Loss: 2.801\n",
      "\n",
      "Using cnn data, inverse temperature = 0.5\n",
      "Epoch: 0 \t Loss: 3.262\n",
      "Epoch: 1 \t Loss: 3.170\n",
      "Epoch: 2 \t Loss: 3.163\n",
      "Epoch: 3 \t Loss: 3.126\n",
      "Epoch: 4 \t Loss: 3.114\n",
      "Epoch: 5 \t Loss: 3.143\n",
      "Epoch: 6 \t Loss: 3.149\n",
      "Epoch: 7 \t Loss: 3.131\n",
      "Epoch: 8 \t Loss: 3.073\n",
      "Epoch: 9 \t Loss: 3.070\n",
      "Epoch: 10 \t Loss: 3.046\n",
      "Epoch: 11 \t Loss: 3.061\n",
      "Epoch: 12 \t Loss: 3.056\n",
      "Epoch: 13 \t Loss: 3.073\n",
      "Epoch: 14 \t Loss: 3.073\n",
      "Epoch: 15 \t Loss: 3.040\n",
      "Epoch: 16 \t Loss: 3.016\n",
      "Epoch: 17 \t Loss: 3.014\n",
      "Epoch: 18 \t Loss: 3.034\n",
      "Epoch: 19 \t Loss: 3.014\n",
      "Epoch: 20 \t Loss: 2.992\n",
      "Epoch: 21 \t Loss: 3.023\n",
      "Epoch: 22 \t Loss: 2.999\n",
      "Epoch: 23 \t Loss: 2.985\n",
      "Epoch: 24 \t Loss: 3.002\n",
      "Time to train model: 510.645 s\n",
      "\n",
      "Train loss 3.002121\n",
      "Method = 1\n",
      "Train accuracy: 0.867 \t Test accuracy: 0.855 \t Loss: 3.069\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.867 \t Test accuracy: 0.855 \t Loss: 3.069\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.873 \t Test accuracy: 0.861 \t Loss: 3.069\n",
      "\n",
      "Using cnn data, inverse temperature = 1.0\n",
      "Epoch: 0 \t Loss: 3.646\n",
      "Epoch: 1 \t Loss: 3.535\n",
      "Epoch: 2 \t Loss: 3.464\n",
      "Epoch: 3 \t Loss: 3.398\n",
      "Epoch: 4 \t Loss: 3.360\n",
      "Epoch: 5 \t Loss: 3.336\n",
      "Epoch: 6 \t Loss: 3.339\n",
      "Epoch: 7 \t Loss: 3.339\n",
      "Epoch: 8 \t Loss: 3.360\n",
      "Epoch: 9 \t Loss: 3.378\n",
      "Epoch: 10 \t Loss: 3.336\n",
      "Epoch: 11 \t Loss: 3.329\n",
      "Epoch: 12 \t Loss: 3.343\n",
      "Epoch: 13 \t Loss: 3.340\n",
      "Epoch: 14 \t Loss: 3.306\n",
      "Epoch: 15 \t Loss: 3.297\n",
      "Epoch: 16 \t Loss: 3.346\n",
      "Epoch: 17 \t Loss: 3.334\n",
      "Epoch: 18 \t Loss: 3.329\n",
      "Epoch: 19 \t Loss: 3.329\n",
      "Epoch: 20 \t Loss: 3.349\n",
      "Epoch: 21 \t Loss: 3.360\n",
      "Epoch: 22 \t Loss: 3.410\n"
     ]
    }
   ],
   "source": [
    "# Try various inverse temperatures\n",
    "\n",
    "for inverse_temperature in [0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]:\n",
    "    print(\"Using cnn data, inverse temperature = %.1f\" % (inverse_temperature))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(inverse_temperature=inverse_temperature)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=25, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, regularization strength = 1.0\n",
      "Epoch: 0 \t Loss: 94.108\n",
      "Epoch: 1 \t Loss: 93.185\n",
      "Epoch: 2 \t Loss: 91.612\n",
      "Epoch: 3 \t Loss: 91.372\n",
      "Epoch: 4 \t Loss: 91.593\n",
      "Epoch: 5 \t Loss: 91.858\n",
      "Epoch: 6 \t Loss: 91.286\n",
      "Epoch: 7 \t Loss: 91.366\n",
      "Epoch: 8 \t Loss: 91.702\n",
      "Epoch: 9 \t Loss: 91.491\n",
      "Epoch: 10 \t Loss: 91.268\n",
      "Epoch: 11 \t Loss: 91.711\n",
      "Epoch: 12 \t Loss: 91.721\n",
      "Epoch: 13 \t Loss: 91.495\n",
      "Epoch: 14 \t Loss: 91.524\n",
      "Epoch: 15 \t Loss: 91.074\n",
      "Epoch: 16 \t Loss: 91.431\n",
      "Epoch: 17 \t Loss: 91.523\n",
      "Epoch: 18 \t Loss: 91.330\n",
      "Epoch: 19 \t Loss: 91.092\n",
      "Epoch: 20 \t Loss: 91.330\n",
      "Epoch: 21 \t Loss: 91.204\n",
      "Epoch: 22 \t Loss: 90.972\n",
      "Epoch: 23 \t Loss: 91.764\n",
      "Epoch: 24 \t Loss: 91.364\n",
      "Time to train model: 538.041 s\n",
      "\n",
      "Train loss 91.36372\n",
      "Method = 1\n",
      "Train accuracy: 0.442 \t Test accuracy: 0.441 \t Loss: 91.674\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.442 \t Test accuracy: 0.441 \t Loss: 91.674\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.451 \t Test accuracy: 0.451 \t Loss: 91.674\n",
      "\n",
      "Using cnn data, regularization strength = 1.5\n",
      "Epoch: 0 \t Loss: 10.749\n",
      "Epoch: 1 \t Loss: 10.687\n",
      "Epoch: 2 \t Loss: 10.756\n",
      "Epoch: 3 \t Loss: 10.744\n",
      "Epoch: 4 \t Loss: 10.707\n",
      "Epoch: 5 \t Loss: 10.678\n",
      "Epoch: 6 \t Loss: 10.663\n",
      "Epoch: 7 \t Loss: 10.622\n",
      "Epoch: 8 \t Loss: 10.629\n",
      "Epoch: 9 \t Loss: 10.627\n",
      "Epoch: 10 \t Loss: 10.578\n",
      "Epoch: 11 \t Loss: 10.616\n",
      "Epoch: 12 \t Loss: 10.612\n",
      "Epoch: 13 \t Loss: 10.682\n",
      "Epoch: 14 \t Loss: 10.620\n",
      "Epoch: 15 \t Loss: 10.651\n",
      "Epoch: 16 \t Loss: 10.638\n",
      "Epoch: 17 \t Loss: 10.639\n",
      "Epoch: 18 \t Loss: 10.590\n",
      "Epoch: 19 \t Loss: 10.685\n",
      "Epoch: 20 \t Loss: 10.628\n",
      "Epoch: 21 \t Loss: 10.638\n",
      "Epoch: 22 \t Loss: 10.639\n",
      "Epoch: 23 \t Loss: 10.581\n",
      "Epoch: 24 \t Loss: 10.626\n",
      "Time to train model: 496.271 s\n",
      "\n",
      "Train loss 10.625796\n",
      "Method = 1\n",
      "Train accuracy: 0.480 \t Test accuracy: 0.482 \t Loss: 10.691\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.480 \t Test accuracy: 0.482 \t Loss: 10.691\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.484 \t Test accuracy: 0.487 \t Loss: 10.691\n",
      "\n",
      "Using cnn data, regularization strength = 2.0\n",
      "Epoch: 0 \t Loss: 3.546\n",
      "Epoch: 1 \t Loss: 3.477\n",
      "Epoch: 2 \t Loss: 3.460\n",
      "Epoch: 3 \t Loss: 3.408\n",
      "Epoch: 4 \t Loss: 3.433\n",
      "Epoch: 5 \t Loss: 3.423\n",
      "Epoch: 6 \t Loss: 3.459\n",
      "Epoch: 7 \t Loss: 3.430\n",
      "Epoch: 8 \t Loss: 3.490\n",
      "Epoch: 9 \t Loss: 3.395\n",
      "Epoch: 10 \t Loss: 3.408\n",
      "Epoch: 11 \t Loss: 3.413\n",
      "Epoch: 12 \t Loss: 3.367\n",
      "Epoch: 13 \t Loss: 3.390\n",
      "Epoch: 14 \t Loss: 3.400\n",
      "Epoch: 15 \t Loss: 3.342\n",
      "Epoch: 16 \t Loss: 3.360\n",
      "Epoch: 17 \t Loss: 3.327\n",
      "Epoch: 18 \t Loss: 3.351\n",
      "Epoch: 19 \t Loss: 3.339\n",
      "Epoch: 20 \t Loss: 3.321\n",
      "Epoch: 21 \t Loss: 3.358\n",
      "Epoch: 22 \t Loss: 3.324\n",
      "Epoch: 23 \t Loss: 3.327\n",
      "Epoch: 24 \t Loss: 3.356\n",
      "Time to train model: 502.510 s\n",
      "\n",
      "Train loss 3.3558545\n",
      "Method = 1\n",
      "Train accuracy: 0.717 \t Test accuracy: 0.715 \t Loss: 3.381\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.717 \t Test accuracy: 0.715 \t Loss: 3.381\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.720 \t Test accuracy: 0.718 \t Loss: 3.381\n",
      "\n",
      "Using cnn data, regularization strength = 2.5\n",
      "Epoch: 0 \t Loss: 1.873\n",
      "Epoch: 1 \t Loss: 1.779\n",
      "Epoch: 2 \t Loss: 1.732\n",
      "Epoch: 3 \t Loss: 1.686\n",
      "Epoch: 4 \t Loss: 1.661\n",
      "Epoch: 5 \t Loss: 1.654\n",
      "Epoch: 6 \t Loss: 1.646\n",
      "Epoch: 7 \t Loss: 1.636\n",
      "Epoch: 8 \t Loss: 1.626\n",
      "Epoch: 9 \t Loss: 1.623\n",
      "Epoch: 10 \t Loss: 1.612\n",
      "Epoch: 11 \t Loss: 1.593\n",
      "Epoch: 12 \t Loss: 1.583\n",
      "Epoch: 13 \t Loss: 1.596\n",
      "Epoch: 14 \t Loss: 1.602\n",
      "Epoch: 15 \t Loss: 1.591\n",
      "Epoch: 16 \t Loss: 1.565\n",
      "Epoch: 17 \t Loss: 1.554\n",
      "Epoch: 18 \t Loss: 1.589\n",
      "Epoch: 19 \t Loss: 1.553\n",
      "Epoch: 20 \t Loss: 1.553\n",
      "Epoch: 21 \t Loss: 1.579\n",
      "Epoch: 22 \t Loss: 1.558\n",
      "Epoch: 23 \t Loss: 1.557\n",
      "Epoch: 24 \t Loss: 1.565\n",
      "Time to train model: 500.865 s\n",
      "\n",
      "Train loss 1.5654554\n",
      "Method = 1\n",
      "Train accuracy: 0.886 \t Test accuracy: 0.880 \t Loss: 1.610\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.886 \t Test accuracy: 0.880 \t Loss: 1.610\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.886 \t Test accuracy: 0.881 \t Loss: 1.610\n",
      "\n",
      "Using cnn data, regularization strength = 3.0\n",
      "Epoch: 0 \t Loss: 1.312\n",
      "Epoch: 1 \t Loss: 1.218\n",
      "Epoch: 2 \t Loss: 1.184\n",
      "Epoch: 3 \t Loss: 1.144\n",
      "Epoch: 4 \t Loss: 1.122\n",
      "Epoch: 5 \t Loss: 1.109\n",
      "Epoch: 6 \t Loss: 1.117\n",
      "Epoch: 7 \t Loss: 1.089\n",
      "Epoch: 8 \t Loss: 1.092\n",
      "Epoch: 9 \t Loss: 1.062\n",
      "Epoch: 10 \t Loss: 1.064\n",
      "Epoch: 11 \t Loss: 1.069\n",
      "Epoch: 12 \t Loss: 1.063\n",
      "Epoch: 13 \t Loss: 1.047\n",
      "Epoch: 14 \t Loss: 1.048\n",
      "Epoch: 15 \t Loss: 1.040\n",
      "Epoch: 16 \t Loss: 1.033\n",
      "Epoch: 17 \t Loss: 1.037\n",
      "Epoch: 18 \t Loss: 1.033\n",
      "Epoch: 19 \t Loss: 1.034\n",
      "Epoch: 20 \t Loss: 1.020\n",
      "Epoch: 21 \t Loss: 1.039\n",
      "Epoch: 22 \t Loss: 1.031\n",
      "Epoch: 23 \t Loss: 1.025\n",
      "Epoch: 24 \t Loss: 1.032\n",
      "Time to train model: 495.502 s\n",
      "\n",
      "Train loss 1.0320653\n",
      "Method = 1\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.899 \t Loss: 1.083\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.899 \t Loss: 1.083\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.905 \t Test accuracy: 0.899 \t Loss: 1.083\n",
      "\n",
      "Using cnn data, regularization strength = 3.5\n",
      "Epoch: 0 \t Loss: 1.054\n",
      "Epoch: 1 \t Loss: 0.973\n",
      "Epoch: 2 \t Loss: 0.942\n",
      "Epoch: 3 \t Loss: 0.908\n",
      "Epoch: 4 \t Loss: 0.892\n",
      "Epoch: 5 \t Loss: 0.872\n",
      "Epoch: 6 \t Loss: 0.872\n",
      "Epoch: 7 \t Loss: 0.886\n",
      "Epoch: 8 \t Loss: 0.857\n",
      "Epoch: 9 \t Loss: 0.870\n",
      "Epoch: 10 \t Loss: 0.847\n",
      "Epoch: 11 \t Loss: 0.842\n",
      "Epoch: 12 \t Loss: 0.822\n",
      "Epoch: 13 \t Loss: 0.824\n",
      "Epoch: 14 \t Loss: 0.820\n",
      "Epoch: 15 \t Loss: 0.818\n",
      "Epoch: 16 \t Loss: 0.799\n",
      "Epoch: 17 \t Loss: 0.806\n",
      "Epoch: 18 \t Loss: 0.805\n",
      "Epoch: 19 \t Loss: 0.804\n",
      "Epoch: 20 \t Loss: 0.801\n",
      "Epoch: 21 \t Loss: 0.823\n",
      "Epoch: 22 \t Loss: 0.804\n",
      "Epoch: 23 \t Loss: 0.804\n",
      "Epoch: 24 \t Loss: 0.798\n",
      "Time to train model: 488.020 s\n",
      "\n",
      "Train loss 0.79821986\n",
      "Method = 1\n",
      "Train accuracy: 0.912 \t Test accuracy: 0.903 \t Loss: 0.859\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.912 \t Test accuracy: 0.903 \t Loss: 0.859\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.913 \t Test accuracy: 0.904 \t Loss: 0.859\n",
      "\n",
      "Using cnn data, regularization strength = 4.0\n",
      "Epoch: 0 \t Loss: 0.933\n",
      "Epoch: 1 \t Loss: 0.837\n",
      "Epoch: 2 \t Loss: 0.809\n",
      "Epoch: 3 \t Loss: 0.787\n",
      "Epoch: 4 \t Loss: 0.767\n",
      "Epoch: 5 \t Loss: 0.771\n",
      "Epoch: 6 \t Loss: 0.770\n",
      "Epoch: 7 \t Loss: 0.747\n",
      "Epoch: 8 \t Loss: 0.738\n",
      "Epoch: 9 \t Loss: 0.735\n",
      "Epoch: 10 \t Loss: 0.717\n",
      "Epoch: 11 \t Loss: 0.720\n",
      "Epoch: 12 \t Loss: 0.730\n",
      "Epoch: 13 \t Loss: 0.722\n",
      "Epoch: 14 \t Loss: 0.701\n",
      "Epoch: 15 \t Loss: 0.709\n",
      "Epoch: 16 \t Loss: 0.697\n",
      "Epoch: 17 \t Loss: 0.688\n",
      "Epoch: 18 \t Loss: 0.705\n",
      "Epoch: 19 \t Loss: 0.685\n",
      "Epoch: 20 \t Loss: 0.683\n",
      "Epoch: 21 \t Loss: 0.673\n",
      "Epoch: 22 \t Loss: 0.673\n",
      "Epoch: 23 \t Loss: 0.666\n",
      "Epoch: 24 \t Loss: 0.669\n",
      "Time to train model: 507.020 s\n",
      "\n",
      "Train loss 0.66863304\n",
      "Method = 1\n",
      "Train accuracy: 0.917 \t Test accuracy: 0.908 \t Loss: 0.734\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.917 \t Test accuracy: 0.908 \t Loss: 0.734\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.917 \t Test accuracy: 0.908 \t Loss: 0.734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various regularization strength\n",
    "\n",
    "for r in [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]:\n",
    "    print(\"Using cnn data, regularization strength = %.1f\" % (r))\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(reg_fn=lambda d: r ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    tree.train(data_train, cnn_probs, batch_size=256, n_epochs=25, show_progress=True)\n",
    "    \n",
    "    print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "    \n",
    "    print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "    \n",
    "    for predict_method in [1, 2, 3]:\n",
    "        print(\"Method = %d\" % (predict_method))\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=predict_method)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=predict_method)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.952\n",
      "Epoch: 1 \t Loss: 0.770\n",
      "Epoch: 2 \t Loss: 0.693\n",
      "Epoch: 3 \t Loss: 0.647\n",
      "Epoch: 4 \t Loss: 0.613\n",
      "Epoch: 5 \t Loss: 0.589\n",
      "Epoch: 6 \t Loss: 0.570\n",
      "Epoch: 7 \t Loss: 0.554\n",
      "Epoch: 8 \t Loss: 0.542\n",
      "Epoch: 9 \t Loss: 0.530\n",
      "Epoch: 10 \t Loss: 0.521\n",
      "Epoch: 11 \t Loss: 0.512\n",
      "Epoch: 12 \t Loss: 0.505\n",
      "Epoch: 13 \t Loss: 0.497\n",
      "Epoch: 14 \t Loss: 0.491\n",
      "Epoch: 15 \t Loss: 0.486\n",
      "Epoch: 16 \t Loss: 0.481\n",
      "Epoch: 17 \t Loss: 0.476\n",
      "Epoch: 18 \t Loss: 0.471\n",
      "Epoch: 19 \t Loss: 0.467\n",
      "Epoch: 20 \t Loss: 0.463\n",
      "Epoch: 21 \t Loss: 0.460\n",
      "Epoch: 22 \t Loss: 0.457\n",
      "Epoch: 23 \t Loss: 0.453\n",
      "Epoch: 24 \t Loss: 0.451\n",
      "Epoch: 25 \t Loss: 0.448\n",
      "Epoch: 26 \t Loss: 0.445\n",
      "Epoch: 27 \t Loss: 0.443\n",
      "Epoch: 28 \t Loss: 0.441\n",
      "Epoch: 29 \t Loss: 0.439\n",
      "Epoch: 30 \t Loss: 0.436\n",
      "Epoch: 31 \t Loss: 0.435\n",
      "Epoch: 32 \t Loss: 0.433\n",
      "Epoch: 33 \t Loss: 0.431\n",
      "Epoch: 34 \t Loss: 0.429\n",
      "Epoch: 35 \t Loss: 0.428\n",
      "Epoch: 36 \t Loss: 0.426\n",
      "Epoch: 37 \t Loss: 0.425\n",
      "Epoch: 38 \t Loss: 0.424\n",
      "Epoch: 39 \t Loss: 0.423\n",
      "Epoch: 40 \t Loss: 0.421\n",
      "Epoch: 41 \t Loss: 0.420\n",
      "Epoch: 42 \t Loss: 0.419\n",
      "Epoch: 43 \t Loss: 0.418\n",
      "Epoch: 44 \t Loss: 0.417\n",
      "Epoch: 45 \t Loss: 0.416\n",
      "Epoch: 46 \t Loss: 0.414\n",
      "Epoch: 47 \t Loss: 0.413\n",
      "Epoch: 48 \t Loss: 0.413\n",
      "Epoch: 49 \t Loss: 0.412\n",
      "Time to train model: 934.401 s\n",
      "\n",
      "Train loss 0.41164118\n",
      "Method = 1\n",
      "Train accuracy: 0.989 \t Test accuracy: 0.947 \t Loss: 0.684\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.989 \t Test accuracy: 0.948 \t Loss: 0.684\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.989 \t Test accuracy: 0.951 \t Loss: 0.684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model using the raw data\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "tree.train(data_train, labels_train_one_hot, batch_size=256, n_epochs=50, show_progress=True)\n",
    "\n",
    "print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "\n",
    "print(\"Train loss\", tree.compute_loss(data_train, labels_train_one_hot))\n",
    "\n",
    "for predict_method in [1, 2, 3]:\n",
    "    print(\"Method = %d\" % (predict_method))\n",
    "\n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=predict_method)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=predict_method)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.947\n",
      "Epoch: 1 \t Loss: 0.764\n",
      "Epoch: 2 \t Loss: 0.687\n",
      "Epoch: 3 \t Loss: 0.642\n",
      "Epoch: 4 \t Loss: 0.609\n",
      "Epoch: 5 \t Loss: 0.586\n",
      "Epoch: 6 \t Loss: 0.567\n",
      "Epoch: 7 \t Loss: 0.552\n",
      "Epoch: 8 \t Loss: 0.540\n",
      "Epoch: 9 \t Loss: 0.529\n",
      "Epoch: 10 \t Loss: 0.520\n",
      "Epoch: 11 \t Loss: 0.512\n",
      "Epoch: 12 \t Loss: 0.505\n",
      "Epoch: 13 \t Loss: 0.498\n",
      "Epoch: 14 \t Loss: 0.492\n",
      "Epoch: 15 \t Loss: 0.487\n",
      "Epoch: 16 \t Loss: 0.482\n",
      "Epoch: 17 \t Loss: 0.478\n",
      "Epoch: 18 \t Loss: 0.474\n",
      "Epoch: 19 \t Loss: 0.470\n",
      "Epoch: 20 \t Loss: 0.467\n",
      "Epoch: 21 \t Loss: 0.464\n",
      "Epoch: 22 \t Loss: 0.461\n",
      "Epoch: 23 \t Loss: 0.458\n",
      "Epoch: 24 \t Loss: 0.455\n",
      "Epoch: 25 \t Loss: 0.452\n",
      "Epoch: 26 \t Loss: 0.450\n",
      "Epoch: 27 \t Loss: 0.448\n",
      "Epoch: 28 \t Loss: 0.446\n",
      "Epoch: 29 \t Loss: 0.444\n",
      "Epoch: 30 \t Loss: 0.442\n",
      "Epoch: 31 \t Loss: 0.441\n",
      "Epoch: 32 \t Loss: 0.439\n",
      "Epoch: 33 \t Loss: 0.438\n",
      "Epoch: 34 \t Loss: 0.436\n",
      "Epoch: 35 \t Loss: 0.435\n",
      "Epoch: 36 \t Loss: 0.433\n",
      "Epoch: 37 \t Loss: 0.432\n",
      "Epoch: 38 \t Loss: 0.431\n",
      "Epoch: 39 \t Loss: 0.430\n",
      "Epoch: 40 \t Loss: 0.429\n",
      "Epoch: 41 \t Loss: 0.428\n",
      "Epoch: 42 \t Loss: 0.427\n",
      "Epoch: 43 \t Loss: 0.426\n",
      "Epoch: 44 \t Loss: 0.425\n",
      "Epoch: 45 \t Loss: 0.424\n",
      "Epoch: 46 \t Loss: 0.423\n",
      "Epoch: 47 \t Loss: 0.422\n",
      "Epoch: 48 \t Loss: 0.422\n",
      "Epoch: 49 \t Loss: 0.421\n",
      "Time to train model: 941.635 s\n",
      "\n",
      "Train loss 0.4208257\n",
      "Method = 1\n",
      "Train accuracy: 0.987 \t Test accuracy: 0.949 \t Loss: 0.651\n",
      "\n",
      "Method = 2\n",
      "Train accuracy: 0.987 \t Test accuracy: 0.951 \t Loss: 0.651\n",
      "\n",
      "Method = 3\n",
      "Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model using the probabilities generated by the CNN\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "tree.train(data_train, cnn_probs, batch_size=256, n_epochs=50, show_progress=True)\n",
    "\n",
    "print(\"Time to train model: %.3f s\\n\" % (time.time() - t))\n",
    "\n",
    "print(\"Train loss\", tree.compute_loss(data_train, cnn_probs))\n",
    "\n",
    "for predict_method in [1, 2, 3]:\n",
    "    print(\"Method = %d\" % (predict_method))\n",
    "\n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=predict_method)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=predict_method)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.892 \t Test accuracy: 0.896 \t Loss: 1.021\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.927 \t Test accuracy: 0.922 \t Loss: 0.839\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.936 \t Test accuracy: 0.929 \t Loss: 0.773\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.943 \t Test accuracy: 0.933 \t Loss: 0.740\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.947 \t Test accuracy: 0.935 \t Loss: 0.717\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.949 \t Test accuracy: 0.937 \t Loss: 0.702\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.952 \t Test accuracy: 0.940 \t Loss: 0.693\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.954 \t Test accuracy: 0.941 \t Loss: 0.684\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.956 \t Test accuracy: 0.940 \t Loss: 0.680\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.958 \t Test accuracy: 0.942 \t Loss: 0.675\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.960 \t Test accuracy: 0.943 \t Loss: 0.673\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.961 \t Test accuracy: 0.943 \t Loss: 0.672\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.962 \t Test accuracy: 0.943 \t Loss: 0.669\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.963 \t Test accuracy: 0.944 \t Loss: 0.668\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.963 \t Test accuracy: 0.944 \t Loss: 0.666\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.964 \t Test accuracy: 0.943 \t Loss: 0.666\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.965 \t Test accuracy: 0.946 \t Loss: 0.664\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.965 \t Test accuracy: 0.943 \t Loss: 0.665\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.966 \t Test accuracy: 0.944 \t Loss: 0.665\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.966 \t Test accuracy: 0.944 \t Loss: 0.663\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.967 \t Test accuracy: 0.944 \t Loss: 0.663\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.967 \t Test accuracy: 0.944 \t Loss: 0.662\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.968 \t Test accuracy: 0.943 \t Loss: 0.665\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.969 \t Test accuracy: 0.944 \t Loss: 0.664\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.968 \t Test accuracy: 0.944 \t Loss: 0.664\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.969 \t Test accuracy: 0.943 \t Loss: 0.665\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.969 \t Test accuracy: 0.943 \t Loss: 0.666\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.970 \t Test accuracy: 0.944 \t Loss: 0.665\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.970 \t Test accuracy: 0.943 \t Loss: 0.667\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.970 \t Test accuracy: 0.944 \t Loss: 0.664\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.971 \t Test accuracy: 0.943 \t Loss: 0.666\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.970 \t Test accuracy: 0.943 \t Loss: 0.667\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.970 \t Test accuracy: 0.944 \t Loss: 0.667\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.970 \t Test accuracy: 0.945 \t Loss: 0.665\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.970 \t Test accuracy: 0.944 \t Loss: 0.668\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.971 \t Test accuracy: 0.944 \t Loss: 0.669\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.971 \t Test accuracy: 0.944 \t Loss: 0.671\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.971 \t Test accuracy: 0.944 \t Loss: 0.669\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.972 \t Test accuracy: 0.943 \t Loss: 0.671\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.972 \t Test accuracy: 0.943 \t Loss: 0.673\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.972 \t Test accuracy: 0.944 \t Loss: 0.672\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.972 \t Test accuracy: 0.942 \t Loss: 0.674\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.972 \t Test accuracy: 0.943 \t Loss: 0.674\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.972 \t Test accuracy: 0.944 \t Loss: 0.675\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.972 \t Test accuracy: 0.943 \t Loss: 0.675\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.972 \t Test accuracy: 0.944 \t Loss: 0.677\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.973 \t Test accuracy: 0.944 \t Loss: 0.677\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.973 \t Test accuracy: 0.943 \t Loss: 0.678\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.973 \t Test accuracy: 0.943 \t Loss: 0.678\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.973 \t Test accuracy: 0.943 \t Loss: 0.678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 7,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, cnn_probs, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=3)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=3)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.794\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.942 \t Loss: 0.730\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.946 \t Loss: 0.693\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.672\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.965 \t Test accuracy: 0.950 \t Loss: 0.656\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.952 \t Loss: 0.644\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.634\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.957 \t Loss: 0.630\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.626\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.959 \t Loss: 0.618\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.979 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.959 \t Loss: 0.617\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.959 \t Loss: 0.621\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.626\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.630\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.631\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.633\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.634\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.634\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.633\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.634\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.636\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.638\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.638\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.640\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.641\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.642\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.644\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.646\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.646\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.646\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.647\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.647\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.986 \t Test accuracy: 0.954 \t Loss: 0.649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, cnn_probs, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=3)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=3)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.918 \t Test accuracy: 0.922 \t Loss: 0.923\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.944 \t Test accuracy: 0.941 \t Loss: 0.768\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.954 \t Test accuracy: 0.946 \t Loss: 0.705\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.961 \t Test accuracy: 0.952 \t Loss: 0.672\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.967 \t Test accuracy: 0.954 \t Loss: 0.650\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.635\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.973 \t Test accuracy: 0.957 \t Loss: 0.623\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.976 \t Test accuracy: 0.958 \t Loss: 0.614\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.978 \t Test accuracy: 0.959 \t Loss: 0.608\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.979 \t Test accuracy: 0.959 \t Loss: 0.602\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.980 \t Test accuracy: 0.961 \t Loss: 0.600\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.982 \t Test accuracy: 0.961 \t Loss: 0.596\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.982 \t Test accuracy: 0.962 \t Loss: 0.594\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.983 \t Test accuracy: 0.961 \t Loss: 0.592\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.984 \t Test accuracy: 0.962 \t Loss: 0.590\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.984 \t Test accuracy: 0.961 \t Loss: 0.590\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.985 \t Test accuracy: 0.961 \t Loss: 0.592\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.985 \t Test accuracy: 0.961 \t Loss: 0.593\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.986 \t Test accuracy: 0.961 \t Loss: 0.594\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.986 \t Test accuracy: 0.962 \t Loss: 0.593\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.986 \t Test accuracy: 0.961 \t Loss: 0.596\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.987 \t Test accuracy: 0.962 \t Loss: 0.596\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.987 \t Test accuracy: 0.962 \t Loss: 0.599\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.987 \t Test accuracy: 0.963 \t Loss: 0.599\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.987 \t Test accuracy: 0.961 \t Loss: 0.601\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.988 \t Test accuracy: 0.963 \t Loss: 0.599\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.988 \t Test accuracy: 0.962 \t Loss: 0.602\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.988 \t Test accuracy: 0.962 \t Loss: 0.603\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.988 \t Test accuracy: 0.961 \t Loss: 0.605\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.988 \t Test accuracy: 0.962 \t Loss: 0.605\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.989 \t Test accuracy: 0.962 \t Loss: 0.605\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.989 \t Test accuracy: 0.961 \t Loss: 0.608\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.989 \t Test accuracy: 0.961 \t Loss: 0.610\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.989 \t Test accuracy: 0.962 \t Loss: 0.611\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.989 \t Test accuracy: 0.962 \t Loss: 0.609\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.990 \t Test accuracy: 0.962 \t Loss: 0.613\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.990 \t Test accuracy: 0.962 \t Loss: 0.615\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.618\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.619\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.621\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.621\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.623\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.625\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.990 \t Test accuracy: 0.961 \t Loss: 0.625\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.990 \t Test accuracy: 0.960 \t Loss: 0.628\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.991 \t Test accuracy: 0.961 \t Loss: 0.629\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.991 \t Test accuracy: 0.960 \t Loss: 0.631\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.991 \t Test accuracy: 0.959 \t Loss: 0.632\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.991 \t Test accuracy: 0.960 \t Loss: 0.633\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.991 \t Test accuracy: 0.959 \t Loss: 0.636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 9,\n",
    "                        learning_rate = 0.01,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, cnn_probs, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=3)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=3)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, depth = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method SoftDecisionTree.__del__ of <__main__.SoftDecisionTree object at 0x7f510f145ac8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-590cfb14dd5a>\", line 246, in __del__\n",
      "AttributeError: 'SoftDecisionTree' object has no attribute 'sess'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.879 \t Test accuracy: 0.877 \t Loss: 1.095\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.914 \t Test accuracy: 0.908 \t Loss: 0.899\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.924 \t Test accuracy: 0.917 \t Loss: 0.828\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.931 \t Test accuracy: 0.920 \t Loss: 0.792\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.934 \t Test accuracy: 0.923 \t Loss: 0.773\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.938 \t Test accuracy: 0.927 \t Loss: 0.758\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.940 \t Test accuracy: 0.927 \t Loss: 0.749\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.942 \t Test accuracy: 0.929 \t Loss: 0.740\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.943 \t Test accuracy: 0.929 \t Loss: 0.735\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.944 \t Test accuracy: 0.929 \t Loss: 0.730\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.944 \t Test accuracy: 0.928 \t Loss: 0.725\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.945 \t Test accuracy: 0.930 \t Loss: 0.722\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.947 \t Test accuracy: 0.931 \t Loss: 0.720\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.947 \t Test accuracy: 0.930 \t Loss: 0.716\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.948 \t Test accuracy: 0.930 \t Loss: 0.715\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.948 \t Test accuracy: 0.932 \t Loss: 0.714\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.949 \t Test accuracy: 0.931 \t Loss: 0.713\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.949 \t Test accuracy: 0.931 \t Loss: 0.710\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.950 \t Test accuracy: 0.930 \t Loss: 0.710\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.951 \t Test accuracy: 0.931 \t Loss: 0.709\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.951 \t Test accuracy: 0.932 \t Loss: 0.708\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.952 \t Test accuracy: 0.933 \t Loss: 0.707\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.952 \t Test accuracy: 0.930 \t Loss: 0.707\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.953 \t Test accuracy: 0.932 \t Loss: 0.706\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.953 \t Test accuracy: 0.931 \t Loss: 0.705\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.953 \t Test accuracy: 0.932 \t Loss: 0.706\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.953 \t Test accuracy: 0.933 \t Loss: 0.706\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.954 \t Test accuracy: 0.932 \t Loss: 0.705\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.954 \t Test accuracy: 0.933 \t Loss: 0.705\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.954 \t Test accuracy: 0.933 \t Loss: 0.705\n",
      "\n",
      "Using cnn data, depth = 7\n",
      "Epoch: 0 \t Train accuracy: 0.892 \t Test accuracy: 0.896 \t Loss: 1.021\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.927 \t Test accuracy: 0.922 \t Loss: 0.838\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.937 \t Test accuracy: 0.929 \t Loss: 0.774\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.943 \t Test accuracy: 0.932 \t Loss: 0.740\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.947 \t Test accuracy: 0.934 \t Loss: 0.717\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.949 \t Test accuracy: 0.938 \t Loss: 0.702\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.953 \t Test accuracy: 0.940 \t Loss: 0.693\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.954 \t Test accuracy: 0.941 \t Loss: 0.684\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.957 \t Test accuracy: 0.941 \t Loss: 0.680\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.958 \t Test accuracy: 0.942 \t Loss: 0.674\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.960 \t Test accuracy: 0.944 \t Loss: 0.673\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.962 \t Test accuracy: 0.944 \t Loss: 0.670\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.963 \t Test accuracy: 0.943 \t Loss: 0.668\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.963 \t Test accuracy: 0.943 \t Loss: 0.668\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.964 \t Test accuracy: 0.943 \t Loss: 0.666\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.964 \t Test accuracy: 0.944 \t Loss: 0.664\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.965 \t Test accuracy: 0.944 \t Loss: 0.663\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.966 \t Test accuracy: 0.944 \t Loss: 0.663\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.966 \t Test accuracy: 0.946 \t Loss: 0.663\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.966 \t Test accuracy: 0.945 \t Loss: 0.661\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.966 \t Test accuracy: 0.945 \t Loss: 0.660\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.967 \t Test accuracy: 0.946 \t Loss: 0.658\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.967 \t Test accuracy: 0.946 \t Loss: 0.662\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.968 \t Test accuracy: 0.946 \t Loss: 0.660\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.968 \t Test accuracy: 0.945 \t Loss: 0.661\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.969 \t Test accuracy: 0.946 \t Loss: 0.659\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.969 \t Test accuracy: 0.946 \t Loss: 0.660\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.969 \t Test accuracy: 0.947 \t Loss: 0.658\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.970 \t Test accuracy: 0.946 \t Loss: 0.660\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.970 \t Test accuracy: 0.945 \t Loss: 0.661\n",
      "\n",
      "Using cnn data, depth = 8\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.937 \t Test accuracy: 0.934 \t Loss: 0.794\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.950 \t Test accuracy: 0.941 \t Loss: 0.731\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.957 \t Test accuracy: 0.946 \t Loss: 0.694\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.672\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.965 \t Test accuracy: 0.951 \t Loss: 0.655\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.644\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.635\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.631\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.628\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.623\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.618\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.629\n",
      "\n",
      "Using cnn data, depth = 9\n",
      "Epoch: 0 \t Train accuracy: 0.918 \t Test accuracy: 0.922 \t Loss: 0.923\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.944 \t Test accuracy: 0.941 \t Loss: 0.767\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.955 \t Test accuracy: 0.948 \t Loss: 0.705\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.962 \t Test accuracy: 0.951 \t Loss: 0.671\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.967 \t Test accuracy: 0.954 \t Loss: 0.649\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.971 \t Test accuracy: 0.956 \t Loss: 0.633\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.974 \t Test accuracy: 0.959 \t Loss: 0.620\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.976 \t Test accuracy: 0.961 \t Loss: 0.612\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.978 \t Test accuracy: 0.961 \t Loss: 0.605\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.979 \t Test accuracy: 0.961 \t Loss: 0.600\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.980 \t Test accuracy: 0.961 \t Loss: 0.596\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.981 \t Test accuracy: 0.962 \t Loss: 0.594\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.982 \t Test accuracy: 0.964 \t Loss: 0.591\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.983 \t Test accuracy: 0.963 \t Loss: 0.591\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.984 \t Test accuracy: 0.965 \t Loss: 0.589\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.984 \t Test accuracy: 0.965 \t Loss: 0.587\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.985 \t Test accuracy: 0.965 \t Loss: 0.589\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.986 \t Test accuracy: 0.964 \t Loss: 0.591\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.986 \t Test accuracy: 0.964 \t Loss: 0.589\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.987 \t Test accuracy: 0.964 \t Loss: 0.590\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.987 \t Test accuracy: 0.964 \t Loss: 0.593\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.987 \t Test accuracy: 0.964 \t Loss: 0.591\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.988 \t Test accuracy: 0.963 \t Loss: 0.594\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.988 \t Test accuracy: 0.963 \t Loss: 0.594\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \t Train accuracy: 0.988 \t Test accuracy: 0.964 \t Loss: 0.596\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.988 \t Test accuracy: 0.964 \t Loss: 0.596\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.989 \t Test accuracy: 0.963 \t Loss: 0.597\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.989 \t Test accuracy: 0.963 \t Loss: 0.596\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.989 \t Test accuracy: 0.964 \t Loss: 0.599\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.989 \t Test accuracy: 0.962 \t Loss: 0.600\n",
      "\n",
      "Using cnn data, depth = 10\n",
      "Epoch: 0 \t Train accuracy: 0.924 \t Test accuracy: 0.926 \t Loss: 0.883\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.949 \t Test accuracy: 0.945 \t Loss: 0.735\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.960 \t Test accuracy: 0.951 \t Loss: 0.677\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.967 \t Test accuracy: 0.956 \t Loss: 0.646\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.972 \t Test accuracy: 0.960 \t Loss: 0.627\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.975 \t Test accuracy: 0.961 \t Loss: 0.613\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.978 \t Test accuracy: 0.963 \t Loss: 0.604\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.981 \t Test accuracy: 0.964 \t Loss: 0.596\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.983 \t Test accuracy: 0.964 \t Loss: 0.591\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.985 \t Test accuracy: 0.966 \t Loss: 0.587\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.986 \t Test accuracy: 0.966 \t Loss: 0.585\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.987 \t Test accuracy: 0.966 \t Loss: 0.585\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.988 \t Test accuracy: 0.965 \t Loss: 0.584\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.989 \t Test accuracy: 0.967 \t Loss: 0.581\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.989 \t Test accuracy: 0.966 \t Loss: 0.581\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.989 \t Test accuracy: 0.966 \t Loss: 0.582\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.990 \t Test accuracy: 0.965 \t Loss: 0.585\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.990 \t Test accuracy: 0.965 \t Loss: 0.585\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.991 \t Test accuracy: 0.966 \t Loss: 0.587\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.991 \t Test accuracy: 0.965 \t Loss: 0.588\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.991 \t Test accuracy: 0.966 \t Loss: 0.590\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.992 \t Test accuracy: 0.965 \t Loss: 0.590\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.992 \t Test accuracy: 0.966 \t Loss: 0.591\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.992 \t Test accuracy: 0.966 \t Loss: 0.590\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.992 \t Test accuracy: 0.966 \t Loss: 0.593\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.992 \t Test accuracy: 0.966 \t Loss: 0.592\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.992 \t Test accuracy: 0.965 \t Loss: 0.593\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.993 \t Test accuracy: 0.966 \t Loss: 0.595\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.993 \t Test accuracy: 0.966 \t Loss: 0.595\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.993 \t Test accuracy: 0.966 \t Loss: 0.595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various depths\n",
    "\n",
    "for depth in [6, 7, 8, 9, 10]:\n",
    "    print(\"Using cnn data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            learning_rate = 0.01,\n",
    "                            inverse_temperature = 0.1,\n",
    "                            reg_fn = lambda d: 4 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=3)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=3)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, temperature = 0.01\n",
      "Epoch: 0 \t Train accuracy: 0.740 \t Test accuracy: 0.734 \t Loss: 1.760\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.867 \t Test accuracy: 0.866 \t Loss: 1.326\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.900 \t Test accuracy: 0.899 \t Loss: 1.130\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.915 \t Test accuracy: 0.912 \t Loss: 1.023\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.924 \t Test accuracy: 0.922 \t Loss: 0.953\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.931 \t Test accuracy: 0.928 \t Loss: 0.903\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.936 \t Test accuracy: 0.932 \t Loss: 0.865\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.940 \t Test accuracy: 0.935 \t Loss: 0.835\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.943 \t Test accuracy: 0.937 \t Loss: 0.811\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.945 \t Test accuracy: 0.939 \t Loss: 0.791\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.947 \t Test accuracy: 0.940 \t Loss: 0.775\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.949 \t Test accuracy: 0.942 \t Loss: 0.760\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.951 \t Test accuracy: 0.944 \t Loss: 0.747\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.952 \t Test accuracy: 0.945 \t Loss: 0.737\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.954 \t Test accuracy: 0.946 \t Loss: 0.728\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.955 \t Test accuracy: 0.947 \t Loss: 0.719\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.956 \t Test accuracy: 0.948 \t Loss: 0.711\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.704\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.958 \t Test accuracy: 0.949 \t Loss: 0.698\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.959 \t Test accuracy: 0.951 \t Loss: 0.693\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.960 \t Test accuracy: 0.951 \t Loss: 0.688\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.961 \t Test accuracy: 0.951 \t Loss: 0.683\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.961 \t Test accuracy: 0.952 \t Loss: 0.678\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.962 \t Test accuracy: 0.952 \t Loss: 0.674\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.963 \t Test accuracy: 0.952 \t Loss: 0.671\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.963 \t Test accuracy: 0.953 \t Loss: 0.667\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.964 \t Test accuracy: 0.954 \t Loss: 0.664\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.965 \t Test accuracy: 0.954 \t Loss: 0.660\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.965 \t Test accuracy: 0.955 \t Loss: 0.658\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.965 \t Test accuracy: 0.955 \t Loss: 0.655\n",
      "\n",
      "Using cnn data, temperature = 0.05\n",
      "Epoch: 0 \t Train accuracy: 0.903 \t Test accuracy: 0.904 \t Loss: 1.059\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.934 \t Test accuracy: 0.931 \t Loss: 0.850\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.947 \t Test accuracy: 0.941 \t Loss: 0.771\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.956 \t Test accuracy: 0.949 \t Loss: 0.727\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.952 \t Loss: 0.697\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.964 \t Test accuracy: 0.953 \t Loss: 0.676\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.966 \t Test accuracy: 0.955 \t Loss: 0.663\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.968 \t Test accuracy: 0.956 \t Loss: 0.652\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.969 \t Test accuracy: 0.957 \t Loss: 0.644\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.971 \t Test accuracy: 0.957 \t Loss: 0.637\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.972 \t Test accuracy: 0.959 \t Loss: 0.632\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.973 \t Test accuracy: 0.958 \t Loss: 0.627\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.974 \t Test accuracy: 0.958 \t Loss: 0.624\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.975 \t Test accuracy: 0.959 \t Loss: 0.622\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.976 \t Test accuracy: 0.959 \t Loss: 0.620\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.977 \t Test accuracy: 0.958 \t Loss: 0.618\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.977 \t Test accuracy: 0.959 \t Loss: 0.615\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.978 \t Test accuracy: 0.960 \t Loss: 0.614\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.978 \t Test accuracy: 0.960 \t Loss: 0.612\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.979 \t Test accuracy: 0.961 \t Loss: 0.611\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.980 \t Test accuracy: 0.961 \t Loss: 0.611\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.980 \t Test accuracy: 0.961 \t Loss: 0.610\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.980 \t Test accuracy: 0.961 \t Loss: 0.609\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.981 \t Test accuracy: 0.962 \t Loss: 0.608\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.981 \t Test accuracy: 0.960 \t Loss: 0.608\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.981 \t Test accuracy: 0.961 \t Loss: 0.609\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.982 \t Test accuracy: 0.961 \t Loss: 0.608\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.982 \t Test accuracy: 0.961 \t Loss: 0.606\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.982 \t Test accuracy: 0.961 \t Loss: 0.608\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.983 \t Test accuracy: 0.961 \t Loss: 0.607\n",
      "\n",
      "Using cnn data, temperature = 0.10\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.937 \t Test accuracy: 0.934 \t Loss: 0.794\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.950 \t Test accuracy: 0.941 \t Loss: 0.731\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.957 \t Test accuracy: 0.946 \t Loss: 0.694\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.672\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.965 \t Test accuracy: 0.951 \t Loss: 0.655\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.644\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.635\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.631\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.628\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.623\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.618\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.629\n",
      "\n",
      "Using cnn data, temperature = 0.20\n",
      "Epoch: 0 \t Train accuracy: 0.898 \t Test accuracy: 0.896 \t Loss: 0.947\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.931 \t Test accuracy: 0.922 \t Loss: 0.793\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.945 \t Test accuracy: 0.934 \t Loss: 0.733\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.954 \t Test accuracy: 0.940 \t Loss: 0.702\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.958 \t Test accuracy: 0.942 \t Loss: 0.681\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.962 \t Test accuracy: 0.942 \t Loss: 0.668\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.965 \t Test accuracy: 0.943 \t Loss: 0.662\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.967 \t Test accuracy: 0.946 \t Loss: 0.655\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.969 \t Test accuracy: 0.947 \t Loss: 0.655\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.971 \t Test accuracy: 0.948 \t Loss: 0.652\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.972 \t Test accuracy: 0.949 \t Loss: 0.649\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.973 \t Test accuracy: 0.949 \t Loss: 0.645\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.975 \t Test accuracy: 0.949 \t Loss: 0.648\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.976 \t Test accuracy: 0.950 \t Loss: 0.646\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.977 \t Test accuracy: 0.950 \t Loss: 0.646\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.977 \t Test accuracy: 0.951 \t Loss: 0.645\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.978 \t Test accuracy: 0.950 \t Loss: 0.643\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.978 \t Test accuracy: 0.949 \t Loss: 0.647\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.979 \t Test accuracy: 0.949 \t Loss: 0.648\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.979 \t Test accuracy: 0.950 \t Loss: 0.649\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.980 \t Test accuracy: 0.951 \t Loss: 0.650\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.980 \t Test accuracy: 0.950 \t Loss: 0.653\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.980 \t Test accuracy: 0.949 \t Loss: 0.656\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Train accuracy: 0.981 \t Test accuracy: 0.948 \t Loss: 0.661\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.981 \t Test accuracy: 0.949 \t Loss: 0.663\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.981 \t Test accuracy: 0.950 \t Loss: 0.660\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.982 \t Test accuracy: 0.949 \t Loss: 0.665\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.982 \t Test accuracy: 0.950 \t Loss: 0.664\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.982 \t Test accuracy: 0.949 \t Loss: 0.667\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.982 \t Test accuracy: 0.950 \t Loss: 0.668\n",
      "\n",
      "Using cnn data, temperature = 0.40\n",
      "Epoch: 0 \t Train accuracy: 0.867 \t Test accuracy: 0.864 \t Loss: 0.998\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.913 \t Test accuracy: 0.902 \t Loss: 0.821\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.928 \t Test accuracy: 0.915 \t Loss: 0.759\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.940 \t Test accuracy: 0.925 \t Loss: 0.727\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.947 \t Test accuracy: 0.926 \t Loss: 0.707\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.952 \t Test accuracy: 0.931 \t Loss: 0.687\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.956 \t Test accuracy: 0.934 \t Loss: 0.682\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.959 \t Test accuracy: 0.936 \t Loss: 0.676\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.961 \t Test accuracy: 0.937 \t Loss: 0.670\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.962 \t Test accuracy: 0.939 \t Loss: 0.667\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.965 \t Test accuracy: 0.941 \t Loss: 0.659\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.966 \t Test accuracy: 0.941 \t Loss: 0.658\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.968 \t Test accuracy: 0.942 \t Loss: 0.656\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.969 \t Test accuracy: 0.946 \t Loss: 0.650\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.970 \t Test accuracy: 0.945 \t Loss: 0.651\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.971 \t Test accuracy: 0.942 \t Loss: 0.661\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.972 \t Test accuracy: 0.945 \t Loss: 0.652\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.972 \t Test accuracy: 0.944 \t Loss: 0.654\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.973 \t Test accuracy: 0.946 \t Loss: 0.651\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.974 \t Test accuracy: 0.946 \t Loss: 0.661\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.975 \t Test accuracy: 0.946 \t Loss: 0.656\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.975 \t Test accuracy: 0.945 \t Loss: 0.657\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.975 \t Test accuracy: 0.947 \t Loss: 0.661\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.976 \t Test accuracy: 0.944 \t Loss: 0.665\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.976 \t Test accuracy: 0.944 \t Loss: 0.668\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.976 \t Test accuracy: 0.944 \t Loss: 0.667\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.977 \t Test accuracy: 0.947 \t Loss: 0.662\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.977 \t Test accuracy: 0.946 \t Loss: 0.662\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.978 \t Test accuracy: 0.946 \t Loss: 0.666\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.978 \t Test accuracy: 0.944 \t Loss: 0.672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various inverse_temperature\n",
    "\n",
    "for temperature in [0.01, 0.05, 0.1, 0.2, 0.4]:\n",
    "    print(\"Using cnn data, temperature = %.2f\" % (temperature))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            learning_rate = 0.01,\n",
    "                            inverse_temperature = temperature,\n",
    "                            reg_fn = lambda d: 4 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=3)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=3)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, strength = 2\n",
      "Epoch: 0 \t Train accuracy: 0.913 \t Test accuracy: 0.910 \t Loss: 3.238\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.935 \t Test accuracy: 0.929 \t Loss: 2.972\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.944 \t Test accuracy: 0.934 \t Loss: 2.889\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.950 \t Test accuracy: 0.940 \t Loss: 2.850\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.954 \t Test accuracy: 0.939 \t Loss: 2.831\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.958 \t Test accuracy: 0.942 \t Loss: 2.814\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.960 \t Test accuracy: 0.943 \t Loss: 2.806\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.962 \t Test accuracy: 0.944 \t Loss: 2.797\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.963 \t Test accuracy: 0.945 \t Loss: 2.792\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.964 \t Test accuracy: 0.945 \t Loss: 2.785\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.965 \t Test accuracy: 0.947 \t Loss: 2.782\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.966 \t Test accuracy: 0.945 \t Loss: 2.779\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.966 \t Test accuracy: 0.946 \t Loss: 2.778\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.967 \t Test accuracy: 0.946 \t Loss: 2.776\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.968 \t Test accuracy: 0.947 \t Loss: 2.773\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.968 \t Test accuracy: 0.948 \t Loss: 2.771\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.969 \t Test accuracy: 0.947 \t Loss: 2.771\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.970 \t Test accuracy: 0.948 \t Loss: 2.772\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.970 \t Test accuracy: 0.949 \t Loss: 2.770\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.971 \t Test accuracy: 0.949 \t Loss: 2.768\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.971 \t Test accuracy: 0.949 \t Loss: 2.771\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.972 \t Test accuracy: 0.948 \t Loss: 2.771\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.972 \t Test accuracy: 0.949 \t Loss: 2.768\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.973 \t Test accuracy: 0.949 \t Loss: 2.769\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.973 \t Test accuracy: 0.950 \t Loss: 2.767\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.973 \t Test accuracy: 0.949 \t Loss: 2.768\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.973 \t Test accuracy: 0.948 \t Loss: 2.771\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.974 \t Test accuracy: 0.949 \t Loss: 2.766\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.974 \t Test accuracy: 0.948 \t Loss: 2.773\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.975 \t Test accuracy: 0.950 \t Loss: 2.771\n",
      "\n",
      "Using cnn data, strength = 3\n",
      "Epoch: 0 \t Train accuracy: 0.913 \t Test accuracy: 0.911 \t Loss: 1.316\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.940 \t Test accuracy: 0.937 \t Loss: 1.141\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 1.069\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.959 \t Test accuracy: 0.950 \t Loss: 1.027\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.963 \t Test accuracy: 0.952 \t Loss: 1.001\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.953 \t Loss: 0.986\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.954 \t Loss: 0.976\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.955 \t Loss: 0.966\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.956 \t Loss: 0.960\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.959\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.954\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.950\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.958 \t Loss: 0.949\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.958 \t Loss: 0.948\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.945\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.957 \t Loss: 0.946\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.979 \t Test accuracy: 0.956 \t Loss: 0.947\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.947\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.980 \t Test accuracy: 0.955 \t Loss: 0.946\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.947\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.946\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.947\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.947\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.947\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.948\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.949\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.956 \t Loss: 0.950\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.948\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.951\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.952\n",
      "\n",
      "Using cnn data, strength = 4\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.937 \t Test accuracy: 0.934 \t Loss: 0.794\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.950 \t Test accuracy: 0.941 \t Loss: 0.731\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.957 \t Test accuracy: 0.946 \t Loss: 0.694\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.672\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.965 \t Test accuracy: 0.951 \t Loss: 0.655\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.644\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.635\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.631\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.628\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.623\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.618\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.629\n",
      "\n",
      "Using cnn data, strength = 5\n",
      "Epoch: 0 \t Train accuracy: 0.909 \t Test accuracy: 0.906 \t Loss: 0.835\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.937 \t Test accuracy: 0.932 \t Loss: 0.669\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.949 \t Test accuracy: 0.941 \t Loss: 0.608\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.955 \t Test accuracy: 0.944 \t Loss: 0.574\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.960 \t Test accuracy: 0.947 \t Loss: 0.553\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.963 \t Test accuracy: 0.950 \t Loss: 0.538\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.965 \t Test accuracy: 0.950 \t Loss: 0.528\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.968 \t Test accuracy: 0.952 \t Loss: 0.520\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.515\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.511\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.507\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.506\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.975 \t Test accuracy: 0.955 \t Loss: 0.504\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.976 \t Test accuracy: 0.955 \t Loss: 0.505\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.502\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.502\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.978 \t Test accuracy: 0.954 \t Loss: 0.502\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.979 \t Test accuracy: 0.955 \t Loss: 0.504\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.979 \t Test accuracy: 0.954 \t Loss: 0.503\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.980 \t Test accuracy: 0.956 \t Loss: 0.502\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.980 \t Test accuracy: 0.955 \t Loss: 0.504\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.980 \t Test accuracy: 0.955 \t Loss: 0.504\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.505\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Train accuracy: 0.981 \t Test accuracy: 0.955 \t Loss: 0.506\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.981 \t Test accuracy: 0.955 \t Loss: 0.507\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.982 \t Test accuracy: 0.955 \t Loss: 0.507\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.982 \t Test accuracy: 0.954 \t Loss: 0.509\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.982 \t Test accuracy: 0.955 \t Loss: 0.510\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.983 \t Test accuracy: 0.954 \t Loss: 0.512\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.983 \t Test accuracy: 0.954 \t Loss: 0.512\n",
      "\n",
      "Using cnn data, strength = 6\n",
      "Epoch: 0 \t Train accuracy: 0.908 \t Test accuracy: 0.907 \t Loss: 0.774\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.937 \t Test accuracy: 0.931 \t Loss: 0.607\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.948 \t Test accuracy: 0.941 \t Loss: 0.546\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.955 \t Test accuracy: 0.945 \t Loss: 0.513\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.959 \t Test accuracy: 0.948 \t Loss: 0.492\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.963 \t Test accuracy: 0.949 \t Loss: 0.478\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.965 \t Test accuracy: 0.951 \t Loss: 0.468\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.967 \t Test accuracy: 0.951 \t Loss: 0.460\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.455\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.450\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.446\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.444\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.974 \t Test accuracy: 0.954 \t Loss: 0.442\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.975 \t Test accuracy: 0.954 \t Loss: 0.443\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.975 \t Test accuracy: 0.954 \t Loss: 0.441\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.976 \t Test accuracy: 0.954 \t Loss: 0.440\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.439\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.978 \t Test accuracy: 0.953 \t Loss: 0.441\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.978 \t Test accuracy: 0.954 \t Loss: 0.441\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.979 \t Test accuracy: 0.954 \t Loss: 0.439\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.979 \t Test accuracy: 0.953 \t Loss: 0.443\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.979 \t Test accuracy: 0.954 \t Loss: 0.442\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.980 \t Test accuracy: 0.954 \t Loss: 0.442\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.980 \t Test accuracy: 0.954 \t Loss: 0.444\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.981 \t Test accuracy: 0.954 \t Loss: 0.444\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.981 \t Test accuracy: 0.954 \t Loss: 0.443\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.981 \t Test accuracy: 0.953 \t Loss: 0.447\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.981 \t Test accuracy: 0.955 \t Loss: 0.447\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.981 \t Test accuracy: 0.954 \t Loss: 0.450\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.982 \t Test accuracy: 0.954 \t Loss: 0.449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various regularization strengths\n",
    "\n",
    "for strength in [2, 3, 4, 5, 6]:\n",
    "    print(\"Using cnn data, strength = %d\" % (strength))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            learning_rate = 0.01,\n",
    "                            inverse_temperature = 0.1,\n",
    "                            reg_fn = lambda d: strength ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=3)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=3)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, learning rate = 0.001\n",
      "Epoch: 0 \t Train accuracy: 0.589 \t Test accuracy: 0.596 \t Loss: 2.285\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.709 \t Test accuracy: 0.709 \t Loss: 1.862\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.785 \t Test accuracy: 0.787 \t Loss: 1.592\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.839 \t Test accuracy: 0.842 \t Loss: 1.394\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.872 \t Test accuracy: 0.873 \t Loss: 1.252\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.889 \t Test accuracy: 0.890 \t Loss: 1.150\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.900 \t Test accuracy: 0.901 \t Loss: 1.076\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.907 \t Test accuracy: 0.908 \t Loss: 1.021\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.913 \t Test accuracy: 0.912 \t Loss: 0.978\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.917 \t Test accuracy: 0.917 \t Loss: 0.944\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.921 \t Test accuracy: 0.919 \t Loss: 0.915\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.925 \t Test accuracy: 0.923 \t Loss: 0.891\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.928 \t Test accuracy: 0.926 \t Loss: 0.870\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.931 \t Test accuracy: 0.928 \t Loss: 0.853\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.934 \t Test accuracy: 0.930 \t Loss: 0.837\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.936 \t Test accuracy: 0.931 \t Loss: 0.823\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.938 \t Test accuracy: 0.934 \t Loss: 0.810\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.941 \t Test accuracy: 0.935 \t Loss: 0.798\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.943 \t Test accuracy: 0.937 \t Loss: 0.788\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.944 \t Test accuracy: 0.937 \t Loss: 0.779\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.946 \t Test accuracy: 0.939 \t Loss: 0.771\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.947 \t Test accuracy: 0.939 \t Loss: 0.763\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.948 \t Test accuracy: 0.941 \t Loss: 0.756\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.949 \t Test accuracy: 0.941 \t Loss: 0.750\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.950 \t Test accuracy: 0.941 \t Loss: 0.744\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.951 \t Test accuracy: 0.942 \t Loss: 0.738\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.952 \t Test accuracy: 0.943 \t Loss: 0.733\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.953 \t Test accuracy: 0.943 \t Loss: 0.728\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.954 \t Test accuracy: 0.943 \t Loss: 0.723\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.955 \t Test accuracy: 0.944 \t Loss: 0.718\n",
      "\n",
      "Using cnn data, learning rate = 0.005\n",
      "Epoch: 0 \t Train accuracy: 0.859 \t Test accuracy: 0.864 \t Loss: 1.273\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.915 \t Test accuracy: 0.915 \t Loss: 0.952\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.930 \t Test accuracy: 0.927 \t Loss: 0.845\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.941 \t Test accuracy: 0.936 \t Loss: 0.790\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.948 \t Test accuracy: 0.940 \t Loss: 0.753\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.952 \t Test accuracy: 0.944 \t Loss: 0.725\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.956 \t Test accuracy: 0.946 \t Loss: 0.707\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.959 \t Test accuracy: 0.948 \t Loss: 0.692\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.680\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.964 \t Test accuracy: 0.949 \t Loss: 0.670\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.966 \t Test accuracy: 0.951 \t Loss: 0.660\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.968 \t Test accuracy: 0.951 \t Loss: 0.654\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.649\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.644\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.640\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.636\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.634\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.974 \t Test accuracy: 0.955 \t Loss: 0.631\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.630\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.626\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.977 \t Test accuracy: 0.956 \t Loss: 0.624\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.977 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.979 \t Test accuracy: 0.959 \t Loss: 0.621\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Using cnn data, learning rate = 0.010\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.937 \t Test accuracy: 0.934 \t Loss: 0.794\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.950 \t Test accuracy: 0.941 \t Loss: 0.731\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.957 \t Test accuracy: 0.946 \t Loss: 0.694\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.672\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.965 \t Test accuracy: 0.951 \t Loss: 0.655\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.644\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.635\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.631\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.628\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.623\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.618\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.629\n",
      "\n",
      "Using cnn data, learning rate = 0.020\n",
      "Epoch: 0 \t Train accuracy: 0.932 \t Test accuracy: 0.928 \t Loss: 0.808\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.953 \t Test accuracy: 0.944 \t Loss: 0.703\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.963 \t Test accuracy: 0.952 \t Loss: 0.653\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.967 \t Test accuracy: 0.955 \t Loss: 0.632\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.971 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.974 \t Test accuracy: 0.958 \t Loss: 0.618\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.976 \t Test accuracy: 0.959 \t Loss: 0.612\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.977 \t Test accuracy: 0.959 \t Loss: 0.610\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.979 \t Test accuracy: 0.960 \t Loss: 0.611\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.980 \t Test accuracy: 0.959 \t Loss: 0.613\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.981 \t Test accuracy: 0.959 \t Loss: 0.612\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.981 \t Test accuracy: 0.961 \t Loss: 0.611\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.982 \t Test accuracy: 0.961 \t Loss: 0.610\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.983 \t Test accuracy: 0.960 \t Loss: 0.612\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.983 \t Test accuracy: 0.960 \t Loss: 0.611\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.984 \t Test accuracy: 0.960 \t Loss: 0.613\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.984 \t Test accuracy: 0.961 \t Loss: 0.615\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.985 \t Test accuracy: 0.960 \t Loss: 0.615\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.985 \t Test accuracy: 0.959 \t Loss: 0.617\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.985 \t Test accuracy: 0.960 \t Loss: 0.619\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.985 \t Test accuracy: 0.959 \t Loss: 0.622\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.986 \t Test accuracy: 0.959 \t Loss: 0.620\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.986 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Train accuracy: 0.986 \t Test accuracy: 0.958 \t Loss: 0.627\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.986 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.630\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.632\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.637\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.638\n",
      "\n",
      "Using cnn data, learning rate = 0.050\n",
      "Epoch: 0 \t Train accuracy: 0.947 \t Test accuracy: 0.935 \t Loss: 0.709\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.959 \t Test accuracy: 0.948 \t Loss: 0.647\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.967 \t Test accuracy: 0.947 \t Loss: 0.633\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.620\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.973 \t Test accuracy: 0.953 \t Loss: 0.614\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.975 \t Test accuracy: 0.954 \t Loss: 0.613\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.976 \t Test accuracy: 0.954 \t Loss: 0.616\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.616\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.979 \t Test accuracy: 0.955 \t Loss: 0.614\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.979 \t Test accuracy: 0.953 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.979 \t Test accuracy: 0.956 \t Loss: 0.610\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.981 \t Test accuracy: 0.955 \t Loss: 0.624\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.617\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.982 \t Test accuracy: 0.953 \t Loss: 0.629\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.624\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.982 \t Test accuracy: 0.955 \t Loss: 0.629\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.983 \t Test accuracy: 0.954 \t Loss: 0.631\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.983 \t Test accuracy: 0.954 \t Loss: 0.639\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.983 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.983 \t Test accuracy: 0.955 \t Loss: 0.631\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.984 \t Test accuracy: 0.954 \t Loss: 0.639\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.632\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.984 \t Test accuracy: 0.954 \t Loss: 0.635\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.954 \t Loss: 0.634\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.633\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.641\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.635\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.955 \t Loss: 0.640\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.954 \t Loss: 0.634\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.953 \t Loss: 0.644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various learning rates\n",
    "\n",
    "for rate in [0.001, 0.005, 0.01, 0.02, 0.05]:\n",
    "    print(\"Using cnn data, learning rate = %.3f\" % (rate))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            learning_rate = rate,\n",
    "                            inverse_temperature = 0.1,\n",
    "                            reg_fn = lambda d: 4 ** -d)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=3)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=3)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
