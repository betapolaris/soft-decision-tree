{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "\n",
    "# Function to set random seed\n",
    "def set_random_seed(seed=42):\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic function for a numpy array\n",
    "def sigmoid(X):\n",
    "    return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "# Softmax function for a numpy array\n",
    "def softmax(X, axis=-1):\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63000, 784) (63000,) (7000, 784) (7000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(data_train, labels_train), (data_test, labels_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten the dataset\n",
    "data_train = data_train.reshape(len(data_train), -1)\n",
    "data_test = data_test.reshape(len(data_test), -1)\n",
    "\n",
    "# Scale the dataset\n",
    "scaler = StandardScaler()\n",
    "data_train = scaler.fit_transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Create one-hot labels\n",
    "binarizer = LabelBinarizer()\n",
    "labels_train_one_hot = binarizer.fit_transform(labels_train)\n",
    "labels_test_one_hot = binarizer.transform(labels_test)\n",
    "\n",
    "print(data_train.shape, labels_train.shape, data_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8\n"
     ]
    }
   ],
   "source": [
    "# Function to display one MNIST data\n",
    "def plot_digit(x):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    x = scaler.inverse_transform(x)\n",
    "    x = x.reshape(28, 28)\n",
    "    plt.imshow(x, cmap=matplotlib.cm.binary)\n",
    "    plt.show()\n",
    "    \n",
    "plot_digit(data_train[0])\n",
    "print(\"Label:\", labels_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator to fetch the dataset one batch at a time\n",
    "def fetch_batch(X, y, batch_size):\n",
    "    shuffled_indices = np.random.permutation(len(X))\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[shuffled_indices[i:i+batch_size]]\n",
    "        y_batch = y[shuffled_indices[i:i+batch_size]]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5:\n",
    "    def __init__(self, dropout_rate=0.5, learning_rate=0.1, momentum=0.5, graph_seed=42):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.graph_seed = graph_seed\n",
    "        \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        height = 28\n",
    "        width = 28\n",
    "        channels = 1\n",
    "        input_size = height * width * channels\n",
    "        output_size = 10\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Set graph-level random seed\n",
    "            tf.set_random_seed(self.graph_seed)\n",
    "            \n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "            self.train_ph = tf.placeholder(shape=(), dtype=tf.bool)\n",
    "            \n",
    "            # First convolutional layer\n",
    "            X_reshaped = tf.reshape(self.X_ph, shape=(-1, height, width, channels))\n",
    "            X = tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='SAME', \n",
    "                                       activation=tf.nn.tanh)(X_reshaped)\n",
    "            \n",
    "            # First pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Second convolutional layer\n",
    "            X = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='VALID', \n",
    "                                       activation=tf.nn.tanh)(X)\n",
    "            \n",
    "            # Second pooling layer\n",
    "            X = tf.nn.avg_pool(X, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "            \n",
    "            # Third convolutional layer\n",
    "            X = tf.keras.layers.Conv2D(filters=120, kernel_size=5, strides=1, padding='VALID', \n",
    "                                       activation=tf.nn.tanh)(X)\n",
    "            \n",
    "            # Fully connected layers\n",
    "            X = tf.reshape(X, shape=(-1, 120))\n",
    "            X = tf.keras.layers.Dropout(self.dropout_rate)(X, training=self.train_ph)\n",
    "            X = tf.keras.layers.Dense(units=84, activation=tf.nn.tanh)(X)\n",
    "            X = tf.keras.layers.Dropout(self.dropout_rate)(X, training=self.train_ph)\n",
    "            self.logits = tf.keras.layers.Dense(units=output_size)(X)\n",
    "            \n",
    "            # Probabilities for each class\n",
    "            self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "            \n",
    "            # Use mean cross entropy as the loss function\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y_ph)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            \n",
    "            # Make optimizer and training operation\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        # Create a session and initialize all variables\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, \n",
    "                                                        self.y_ph: y_batch, \n",
    "                                                        self.train_ph: True})\n",
    "                \n",
    "    def predict(self, X):\n",
    "        # Return the prediction, a number between 0-9, for each row of X\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return np.argmax(logits, axis=1)\n",
    "        \n",
    "    def get_probs(self, X):\n",
    "        # Return the estimated probabilities of all ten classes, for each row of X\n",
    "        probs = self.sess.run(self.probs, feed_dict={self.X_ph: X, self.train_ph: False})\n",
    "        return probs\n",
    "    \n",
    "    def reset_session(self):\n",
    "        # Reset the session and initialize all variables\n",
    "        self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ice-bear/anaconda3/envs/machine_learning/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch: 0 \t Test accuracy: 0.9399\n",
      "Epoch: 1 \t Test accuracy: 0.9649\n",
      "Epoch: 2 \t Test accuracy: 0.9733\n",
      "Epoch: 3 \t Test accuracy: 0.9784\n",
      "Epoch: 4 \t Test accuracy: 0.9791\n",
      "Epoch: 5 \t Test accuracy: 0.9816\n",
      "Epoch: 6 \t Test accuracy: 0.9819\n",
      "Epoch: 7 \t Test accuracy: 0.9834\n",
      "Epoch: 8 \t Test accuracy: 0.9849\n",
      "Epoch: 9 \t Test accuracy: 0.9844\n",
      "Epoch: 10 \t Test accuracy: 0.9869\n",
      "Epoch: 11 \t Test accuracy: 0.9859\n",
      "Epoch: 12 \t Test accuracy: 0.9851\n",
      "Epoch: 13 \t Test accuracy: 0.9871\n",
      "Epoch: 14 \t Test accuracy: 0.9863\n",
      "Epoch: 15 \t Test accuracy: 0.9856\n",
      "Epoch: 16 \t Test accuracy: 0.9873\n",
      "Epoch: 17 \t Test accuracy: 0.9877\n",
      "Epoch: 18 \t Test accuracy: 0.9889\n",
      "Epoch: 19 \t Test accuracy: 0.9873\n",
      "Epoch: 20 \t Test accuracy: 0.9879\n",
      "Epoch: 21 \t Test accuracy: 0.9886\n",
      "Epoch: 22 \t Test accuracy: 0.9889\n",
      "Epoch: 23 \t Test accuracy: 0.9886\n",
      "Epoch: 24 \t Test accuracy: 0.9871\n",
      "Epoch: 25 \t Test accuracy: 0.9889\n",
      "Epoch: 26 \t Test accuracy: 0.9891\n",
      "Epoch: 27 \t Test accuracy: 0.9877\n",
      "Epoch: 28 \t Test accuracy: 0.9896\n",
      "Epoch: 29 \t Test accuracy: 0.9893\n",
      "Epoch: 30 \t Test accuracy: 0.9891\n",
      "Epoch: 31 \t Test accuracy: 0.9901\n",
      "Epoch: 32 \t Test accuracy: 0.9889\n",
      "Epoch: 33 \t Test accuracy: 0.9891\n",
      "Epoch: 34 \t Test accuracy: 0.9900\n",
      "Epoch: 35 \t Test accuracy: 0.9894\n",
      "Epoch: 36 \t Test accuracy: 0.9913\n",
      "Epoch: 37 \t Test accuracy: 0.9894\n",
      "Epoch: 38 \t Test accuracy: 0.9884\n",
      "Epoch: 39 \t Test accuracy: 0.9896\n",
      "Epoch: 40 \t Test accuracy: 0.9893\n",
      "Epoch: 41 \t Test accuracy: 0.9884\n",
      "Epoch: 42 \t Test accuracy: 0.9906\n",
      "Epoch: 43 \t Test accuracy: 0.9899\n",
      "Epoch: 44 \t Test accuracy: 0.9901\n",
      "Epoch: 45 \t Test accuracy: 0.9913\n",
      "Epoch: 46 \t Test accuracy: 0.9901\n",
      "Epoch: 47 \t Test accuracy: 0.9900\n",
      "Epoch: 48 \t Test accuracy: 0.9883\n",
      "Epoch: 49 \t Test accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "lenet5 = LeNet5(dropout_rate=0.5, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "lenet5.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    lenet5.train(data_train, labels_train)\n",
    "    prediction = lenet5.predict(data_test)\n",
    "    print(\"Epoch: %d \\t Test accuracy: %.4f\" % (epoch, sum(prediction==labels_test) / len(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADFCAYAAAARxr1AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADJ9JREFUeJzt3WtsVGUaB/D/Iy4fLJeoNQ2hQI2QjUhid50gEd24EVZESMEYIiGbagwqkYRVPoAkRkKCwdu6RI1RsWk3XMTLLhJDlpKGwJIgMhLlolkR0tVqbak3ipIQ4NkPc2ZT2uc8nZlz5tr/LzHMPH17zjvCv6fzzjnPEVUFEdkuK/YEiEoZA0LkYECIHAwIkYMBIXIwIEQOBoTIwYAQORgQIsflUb5ZRGYBWA9gGIANqrrOG19dXa11dXVRdkkUi/b2dvT09Mhg43IOiIgMA/AKgJkAOgAcFJHtqvpZ2PfU1dUhmUzmukui2CQSiYzGRfkVayqAL1X1pKqeA/AWgIYI2yMqOVECMhbA132edwS1S4jIQyKSFJHkqVOnIuyOqPDy/iZdVV9X1YSqJq655pp8744oVlEC8g2AcX2e1wY1oooRJSAHAUwSkWtFZDiA+wBsj2daRKUh51UsVT0vIksB7ERqmbdJVY/FNjOiEhDpcxBV3QFgR0xzISo5/CSdyMGAEDkYECIHA0LkYECIHAwIkYMBIXIwIEQOBoTIwYAQORgQIgcDQuRgQIgcDAiRgwEhcjAgRA4GhMjBgBA5GBAiR6Rr0ilcc3PzgFpnZ2de97lr1y6zPnPmzKzGW2pra836hx9+aNYfeOCBjLcNACNHjjTrS5cuzWo7cYvavLodQC+ACwDOq2pmDU+JykQcR5A/qmpPDNshKjl8D0LkiBoQBdAqIh+LyEPWADavpnIWNSC3qurvAdwF4FER+UP/AWxeTeVMVDWeDYmsBnBGVZ8PG5NIJLTUb6Bz+PBhs753716zvn79erP+1VdfDaidP38+94lFEPZ3LDLoDZYK5rLL7J/VVVVVGW/jp59+ynhsIpFAMpkc9H9AzkcQEakSkZHpxwD+BOBortsjKkVRVrFqAPwz+Cl0OYDNqvqvWGZFVCKidHc/CeDGGOdCVHK4zEvkYECIHEP2XKwjR46Y9TvvvNOsd3d3R95n2DL37Nmzzfq+ffvM+okTJ8z6TTfdZNazWTmcOHGiWQ/7DOvnn3/OeNue6dOnm/Xrr79+QO3JJ5+MZZ+Z4BGEyMGAEDkYECIHA0LkYECIHEN2FWvGjBlmvacnnktbbrnllgG1jRs3mmMnTJhg1sOuQDx9+rRZr66uNuvZvKZRo0aZ9Q8++MCsP/LIIxlvGwBeeukls75o0SKzPnr06Ky2HzceQYgcDAiRgwEhcjAgRA4GhMgxZFexbrzRPlN/z549Zj3bqwGPHh147djOnTvNsfPnzzfrY8aMyaoe5uqrr854bNjKWUtLS1b7DDN27FizXuzVqjA8ghA5GBAiBwNC5GBAiBwMCJFj0FUsEWkCMAdAt6pOCWpXAdgKoA5AO4AFqvpj/qYZv9bWVrP+zDPPmPU333zTrIdd3WedL7VkyRJz7LvvvmvWn332WbNeX19v1rPV1dU1oPbaa6+ZY/fv35/Vtq0rAQH7HLVSlskRpBnArH61lQDaVHUSgLbgOVHFGTQgqroXwA/9yg0A0gvjLQDmxTwvopKQ63uQGlVNf6L0HVJN5ExsXk3lLPKbdE01fg1t8Mvm1VTOcj3VpEtExqhqp4iMARC9J06JWLFihVlfuHChWW9sbDTr7e3tA2pWQ2sAaGtrM+u33XabWZ87d65ZD7sYKax5dUNDw4DawYMHzbFhRowYYdaXL19u1svth2SuR5DtANL/MhoBvB/PdIhKy6ABEZEtAPYD+K2IdIjIgwDWAZgpIscBzAieE1WcQX/FUlX7dwvgjpjnQlRy+Ek6kYMBIXIM2QumsjV+/Hizvnv3brNuXXjU3Nxsjn366afN+q+//mrWt27dataPHz9u1i9evGjWP/nkE7NuueKKK8x62C3o7r///oy3Xcp4BCFyMCBEDgaEyMGAEDkYECIHV7HyxGrN88QTT5hjwy6AWrNmjVn/6KOPzPqhQ4fMeti5WMEtvC8xfPhwc+y8efYVDZWyWhWGRxAiBwNC5GBAiBwMCJGDASFycBWrBHz//fdm/ezZswWeSfiVk01NTQWeSWngEYTIwYAQORgQIgcDQuRgQIgcuTavXg1gMYB0q8RVqrojX5MsR1ZfrC1btphjN2/ebNaPHTsWy1zCzsWyfPrpp2Y9bKUtm9u7laNcm1cDwIuqWh/8x3BQRcq1eTXRkBDlPchSETksIk0icmXYIDavpnKWa0BeBXAdgHoAnQBeCBvI5tVUznIKiKp2qeoFVb0I4A0AU+OdFlFpyOlcrHRn9+DpfABH45tSeTl58qRZt/pFvfzyy1lt27rizxN2e7MLFy6Y9QMHDgyohfXKmjNnjlnftm2bWa+pCb1lTFnJZJl3C4DbAVSLSAeApwDcLiL1SN0XpB3Aw3mcI1HR5Nq82r6jJVGF4SfpRA4GhMjBgBA5eEVhhk6cOGHWw1Z3vvjii8j7rKqqMuuLFi0y688995xZDzsXq7a2dkDtzJkz5tiwXlxh912slFUsHkGIHAwIkYMBIXIwIEQOvknvZ+PGjWY9rPH0t99+G3mfN998s1l//PHHzfq9994beZ8AcMMNNwyoWaefDGU8ghA5GBAiBwNC5GBAiBwMCJGDq1j9rFu3zqxnu1pltcMJOy3lhRfsK5avvDL0Uv9YTJs2bUCNq1iX4hGEyMGAEDkYECIHA0LkYECIHJl0NRkH4O8AapDqYvK6qq4XkasAbAVQh1RnkwWq+mP+phqvsFWpsAuAsmWtWBXrNmZhFztt2LChwDMpP5kcQc4DWK6qkwFMA/CoiEwGsBJAm6pOAtAWPCeqKJk0r+5U1UPB414AnwMYC6ABQEswrAXAvHxNkqhYsnoPIiJ1AH4H4ACAmj7dFb9D6lcw63vYvJrKVsYBEZERAN4D8BdVPd33a5rqCmB2BmDzaipnGQVERH6DVDg2qeo/gnKXiIwJvj4GQHd+pkhUPJmsYglSrUY/V9W/9vnSdgCNANYFf76flxnmSUtLi1n/5ZdfYtn+kiVLMh7b0dFh1nt7e7PaZ2trq1l/7LHHzHo2zbHnzp1r1idOnJjxNspRJicrTgfwZwBHRCTd+nsVUsF4W0QeBPBfAAvyM0Wi4smkefU+AGE/au6IdzpEpYWfpBM5GBAiBwNC5BiyVxSGrcqsXbvWrJ89ezar7a9Zs2ZAzbrKEAD27Nlj1uM6Lywbd999t1kPW/UbPXp0PqdTdDyCEDkYECIHA0LkYECIHAwIkWPIrmJNmTLFrN9zzz1mfdOmTVltf8eOHVnPKV9GjRpl1levXj2gtnjxYnNs2O3gKh2PIEQOBoTIwYAQORgQIgcDQuQYsqtYYZYtW2bWz507Z9bfeeedfE7H1NDQYNbD7nW4YsWKfE6novEIQuRgQIgcDAiRgwEhckiq55szILx59WoAiwGk2yWuUlX3/IpEIqHJZDLypImiSiQSSCaTg/Y9ymQVK928+pCIjATwsYjsCr72oqo+H2WiRKUsk7Y/nQA6g8e9IpJuXk1U8aI0rwaApSJyWESaRMS8JSubV1M5i9K8+lUA1wGoR+oIY97LmM2rqZzl3LxaVbtU9YKqXgTwBoCp+ZsmUXEMGpCw5tXpzu6B+QCOxj89ouKK0rx6oYjUI7X02w7g4bzMkKiIojSvLp1rSonyhJ+kEzkYECIHA0LkYECIHAwIkYMBIXIwIEQOBoTIwYAQOQa9ojDWnYmcQuqe6gBQDaCnYDsvHr7O0jRBVQc9vbygAblkxyJJVU0UZecFxNdZ3vgrFpGDASFyFDMgrxdx34XE11nGivYehKgc8FcsIgcDQuQoeEBEZJaI/EdEvhSRlYXefz4F7Y+6ReRon9pVIrJLRI4Hf5rtkcqJiIwTkd0i8pmIHBORZUG94l5rQQMiIsMAvALgLgCTkbqufXIh55BnzQBm9autBNCmqpMAtAXPy1262+ZkANMAPBr8PVbcay30EWQqgC9V9aSqngPwFgD7bjBlSFX3AvihX7kBQEvwuAXAvIJOKg9UtVNVDwWPewGku21W3GstdEDGAvi6z/MOVH4b05qgfSsAfIdUE/CK0a/bZsW9Vr5JLyBNralXzLq60W3z/yrltRY6IN8AGNfneW1Qq2Rd6SZ7wZ/dRZ5PLKxum6jA11rogBwEMElErhWR4QDuA7C9wHMotO0AGoPHjQDeL+JcYhHWbROV+FoL/Um6iMwG8DcAwwA0qeragk4gj0RkC4DbkTr1uwvAUwC2AXgbwHikTvVfoKr938iXFRG5FcC/ARwBcDEor0LqfUhlvVaeakIUjm/SiRwMCJGDASFyMCBEDgaEyMGAEDkYECLH/wCcYMf2HdEY7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained CNN to generate target probabilities\n",
    "cnn_probs = lenet5.get_probs(data_train)\n",
    "\n",
    "# Show some examples to make sure everything's okay\n",
    "for i in range(5):\n",
    "    plot_digit(data_train[i])\n",
    "    print(np.around(cnn_probs[i],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the soft targets in a file\n",
    "np.save(\"soft_targets.npy\", cnn_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDecisionTree:\n",
    "    def __init__(self, max_depth=8, inverse_temperature=1.0, reg_fn=lambda d: 2 ** -d, \n",
    "                 learning_rate=0.1, epsilon=1e-10, graph_seed=42):\n",
    "        self.max_depth = max_depth\n",
    "        self.inverse_temperature = inverse_temperature\n",
    "        self.reg_fn = reg_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.graph_seed = graph_seed\n",
    "    \n",
    "    def build_graph(self):\n",
    "        # MNIST properties\n",
    "        input_size = 28 * 28\n",
    "        output_size = 10\n",
    "        \n",
    "        # Create lists for storing variables\n",
    "        n_nodes = 2 ** (self.max_depth + 1)\n",
    "        self.weights = [0] * n_nodes\n",
    "        self.bias = [0] * n_nodes\n",
    "        self.leaf_logits = [0] * n_nodes\n",
    "        self.path_probs = [0] * n_nodes\n",
    "        \n",
    "        # Build the graph\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # Set graph-level random seed\n",
    "            tf.set_random_seed(self.graph_seed)\n",
    "            \n",
    "            # Make input placeholders\n",
    "            self.X_ph = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "            self.y_ph = tf.placeholder(shape=(None, output_size), dtype=tf.float32)\n",
    "            \n",
    "            # Initialize the loss function\n",
    "            self.loss = 0\n",
    "            \n",
    "            # Start building from the root node\n",
    "            batch_size = tf.shape(self.X_ph)[0]\n",
    "            self.path_probs[1] = tf.fill([batch_size], 1.0)\n",
    "            self.build_node(self.X_ph, self.y_ph)\n",
    "            \n",
    "            # Make optimizer and training operation\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "            \n",
    "            # Variables initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            \n",
    "        # Create a session and initialize all variables\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "        return self.graph\n",
    "    \n",
    "    def build_node(self, X, y, current_depth=1, index=1):\n",
    "        if current_depth == self.max_depth:\n",
    "            # Build a leaf node\n",
    "            self.leaf_logits[index] = tf.Variable(initial_value=np.random.randn(y.shape[1], 1), dtype=tf.float32)\n",
    "            \n",
    "            # Class distribution at this leaf node\n",
    "            probs = tf.nn.softmax(self.leaf_logits[index], axis=0)\n",
    "            \n",
    "            # Compute loss at this leaf node\n",
    "            cross_entropy = tf.squeeze(tf.matmul(y, -tf.log(probs)), axis=1)\n",
    "            weighted_cross_entropy = tf.multiply(self.path_probs[index], cross_entropy)\n",
    "            \n",
    "            self.loss = self.loss + tf.reduce_mean(weighted_cross_entropy)\n",
    "        else:\n",
    "            # Build an internal node\n",
    "            self.weights[index] = tf.Variable(initial_value=np.random.randn(X.shape[1], 1), dtype=tf.float32)\n",
    "            self.bias[index] = tf.Variable(initial_value=0.0)\n",
    "            \n",
    "            # Use sigmoid to determine, for each data, the probability of going to the right child node\n",
    "            mul = tf.matmul(X, self.weights[index]) + self.bias[index]\n",
    "            probs = tf.squeeze(tf.sigmoid(self.inverse_temperature * mul), axis=1)\n",
    "            \n",
    "            # Compute path probabilities of left and right child nodes\n",
    "            left_index = 2 * index\n",
    "            right_index = 2 * index + 1\n",
    "            self.path_probs[left_index] = tf.multiply(self.path_probs[index], 1-probs)\n",
    "            self.path_probs[right_index] = tf.multiply(self.path_probs[index], probs)\n",
    "            \n",
    "            # Regularization term\n",
    "            reg_strength = self.reg_fn(current_depth)\n",
    "            left_ratio = tf.reduce_sum(self.path_probs[left_index]) / (tf.reduce_sum(self.path_probs[index]) + self.epsilon)\n",
    "            right_ratio = tf.reduce_sum(self.path_probs[right_index]) / (tf.reduce_sum(self.path_probs[index]) + self.epsilon)\n",
    "            \n",
    "            self.loss = self.loss - reg_strength * (0.5 * tf.log(left_ratio + self.epsilon) + 0.5 * tf.log(right_ratio + self.epsilon))\n",
    "            \n",
    "            # Build left and right subtrees\n",
    "            self.build_node(X, y, current_depth+1, left_index)\n",
    "            self.build_node(X, y, current_depth+1, right_index)\n",
    "            \n",
    "    def train(self, X, y, batch_size=100, n_epochs=1):\n",
    "        for epoch in range(n_epochs):\n",
    "            # Perform mini-batch gradient descent using the entire dataset\n",
    "            for X_batch, y_batch in fetch_batch(X, y, batch_size):\n",
    "                if len(X_batch) < batch_size: continue  # skip last batch if it is small\n",
    "                self.sess.run(self.train_op, feed_dict={self.X_ph: X_batch, self.y_ph: y_batch})\n",
    "                   \n",
    "    def get_logits(self, X, current_depth=1, index=1):\n",
    "        if len(X) == 0:\n",
    "            # If there's no sample, return nothing\n",
    "            return None\n",
    "        elif current_depth == self.max_depth:\n",
    "            # At leaf node: simply return the logits for every sample that ends up here\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            logits = logits.ravel()\n",
    "            return [logits for _ in range(len(X))]\n",
    "        else:\n",
    "            # At internal node: split the dataset, get the logits of each, and then combine them\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            mul = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * mul), axis=1)\n",
    "            \n",
    "            mask = np.array(probs < 0.5)\n",
    "            \n",
    "            indices_left = np.nonzero(mask)\n",
    "            indices_right = np.nonzero(np.logical_not(mask))\n",
    "            \n",
    "            logits_left = self.get_logits(X[indices_left], current_depth+1, index*2)\n",
    "            logits_right = self.get_logits(X[indices_right], current_depth+1, index*2+1)\n",
    "            \n",
    "            logits = []\n",
    "            it_left, it_right = 0, 0\n",
    "            for m in mask:\n",
    "                if m:\n",
    "                    logits.append(logits_left[it_left])\n",
    "                    it_left += 1\n",
    "                else:\n",
    "                    logits.append(logits_right[it_right])\n",
    "                    it_right += 1\n",
    "            \n",
    "            return logits\n",
    "    \n",
    "    def get_probs_global(self, X, path_probs, current_depth=1, index=1):\n",
    "        if current_depth == self.max_depth:\n",
    "            # At leaf node: return class distribution at this leaf node\n",
    "            logits = self.sess.run(self.leaf_logits[index])\n",
    "            probs = softmax(logits, axis=0)\n",
    "            distrib = np.dot(path_probs.reshape(-1, 1), probs.reshape(1, -1))\n",
    "            return distrib\n",
    "        else:\n",
    "            # At internal node: get the probabilities from both child nodes and add them together\n",
    "            weights, bias = self.sess.run([self.weights[index], self.bias[index]])\n",
    "            \n",
    "            mul = np.dot(X, weights) + bias\n",
    "            probs = np.squeeze(sigmoid(self.inverse_temperature * mul), axis=1)\n",
    "            \n",
    "            path_probs_left = np.multiply(path_probs, 1-probs)\n",
    "            path_probs_right = np.multiply(path_probs, probs)\n",
    "            \n",
    "            probs_left = self.get_probs_global(X, path_probs_left, current_depth+1, index*2)\n",
    "            probs_right = self.get_probs_global(X, path_probs_right, current_depth+1, index*2+1)\n",
    "            \n",
    "            return probs_left + probs_right\n",
    "    \n",
    "    def predict(self, X, method=2):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits(X)\n",
    "            return np.argmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return np.argmax(probs, axis=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def get_probs(self, X, method=2):\n",
    "        if method == 1:\n",
    "            logits = self.get_logits(X)\n",
    "            return softmax(logits, axis=1)\n",
    "        elif method == 2:\n",
    "            root_probs = np.full((len(X)), 1.0)\n",
    "            probs = self.get_probs_global(X, root_probs)\n",
    "            return probs\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    def compute_loss(self, X, y):\n",
    "        return self.sess.run(self.loss, feed_dict={self.X_ph: X, self.y_ph: y})\n",
    "    \n",
    "    def reset_session(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        self.sess.run(self.init)\n",
    "            \n",
    "    def __del__(self):\n",
    "        if self.sess:\n",
    "            self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load soft targets\n",
    "cnn_probs = np.load(\"soft_targets.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.681 \t Test accuracy: 0.678 \t Loss: 3.575\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.700 \t Test accuracy: 0.706 \t Loss: 3.458\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.731 \t Test accuracy: 0.736 \t Loss: 3.412\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.720 \t Test accuracy: 0.720 \t Loss: 3.412\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.752 \t Test accuracy: 0.752 \t Loss: 3.337\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.720 \t Test accuracy: 0.724 \t Loss: 3.361\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.726 \t Test accuracy: 0.734 \t Loss: 3.357\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.725 \t Test accuracy: 0.731 \t Loss: 3.363\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.719 \t Test accuracy: 0.713 \t Loss: 3.418\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.754 \t Test accuracy: 0.761 \t Loss: 3.296\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.749 \t Test accuracy: 0.753 \t Loss: 3.283\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.752 \t Test accuracy: 0.757 \t Loss: 3.268\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.741 \t Test accuracy: 0.743 \t Loss: 3.298\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.744 \t Test accuracy: 0.736 \t Loss: 3.292\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.752 \t Test accuracy: 0.756 \t Loss: 3.287\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.742 \t Test accuracy: 0.746 \t Loss: 3.294\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.760 \t Test accuracy: 0.765 \t Loss: 3.252\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.766 \t Test accuracy: 0.765 \t Loss: 3.246\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.757 \t Test accuracy: 0.757 \t Loss: 3.262\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.767 \t Test accuracy: 0.766 \t Loss: 3.251\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.768 \t Test accuracy: 0.768 \t Loss: 3.247\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.755 \t Test accuracy: 0.760 \t Loss: 3.279\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.772 \t Test accuracy: 0.771 \t Loss: 3.273\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.761 \t Test accuracy: 0.765 \t Loss: 3.257\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.769 \t Test accuracy: 0.770 \t Loss: 3.266\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.774 \t Test accuracy: 0.773 \t Loss: 3.242\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.783 \t Test accuracy: 0.780 \t Loss: 3.226\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.784 \t Test accuracy: 0.789 \t Loss: 3.222\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.791 \t Test accuracy: 0.789 \t Loss: 3.208\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.792 \t Test accuracy: 0.783 \t Loss: 3.227\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.800 \t Test accuracy: 0.795 \t Loss: 3.211\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.798 \t Test accuracy: 0.791 \t Loss: 3.204\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.816 \t Test accuracy: 0.809 \t Loss: 3.171\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.795 \t Test accuracy: 0.782 \t Loss: 3.230\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.803 \t Test accuracy: 0.796 \t Loss: 3.194\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.821 \t Test accuracy: 0.809 \t Loss: 3.169\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.812 \t Test accuracy: 0.805 \t Loss: 3.169\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.806 \t Test accuracy: 0.802 \t Loss: 3.192\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.814 \t Test accuracy: 0.804 \t Loss: 3.199\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.802 \t Test accuracy: 0.792 \t Loss: 3.203\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.798 \t Test accuracy: 0.790 \t Loss: 3.186\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.809 \t Test accuracy: 0.800 \t Loss: 3.167\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.815 \t Test accuracy: 0.811 \t Loss: 3.164\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.797 \t Test accuracy: 0.787 \t Loss: 3.213\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.803 \t Test accuracy: 0.798 \t Loss: 3.162\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.803 \t Test accuracy: 0.799 \t Loss: 3.162\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.802 \t Test accuracy: 0.799 \t Loss: 3.163\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.808 \t Test accuracy: 0.802 \t Loss: 3.174\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.817 \t Test accuracy: 0.812 \t Loss: 3.157\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.799 \t Test accuracy: 0.792 \t Loss: 3.181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using the hard data and with default hyperparameters\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree()\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, labels_train_one_hot, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=2)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=2)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.642 \t Test accuracy: 0.632 \t Loss: 3.653\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.707 \t Test accuracy: 0.706 \t Loss: 3.466\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.723 \t Test accuracy: 0.736 \t Loss: 3.396\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.737 \t Test accuracy: 0.735 \t Loss: 3.380\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.710 \t Test accuracy: 0.711 \t Loss: 3.411\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.700 \t Test accuracy: 0.696 \t Loss: 3.450\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.708 \t Test accuracy: 0.706 \t Loss: 3.412\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.698 \t Test accuracy: 0.692 \t Loss: 3.426\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.718 \t Test accuracy: 0.720 \t Loss: 3.402\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.723 \t Test accuracy: 0.721 \t Loss: 3.386\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.718 \t Test accuracy: 0.718 \t Loss: 3.359\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.737 \t Test accuracy: 0.739 \t Loss: 3.338\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.733 \t Test accuracy: 0.733 \t Loss: 3.337\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.742 \t Test accuracy: 0.737 \t Loss: 3.344\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.733 \t Test accuracy: 0.733 \t Loss: 3.341\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.745 \t Test accuracy: 0.745 \t Loss: 3.312\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.741 \t Test accuracy: 0.743 \t Loss: 3.295\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.750 \t Test accuracy: 0.756 \t Loss: 3.278\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.762 \t Test accuracy: 0.766 \t Loss: 3.261\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.767 \t Test accuracy: 0.765 \t Loss: 3.260\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.771 \t Test accuracy: 0.762 \t Loss: 3.291\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.758 \t Test accuracy: 0.744 \t Loss: 3.325\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.754 \t Test accuracy: 0.747 \t Loss: 3.317\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.770 \t Test accuracy: 0.772 \t Loss: 3.265\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.769 \t Test accuracy: 0.773 \t Loss: 3.257\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.763 \t Test accuracy: 0.770 \t Loss: 3.268\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.765 \t Test accuracy: 0.772 \t Loss: 3.231\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.769 \t Test accuracy: 0.772 \t Loss: 3.225\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.770 \t Test accuracy: 0.774 \t Loss: 3.236\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.763 \t Test accuracy: 0.769 \t Loss: 3.238\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.761 \t Test accuracy: 0.759 \t Loss: 3.256\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.772 \t Test accuracy: 0.774 \t Loss: 3.207\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.770 \t Test accuracy: 0.769 \t Loss: 3.229\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.782 \t Test accuracy: 0.783 \t Loss: 3.236\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.789 \t Test accuracy: 0.792 \t Loss: 3.197\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.794 \t Test accuracy: 0.793 \t Loss: 3.193\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.782 \t Test accuracy: 0.788 \t Loss: 3.197\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.784 \t Test accuracy: 0.789 \t Loss: 3.182\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.791 \t Test accuracy: 0.796 \t Loss: 3.167\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.785 \t Test accuracy: 0.792 \t Loss: 3.186\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.791 \t Test accuracy: 0.794 \t Loss: 3.187\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.789 \t Test accuracy: 0.799 \t Loss: 3.173\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.791 \t Test accuracy: 0.799 \t Loss: 3.176\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.793 \t Test accuracy: 0.795 \t Loss: 3.180\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.792 \t Test accuracy: 0.797 \t Loss: 3.192\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.782 \t Test accuracy: 0.785 \t Loss: 3.199\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.787 \t Test accuracy: 0.790 \t Loss: 3.203\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.787 \t Test accuracy: 0.793 \t Loss: 3.189\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.795 \t Test accuracy: 0.803 \t Loss: 3.168\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.791 \t Test accuracy: 0.793 \t Loss: 3.186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Measure performance of the model using the soft data and with default hyperparameters\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree()\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, cnn_probs, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=2)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=2)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.909 \t Test accuracy: 0.908 \t Loss: 0.962\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.938 \t Test accuracy: 0.934 \t Loss: 0.795\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.729\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.946 \t Loss: 0.693\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.963 \t Test accuracy: 0.950 \t Loss: 0.671\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.953 \t Loss: 0.654\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.644\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.635\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.956 \t Loss: 0.630\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.974 \t Test accuracy: 0.955 \t Loss: 0.628\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.955 \t Loss: 0.624\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.977 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.979 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.628\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.629\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.630\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.632\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.632\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.635\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.636\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.986 \t Test accuracy: 0.957 \t Loss: 0.637\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.639\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.643\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.643\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.645\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.646\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.650\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.650\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.988 \t Test accuracy: 0.956 \t Loss: 0.650\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.988 \t Test accuracy: 0.956 \t Loss: 0.651\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.988 \t Test accuracy: 0.956 \t Loss: 0.653\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.988 \t Test accuracy: 0.957 \t Loss: 0.654\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.988 \t Test accuracy: 0.956 \t Loss: 0.655\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.988 \t Test accuracy: 0.956 \t Loss: 0.659\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.659\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.661\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.664\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.666\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.668\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.671\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.989 \t Test accuracy: 0.955 \t Loss: 0.673\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.990 \t Test accuracy: 0.954 \t Loss: 0.673\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.990 \t Test accuracy: 0.954 \t Loss: 0.677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model using the hard data and with better hyperparameters\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d,\n",
    "                        learning_rate = 0.01)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, labels_train_one_hot, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=2)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=2)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.793\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.727\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.691\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.670\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.951 \t Loss: 0.653\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.954 \t Loss: 0.642\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.633\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.629\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.619\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.623\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.628\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.629\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.631\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.633\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.635\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.634\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.636\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.637\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.638\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.641\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.643\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.642\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.645\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.646\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.649\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.649\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.649\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.650\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.650\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model using the soft data and with better hyperparameters\n",
    "\n",
    "set_random_seed(42)\n",
    "t = time.time()\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 8,\n",
    "                        inverse_temperature = 0.1,\n",
    "                        reg_fn = lambda d: 4 ** -d,\n",
    "                        learning_rate = 0.01)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, cnn_probs, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=2)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=2)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, depth = 6\n",
      "Epoch: 0 \t Train accuracy: 0.879 \t Test accuracy: 0.876 \t Loss: 1.097\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.915 \t Test accuracy: 0.908 \t Loss: 0.899\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.924 \t Test accuracy: 0.918 \t Loss: 0.827\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.931 \t Test accuracy: 0.920 \t Loss: 0.790\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.934 \t Test accuracy: 0.924 \t Loss: 0.770\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.937 \t Test accuracy: 0.927 \t Loss: 0.755\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.939 \t Test accuracy: 0.927 \t Loss: 0.745\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.942 \t Test accuracy: 0.930 \t Loss: 0.736\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.943 \t Test accuracy: 0.929 \t Loss: 0.731\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.945 \t Test accuracy: 0.931 \t Loss: 0.726\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.946 \t Test accuracy: 0.929 \t Loss: 0.722\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.947 \t Test accuracy: 0.931 \t Loss: 0.719\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.948 \t Test accuracy: 0.932 \t Loss: 0.717\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.948 \t Test accuracy: 0.930 \t Loss: 0.714\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.949 \t Test accuracy: 0.931 \t Loss: 0.713\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.950 \t Test accuracy: 0.932 \t Loss: 0.713\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.950 \t Test accuracy: 0.931 \t Loss: 0.713\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.951 \t Test accuracy: 0.931 \t Loss: 0.710\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.951 \t Test accuracy: 0.931 \t Loss: 0.711\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.951 \t Test accuracy: 0.930 \t Loss: 0.709\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.952 \t Test accuracy: 0.931 \t Loss: 0.710\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.952 \t Test accuracy: 0.930 \t Loss: 0.709\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.952 \t Test accuracy: 0.930 \t Loss: 0.710\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.953 \t Test accuracy: 0.931 \t Loss: 0.710\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.953 \t Test accuracy: 0.929 \t Loss: 0.709\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.953 \t Test accuracy: 0.929 \t Loss: 0.709\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.953 \t Test accuracy: 0.930 \t Loss: 0.709\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.954 \t Test accuracy: 0.930 \t Loss: 0.707\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.954 \t Test accuracy: 0.928 \t Loss: 0.710\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.954 \t Test accuracy: 0.929 \t Loss: 0.709\n",
      "\n",
      "Using cnn data, depth = 7\n",
      "Epoch: 0 \t Train accuracy: 0.892 \t Test accuracy: 0.897 \t Loss: 1.023\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.927 \t Test accuracy: 0.922 \t Loss: 0.838\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.937 \t Test accuracy: 0.930 \t Loss: 0.772\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.943 \t Test accuracy: 0.935 \t Loss: 0.738\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.947 \t Test accuracy: 0.936 \t Loss: 0.715\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.951 \t Test accuracy: 0.939 \t Loss: 0.700\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.953 \t Test accuracy: 0.942 \t Loss: 0.690\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.955 \t Test accuracy: 0.942 \t Loss: 0.682\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.957 \t Test accuracy: 0.942 \t Loss: 0.676\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.959 \t Test accuracy: 0.944 \t Loss: 0.671\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.960 \t Test accuracy: 0.945 \t Loss: 0.669\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.961 \t Test accuracy: 0.945 \t Loss: 0.666\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.962 \t Test accuracy: 0.946 \t Loss: 0.664\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.963 \t Test accuracy: 0.946 \t Loss: 0.664\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.964 \t Test accuracy: 0.945 \t Loss: 0.662\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.965 \t Test accuracy: 0.946 \t Loss: 0.662\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.965 \t Test accuracy: 0.945 \t Loss: 0.660\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.966 \t Test accuracy: 0.945 \t Loss: 0.662\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.967 \t Test accuracy: 0.945 \t Loss: 0.661\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.967 \t Test accuracy: 0.947 \t Loss: 0.659\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.967 \t Test accuracy: 0.945 \t Loss: 0.659\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.968 \t Test accuracy: 0.946 \t Loss: 0.658\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.968 \t Test accuracy: 0.946 \t Loss: 0.659\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.969 \t Test accuracy: 0.945 \t Loss: 0.659\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.969 \t Test accuracy: 0.945 \t Loss: 0.659\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.970 \t Test accuracy: 0.946 \t Loss: 0.660\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.970 \t Test accuracy: 0.946 \t Loss: 0.661\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.970 \t Test accuracy: 0.946 \t Loss: 0.660\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.970 \t Test accuracy: 0.946 \t Loss: 0.662\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.970 \t Test accuracy: 0.946 \t Loss: 0.662\n",
      "\n",
      "Using cnn data, depth = 8\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.793\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.727\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.691\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.670\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.951 \t Loss: 0.653\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.954 \t Loss: 0.642\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.633\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.629\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.619\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.623\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Using cnn data, depth = 9\n",
      "Epoch: 0 \t Train accuracy: 0.918 \t Test accuracy: 0.923 \t Loss: 0.924\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.944 \t Test accuracy: 0.941 \t Loss: 0.768\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.955 \t Test accuracy: 0.947 \t Loss: 0.706\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.962 \t Test accuracy: 0.951 \t Loss: 0.672\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.967 \t Test accuracy: 0.954 \t Loss: 0.649\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.971 \t Test accuracy: 0.955 \t Loss: 0.633\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.974 \t Test accuracy: 0.959 \t Loss: 0.621\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.976 \t Test accuracy: 0.960 \t Loss: 0.613\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.978 \t Test accuracy: 0.960 \t Loss: 0.607\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.980 \t Test accuracy: 0.960 \t Loss: 0.602\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.982 \t Test accuracy: 0.962 \t Loss: 0.596\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.983 \t Test accuracy: 0.963 \t Loss: 0.593\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.983 \t Test accuracy: 0.964 \t Loss: 0.591\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.984 \t Test accuracy: 0.963 \t Loss: 0.591\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.985 \t Test accuracy: 0.964 \t Loss: 0.590\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.985 \t Test accuracy: 0.963 \t Loss: 0.588\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.986 \t Test accuracy: 0.963 \t Loss: 0.589\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.986 \t Test accuracy: 0.964 \t Loss: 0.593\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.987 \t Test accuracy: 0.963 \t Loss: 0.592\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.987 \t Test accuracy: 0.963 \t Loss: 0.592\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.988 \t Test accuracy: 0.963 \t Loss: 0.594\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.988 \t Test accuracy: 0.964 \t Loss: 0.592\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.989 \t Test accuracy: 0.963 \t Loss: 0.595\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Train accuracy: 0.989 \t Test accuracy: 0.963 \t Loss: 0.595\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.989 \t Test accuracy: 0.963 \t Loss: 0.597\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.990 \t Test accuracy: 0.965 \t Loss: 0.597\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.990 \t Test accuracy: 0.964 \t Loss: 0.599\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.990 \t Test accuracy: 0.965 \t Loss: 0.598\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.990 \t Test accuracy: 0.964 \t Loss: 0.600\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.990 \t Test accuracy: 0.964 \t Loss: 0.600\n",
      "\n",
      "Using cnn data, depth = 10\n",
      "Epoch: 0 \t Train accuracy: 0.924 \t Test accuracy: 0.926 \t Loss: 0.884\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.949 \t Test accuracy: 0.946 \t Loss: 0.734\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.960 \t Test accuracy: 0.952 \t Loss: 0.677\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.967 \t Test accuracy: 0.956 \t Loss: 0.645\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.972 \t Test accuracy: 0.960 \t Loss: 0.627\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.977 \t Test accuracy: 0.961 \t Loss: 0.614\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.980 \t Test accuracy: 0.963 \t Loss: 0.602\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.982 \t Test accuracy: 0.964 \t Loss: 0.594\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.984 \t Test accuracy: 0.965 \t Loss: 0.591\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.985 \t Test accuracy: 0.965 \t Loss: 0.585\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.987 \t Test accuracy: 0.966 \t Loss: 0.582\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.988 \t Test accuracy: 0.967 \t Loss: 0.582\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.988 \t Test accuracy: 0.966 \t Loss: 0.582\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.989 \t Test accuracy: 0.967 \t Loss: 0.580\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.990 \t Test accuracy: 0.967 \t Loss: 0.579\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.991 \t Test accuracy: 0.967 \t Loss: 0.579\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.991 \t Test accuracy: 0.967 \t Loss: 0.579\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.991 \t Test accuracy: 0.967 \t Loss: 0.580\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.992 \t Test accuracy: 0.968 \t Loss: 0.580\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.992 \t Test accuracy: 0.967 \t Loss: 0.578\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.992 \t Test accuracy: 0.967 \t Loss: 0.580\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.993 \t Test accuracy: 0.967 \t Loss: 0.580\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.993 \t Test accuracy: 0.967 \t Loss: 0.581\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.993 \t Test accuracy: 0.968 \t Loss: 0.578\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.993 \t Test accuracy: 0.968 \t Loss: 0.581\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.993 \t Test accuracy: 0.968 \t Loss: 0.582\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.993 \t Test accuracy: 0.967 \t Loss: 0.582\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.993 \t Test accuracy: 0.968 \t Loss: 0.585\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.994 \t Test accuracy: 0.968 \t Loss: 0.583\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.994 \t Test accuracy: 0.968 \t Loss: 0.586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various depths\n",
    "\n",
    "for depth in [6, 7, 8, 9, 10]:\n",
    "    print(\"Using cnn data, depth = %d\" % (depth))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = depth,\n",
    "                            inverse_temperature = 0.1,\n",
    "                            reg_fn = lambda d: 4 ** -d,\n",
    "                            learning_rate = 0.01)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=2)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=2)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, temperature = 0.01\n",
      "Epoch: 0 \t Train accuracy: 0.741 \t Test accuracy: 0.735 \t Loss: 1.763\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.867 \t Test accuracy: 0.867 \t Loss: 1.326\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.900 \t Test accuracy: 0.899 \t Loss: 1.130\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.915 \t Test accuracy: 0.913 \t Loss: 1.022\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.924 \t Test accuracy: 0.924 \t Loss: 0.953\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.931 \t Test accuracy: 0.928 \t Loss: 0.902\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.936 \t Test accuracy: 0.932 \t Loss: 0.864\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.940 \t Test accuracy: 0.935 \t Loss: 0.834\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.943 \t Test accuracy: 0.938 \t Loss: 0.810\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.945 \t Test accuracy: 0.940 \t Loss: 0.790\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.948 \t Test accuracy: 0.942 \t Loss: 0.773\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.949 \t Test accuracy: 0.944 \t Loss: 0.759\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.951 \t Test accuracy: 0.946 \t Loss: 0.746\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.953 \t Test accuracy: 0.946 \t Loss: 0.735\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.954 \t Test accuracy: 0.947 \t Loss: 0.726\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.956 \t Test accuracy: 0.947 \t Loss: 0.717\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.957 \t Test accuracy: 0.948 \t Loss: 0.710\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.958 \t Test accuracy: 0.949 \t Loss: 0.703\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.959 \t Test accuracy: 0.950 \t Loss: 0.696\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.959 \t Test accuracy: 0.950 \t Loss: 0.691\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.961 \t Test accuracy: 0.951 \t Loss: 0.685\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.961 \t Test accuracy: 0.951 \t Loss: 0.681\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.962 \t Test accuracy: 0.952 \t Loss: 0.676\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.963 \t Test accuracy: 0.952 \t Loss: 0.672\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.963 \t Test accuracy: 0.953 \t Loss: 0.668\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.964 \t Test accuracy: 0.953 \t Loss: 0.665\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.964 \t Test accuracy: 0.954 \t Loss: 0.661\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.965 \t Test accuracy: 0.954 \t Loss: 0.659\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.966 \t Test accuracy: 0.955 \t Loss: 0.656\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.966 \t Test accuracy: 0.955 \t Loss: 0.653\n",
      "\n",
      "Using cnn data, temperature = 0.05\n",
      "Epoch: 0 \t Train accuracy: 0.903 \t Test accuracy: 0.905 \t Loss: 1.060\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.934 \t Test accuracy: 0.932 \t Loss: 0.849\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.947 \t Test accuracy: 0.942 \t Loss: 0.770\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.956 \t Test accuracy: 0.948 \t Loss: 0.725\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.953 \t Loss: 0.694\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.964 \t Test accuracy: 0.955 \t Loss: 0.674\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.967 \t Test accuracy: 0.956 \t Loss: 0.660\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.968 \t Test accuracy: 0.957 \t Loss: 0.648\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.970 \t Test accuracy: 0.958 \t Loss: 0.640\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.972 \t Test accuracy: 0.959 \t Loss: 0.633\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.973 \t Test accuracy: 0.959 \t Loss: 0.628\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.960 \t Loss: 0.622\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.960 \t Loss: 0.619\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.976 \t Test accuracy: 0.961 \t Loss: 0.617\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.977 \t Test accuracy: 0.961 \t Loss: 0.614\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.978 \t Test accuracy: 0.962 \t Loss: 0.612\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.978 \t Test accuracy: 0.962 \t Loss: 0.610\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.979 \t Test accuracy: 0.962 \t Loss: 0.609\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.979 \t Test accuracy: 0.962 \t Loss: 0.607\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.980 \t Test accuracy: 0.963 \t Loss: 0.605\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.980 \t Test accuracy: 0.962 \t Loss: 0.606\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.980 \t Test accuracy: 0.962 \t Loss: 0.605\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.981 \t Test accuracy: 0.963 \t Loss: 0.604\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.981 \t Test accuracy: 0.962 \t Loss: 0.604\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.982 \t Test accuracy: 0.962 \t Loss: 0.603\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.982 \t Test accuracy: 0.962 \t Loss: 0.604\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.982 \t Test accuracy: 0.963 \t Loss: 0.603\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.982 \t Test accuracy: 0.962 \t Loss: 0.602\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.983 \t Test accuracy: 0.962 \t Loss: 0.603\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.983 \t Test accuracy: 0.962 \t Loss: 0.604\n",
      "\n",
      "Using cnn data, temperature = 0.10\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.793\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.727\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.691\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.670\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.951 \t Loss: 0.653\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.954 \t Loss: 0.642\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.633\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.629\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.619\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.623\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Using cnn data, temperature = 0.20\n",
      "Epoch: 0 \t Train accuracy: 0.898 \t Test accuracy: 0.895 \t Loss: 0.948\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.931 \t Test accuracy: 0.922 \t Loss: 0.790\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.946 \t Test accuracy: 0.935 \t Loss: 0.729\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.954 \t Test accuracy: 0.940 \t Loss: 0.698\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.959 \t Test accuracy: 0.942 \t Loss: 0.678\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.963 \t Test accuracy: 0.943 \t Loss: 0.665\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.966 \t Test accuracy: 0.945 \t Loss: 0.659\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.968 \t Test accuracy: 0.947 \t Loss: 0.647\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.970 \t Test accuracy: 0.949 \t Loss: 0.646\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.972 \t Test accuracy: 0.950 \t Loss: 0.644\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.973 \t Test accuracy: 0.950 \t Loss: 0.641\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.950 \t Loss: 0.638\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.950 \t Loss: 0.636\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.952 \t Loss: 0.636\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.977 \t Test accuracy: 0.950 \t Loss: 0.635\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.978 \t Test accuracy: 0.953 \t Loss: 0.633\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.979 \t Test accuracy: 0.952 \t Loss: 0.635\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.951 \t Loss: 0.635\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.980 \t Test accuracy: 0.950 \t Loss: 0.636\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.951 \t Loss: 0.637\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.981 \t Test accuracy: 0.951 \t Loss: 0.639\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.951 \t Loss: 0.640\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.951 \t Loss: 0.639\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.952 \t Loss: 0.641\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.951 \t Loss: 0.641\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.951 \t Loss: 0.642\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.951 \t Loss: 0.645\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.983 \t Test accuracy: 0.951 \t Loss: 0.645\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.983 \t Test accuracy: 0.952 \t Loss: 0.648\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.951 \t Loss: 0.647\n",
      "\n",
      "Using cnn data, temperature = 0.40\n",
      "Epoch: 0 \t Train accuracy: 0.867 \t Test accuracy: 0.863 \t Loss: 1.003\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.912 \t Test accuracy: 0.901 \t Loss: 0.822\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.930 \t Test accuracy: 0.915 \t Loss: 0.758\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.939 \t Test accuracy: 0.921 \t Loss: 0.729\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.947 \t Test accuracy: 0.928 \t Loss: 0.706\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.953 \t Test accuracy: 0.935 \t Loss: 0.684\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.957 \t Test accuracy: 0.933 \t Loss: 0.681\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.960 \t Test accuracy: 0.937 \t Loss: 0.674\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.962 \t Test accuracy: 0.939 \t Loss: 0.670\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.964 \t Test accuracy: 0.942 \t Loss: 0.666\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.966 \t Test accuracy: 0.939 \t Loss: 0.667\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.968 \t Test accuracy: 0.941 \t Loss: 0.665\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.968 \t Test accuracy: 0.939 \t Loss: 0.673\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.969 \t Test accuracy: 0.942 \t Loss: 0.664\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.971 \t Test accuracy: 0.943 \t Loss: 0.666\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.971 \t Test accuracy: 0.943 \t Loss: 0.664\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.972 \t Test accuracy: 0.945 \t Loss: 0.663\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.973 \t Test accuracy: 0.944 \t Loss: 0.664\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.973 \t Test accuracy: 0.944 \t Loss: 0.660\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.974 \t Test accuracy: 0.943 \t Loss: 0.667\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.975 \t Test accuracy: 0.942 \t Loss: 0.670\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.975 \t Test accuracy: 0.944 \t Loss: 0.669\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.975 \t Test accuracy: 0.944 \t Loss: 0.668\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.976 \t Test accuracy: 0.945 \t Loss: 0.671\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.976 \t Test accuracy: 0.942 \t Loss: 0.671\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.977 \t Test accuracy: 0.944 \t Loss: 0.667\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.977 \t Test accuracy: 0.945 \t Loss: 0.670\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.977 \t Test accuracy: 0.943 \t Loss: 0.673\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.978 \t Test accuracy: 0.945 \t Loss: 0.665\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.978 \t Test accuracy: 0.945 \t Loss: 0.671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various inverse_temperature\n",
    "\n",
    "for temperature in [0.01, 0.05, 0.1, 0.2, 0.4]:\n",
    "    print(\"Using cnn data, temperature = %.2f\" % (temperature))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            inverse_temperature = temperature,\n",
    "                            reg_fn = lambda d: 4 ** -d,\n",
    "                            learning_rate = 0.01)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=2)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=2)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, strength = 3.0\n",
      "Epoch: 0 \t Train accuracy: 0.914 \t Test accuracy: 0.911 \t Loss: 1.317\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.940 \t Test accuracy: 0.937 \t Loss: 1.140\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 1.067\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.959 \t Test accuracy: 0.949 \t Loss: 1.026\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.963 \t Test accuracy: 0.951 \t Loss: 1.000\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.953 \t Loss: 0.984\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.974\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.971 \t Test accuracy: 0.955 \t Loss: 0.964\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.957 \t Loss: 0.958\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.957\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.957 \t Loss: 0.953\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.957 \t Loss: 0.950\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.948\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.948\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.947\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.948\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.948\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.950\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.947\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.951\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.981 \t Test accuracy: 0.957 \t Loss: 0.949\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.951\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.951\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.951\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.956 \t Loss: 0.952\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.950\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.952\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.952\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.954\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.953\n",
      "\n",
      "Using cnn data, strength = 3.5\n",
      "Epoch: 0 \t Train accuracy: 0.911 \t Test accuracy: 0.910 \t Loss: 1.085\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.915\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.944 \t Loss: 0.845\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.949 \t Loss: 0.808\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.963 \t Test accuracy: 0.952 \t Loss: 0.786\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.954 \t Loss: 0.769\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.955 \t Loss: 0.758\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.971 \t Test accuracy: 0.955 \t Loss: 0.749\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.956 \t Loss: 0.741\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.974 \t Test accuracy: 0.957 \t Loss: 0.741\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.976 \t Test accuracy: 0.957 \t Loss: 0.737\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.977 \t Test accuracy: 0.958 \t Loss: 0.735\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.978 \t Test accuracy: 0.959 \t Loss: 0.732\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.980 \t Test accuracy: 0.959 \t Loss: 0.733\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.980 \t Test accuracy: 0.959 \t Loss: 0.731\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.981 \t Test accuracy: 0.959 \t Loss: 0.733\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.981 \t Test accuracy: 0.959 \t Loss: 0.734\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.734\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.982 \t Test accuracy: 0.959 \t Loss: 0.734\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.983 \t Test accuracy: 0.959 \t Loss: 0.736\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.983 \t Test accuracy: 0.959 \t Loss: 0.737\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.983 \t Test accuracy: 0.960 \t Loss: 0.737\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.739\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.739\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.741\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.742\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.959 \t Loss: 0.742\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.960 \t Loss: 0.743\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.745\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.959 \t Loss: 0.745\n",
      "\n",
      "Using cnn data, strength = 4.0\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.793\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.727\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.691\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.670\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.951 \t Loss: 0.653\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.954 \t Loss: 0.642\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.633\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.629\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.619\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.623\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Using cnn data, strength = 4.5\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.885\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.938 \t Test accuracy: 0.933 \t Loss: 0.717\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.950 \t Test accuracy: 0.943 \t Loss: 0.653\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.957 \t Test accuracy: 0.948 \t Loss: 0.619\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.949 \t Loss: 0.600\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.965 \t Test accuracy: 0.950 \t Loss: 0.583\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.967 \t Test accuracy: 0.951 \t Loss: 0.573\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.564\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.559\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.558\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.553\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.955 \t Loss: 0.552\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.955 \t Loss: 0.550\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.977 \t Test accuracy: 0.955 \t Loss: 0.550\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.978 \t Test accuracy: 0.956 \t Loss: 0.550\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.957 \t Loss: 0.550\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.956 \t Loss: 0.550\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.980 \t Test accuracy: 0.955 \t Loss: 0.552\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.981 \t Test accuracy: 0.955 \t Loss: 0.551\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.551\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.554\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.555\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.983 \t Test accuracy: 0.956 \t Loss: 0.556\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \t Train accuracy: 0.983 \t Test accuracy: 0.955 \t Loss: 0.558\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.983 \t Test accuracy: 0.955 \t Loss: 0.558\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.954 \t Loss: 0.558\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.560\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.560\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.563\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.565\n",
      "\n",
      "Using cnn data, strength = 5.0\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.835\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.938 \t Test accuracy: 0.933 \t Loss: 0.667\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.950 \t Test accuracy: 0.943 \t Loss: 0.604\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.956 \t Test accuracy: 0.947 \t Loss: 0.571\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.551\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.964 \t Test accuracy: 0.950 \t Loss: 0.536\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.966 \t Test accuracy: 0.949 \t Loss: 0.526\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.969 \t Test accuracy: 0.952 \t Loss: 0.517\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.971 \t Test accuracy: 0.953 \t Loss: 0.513\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.972 \t Test accuracy: 0.953 \t Loss: 0.510\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.505\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.954 \t Loss: 0.504\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.975 \t Test accuracy: 0.952 \t Loss: 0.502\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.976 \t Test accuracy: 0.954 \t Loss: 0.502\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.501\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.978 \t Test accuracy: 0.954 \t Loss: 0.501\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.978 \t Test accuracy: 0.953 \t Loss: 0.501\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.979 \t Test accuracy: 0.954 \t Loss: 0.504\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.980 \t Test accuracy: 0.954 \t Loss: 0.503\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.980 \t Test accuracy: 0.954 \t Loss: 0.504\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.981 \t Test accuracy: 0.954 \t Loss: 0.506\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.981 \t Test accuracy: 0.953 \t Loss: 0.507\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.982 \t Test accuracy: 0.954 \t Loss: 0.506\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.982 \t Test accuracy: 0.954 \t Loss: 0.508\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.982 \t Test accuracy: 0.953 \t Loss: 0.509\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.983 \t Test accuracy: 0.953 \t Loss: 0.508\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.983 \t Test accuracy: 0.955 \t Loss: 0.511\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.983 \t Test accuracy: 0.954 \t Loss: 0.511\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.983 \t Test accuracy: 0.953 \t Loss: 0.514\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.983 \t Test accuracy: 0.953 \t Loss: 0.515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various regularization strengths\n",
    "\n",
    "for strength in [3.0, 3.5, 4.0, 4.5, 5.0]:\n",
    "    print(\"Using cnn data, strength = %.1f\" % (strength))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            inverse_temperature = 0.1,\n",
    "                            reg_fn = lambda d: strength ** -d,\n",
    "                            learning_rate = 0.01)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=2)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=2)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cnn data, learning rate = 0.001\n",
      "Epoch: 0 \t Train accuracy: 0.587 \t Test accuracy: 0.594 \t Loss: 2.287\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.709 \t Test accuracy: 0.707 \t Loss: 1.863\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.785 \t Test accuracy: 0.787 \t Loss: 1.592\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.839 \t Test accuracy: 0.841 \t Loss: 1.394\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.872 \t Test accuracy: 0.873 \t Loss: 1.252\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.889 \t Test accuracy: 0.891 \t Loss: 1.150\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.900 \t Test accuracy: 0.901 \t Loss: 1.076\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.908 \t Test accuracy: 0.909 \t Loss: 1.020\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.913 \t Test accuracy: 0.913 \t Loss: 0.977\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.918 \t Test accuracy: 0.917 \t Loss: 0.942\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.922 \t Test accuracy: 0.920 \t Loss: 0.913\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.925 \t Test accuracy: 0.924 \t Loss: 0.889\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.928 \t Test accuracy: 0.927 \t Loss: 0.868\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.931 \t Test accuracy: 0.929 \t Loss: 0.850\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.934 \t Test accuracy: 0.929 \t Loss: 0.834\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.936 \t Test accuracy: 0.932 \t Loss: 0.820\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.807\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.941 \t Test accuracy: 0.936 \t Loss: 0.796\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.943 \t Test accuracy: 0.937 \t Loss: 0.785\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.944 \t Test accuracy: 0.938 \t Loss: 0.776\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.946 \t Test accuracy: 0.939 \t Loss: 0.768\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.947 \t Test accuracy: 0.939 \t Loss: 0.760\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.948 \t Test accuracy: 0.942 \t Loss: 0.753\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.950 \t Test accuracy: 0.942 \t Loss: 0.746\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.950 \t Test accuracy: 0.943 \t Loss: 0.740\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.734\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.953 \t Test accuracy: 0.945 \t Loss: 0.728\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.954 \t Test accuracy: 0.945 \t Loss: 0.723\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.954 \t Test accuracy: 0.946 \t Loss: 0.718\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.955 \t Test accuracy: 0.946 \t Loss: 0.713\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.956 \t Test accuracy: 0.946 \t Loss: 0.709\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.957 \t Test accuracy: 0.947 \t Loss: 0.705\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.958 \t Test accuracy: 0.947 \t Loss: 0.701\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.697\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.959 \t Test accuracy: 0.948 \t Loss: 0.694\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.960 \t Test accuracy: 0.948 \t Loss: 0.690\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.960 \t Test accuracy: 0.948 \t Loss: 0.687\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.684\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.682\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.961 \t Test accuracy: 0.949 \t Loss: 0.679\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.962 \t Test accuracy: 0.949 \t Loss: 0.677\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.674\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.963 \t Test accuracy: 0.950 \t Loss: 0.672\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.963 \t Test accuracy: 0.950 \t Loss: 0.670\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.964 \t Test accuracy: 0.951 \t Loss: 0.668\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.964 \t Test accuracy: 0.951 \t Loss: 0.667\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.965 \t Test accuracy: 0.951 \t Loss: 0.665\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.965 \t Test accuracy: 0.952 \t Loss: 0.663\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.966 \t Test accuracy: 0.952 \t Loss: 0.662\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.966 \t Test accuracy: 0.952 \t Loss: 0.660\n",
      "\n",
      "Epoch: 50 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.659\n",
      "\n",
      "Epoch: 51 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.657\n",
      "\n",
      "Epoch: 52 \t Train accuracy: 0.967 \t Test accuracy: 0.952 \t Loss: 0.656\n",
      "\n",
      "Epoch: 53 \t Train accuracy: 0.968 \t Test accuracy: 0.953 \t Loss: 0.654\n",
      "\n",
      "Epoch: 54 \t Train accuracy: 0.968 \t Test accuracy: 0.953 \t Loss: 0.653\n",
      "\n",
      "Epoch: 55 \t Train accuracy: 0.968 \t Test accuracy: 0.953 \t Loss: 0.652\n",
      "\n",
      "Epoch: 56 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.651\n",
      "\n",
      "Epoch: 57 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.650\n",
      "\n",
      "Epoch: 58 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.649\n",
      "\n",
      "Epoch: 59 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.648\n",
      "\n",
      "Epoch: 60 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.647\n",
      "\n",
      "Epoch: 61 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.646\n",
      "\n",
      "Epoch: 62 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.645\n",
      "\n",
      "Epoch: 63 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.644\n",
      "\n",
      "Epoch: 64 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.644\n",
      "\n",
      "Epoch: 65 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.643\n",
      "\n",
      "Epoch: 66 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.642\n",
      "\n",
      "Epoch: 67 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.641\n",
      "\n",
      "Epoch: 68 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.640\n",
      "\n",
      "Epoch: 69 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.640\n",
      "\n",
      "Epoch: 70 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.639\n",
      "\n",
      "Epoch: 71 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.638\n",
      "\n",
      "Epoch: 72 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.638\n",
      "\n",
      "Epoch: 73 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.637\n",
      "\n",
      "Epoch: 74 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.636\n",
      "\n",
      "Using cnn data, learning rate = 0.005\n",
      "Epoch: 0 \t Train accuracy: 0.858 \t Test accuracy: 0.864 \t Loss: 1.275\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.915 \t Test accuracy: 0.914 \t Loss: 0.951\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.931 \t Test accuracy: 0.927 \t Loss: 0.844\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.941 \t Test accuracy: 0.938 \t Loss: 0.789\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.948 \t Test accuracy: 0.940 \t Loss: 0.752\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.953 \t Test accuracy: 0.946 \t Loss: 0.724\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.956 \t Test accuracy: 0.947 \t Loss: 0.705\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.960 \t Test accuracy: 0.948 \t Loss: 0.689\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.678\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.964 \t Test accuracy: 0.950 \t Loss: 0.669\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.966 \t Test accuracy: 0.952 \t Loss: 0.659\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.968 \t Test accuracy: 0.952 \t Loss: 0.652\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.969 \t Test accuracy: 0.953 \t Loss: 0.648\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.970 \t Test accuracy: 0.953 \t Loss: 0.643\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.971 \t Test accuracy: 0.954 \t Loss: 0.640\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.972 \t Test accuracy: 0.954 \t Loss: 0.636\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.973 \t Test accuracy: 0.955 \t Loss: 0.633\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.974 \t Test accuracy: 0.956 \t Loss: 0.631\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.975 \t Test accuracy: 0.957 \t Loss: 0.629\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.975 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.978 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.623\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.980 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.621\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.622\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.983 \t Test accuracy: 0.959 \t Loss: 0.622\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.624\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.624\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.625\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.626\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.628\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.984 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.628\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.629\n",
      "\n",
      "Epoch: 50 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.630\n",
      "\n",
      "Epoch: 51 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.631\n",
      "\n",
      "Epoch: 52 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.631\n",
      "\n",
      "Epoch: 53 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.632\n",
      "\n",
      "Epoch: 54 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.632\n",
      "\n",
      "Epoch: 55 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.632\n",
      "\n",
      "Epoch: 56 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.634\n",
      "\n",
      "Epoch: 57 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.634\n",
      "\n",
      "Epoch: 58 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.636\n",
      "\n",
      "Epoch: 59 \t Train accuracy: 0.986 \t Test accuracy: 0.954 \t Loss: 0.636\n",
      "\n",
      "Epoch: 60 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.637\n",
      "\n",
      "Epoch: 61 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.637\n",
      "\n",
      "Epoch: 62 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.637\n",
      "\n",
      "Epoch: 63 \t Train accuracy: 0.986 \t Test accuracy: 0.954 \t Loss: 0.638\n",
      "\n",
      "Epoch: 64 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.637\n",
      "\n",
      "Epoch: 65 \t Train accuracy: 0.986 \t Test accuracy: 0.955 \t Loss: 0.641\n",
      "\n",
      "Epoch: 66 \t Train accuracy: 0.986 \t Test accuracy: 0.954 \t Loss: 0.640\n",
      "\n",
      "Epoch: 67 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.641\n",
      "\n",
      "Epoch: 68 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.641\n",
      "\n",
      "Epoch: 69 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.642\n",
      "\n",
      "Epoch: 70 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.643\n",
      "\n",
      "Epoch: 71 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.643\n",
      "\n",
      "Epoch: 72 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.644\n",
      "\n",
      "Epoch: 73 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.645\n",
      "\n",
      "Epoch: 74 \t Train accuracy: 0.987 \t Test accuracy: 0.954 \t Loss: 0.646\n",
      "\n",
      "Using cnn data, learning rate = 0.010\n",
      "Epoch: 0 \t Train accuracy: 0.910 \t Test accuracy: 0.908 \t Loss: 0.960\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.939 \t Test accuracy: 0.935 \t Loss: 0.793\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.951 \t Test accuracy: 0.943 \t Loss: 0.727\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.958 \t Test accuracy: 0.948 \t Loss: 0.691\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.962 \t Test accuracy: 0.950 \t Loss: 0.670\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.966 \t Test accuracy: 0.951 \t Loss: 0.653\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.968 \t Test accuracy: 0.954 \t Loss: 0.642\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.970 \t Test accuracy: 0.954 \t Loss: 0.633\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.972 \t Test accuracy: 0.955 \t Loss: 0.629\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.973 \t Test accuracy: 0.954 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.975 \t Test accuracy: 0.956 \t Loss: 0.623\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.976 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.957 \t Loss: 0.619\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.979 \t Test accuracy: 0.958 \t Loss: 0.617\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.980 \t Test accuracy: 0.957 \t Loss: 0.618\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.982 \t Test accuracy: 0.957 \t Loss: 0.620\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.982 \t Test accuracy: 0.958 \t Loss: 0.619\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.983 \t Test accuracy: 0.957 \t Loss: 0.621\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.983 \t Test accuracy: 0.958 \t Loss: 0.622\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.984 \t Test accuracy: 0.959 \t Loss: 0.623\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.623\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.984 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.625\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.627\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.985 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Using cnn data, learning rate = 0.020\n",
      "Epoch: 0 \t Train accuracy: 0.933 \t Test accuracy: 0.929 \t Loss: 0.808\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.953 \t Test accuracy: 0.945 \t Loss: 0.702\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.962 \t Test accuracy: 0.951 \t Loss: 0.655\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.967 \t Test accuracy: 0.955 \t Loss: 0.633\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.971 \t Test accuracy: 0.956 \t Loss: 0.622\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.973 \t Test accuracy: 0.958 \t Loss: 0.616\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.975 \t Test accuracy: 0.958 \t Loss: 0.610\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.978 \t Test accuracy: 0.958 \t Loss: 0.609\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.979 \t Test accuracy: 0.959 \t Loss: 0.608\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.981 \t Test accuracy: 0.958 \t Loss: 0.612\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.982 \t Test accuracy: 0.959 \t Loss: 0.612\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.982 \t Test accuracy: 0.959 \t Loss: 0.610\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.983 \t Test accuracy: 0.959 \t Loss: 0.611\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.983 \t Test accuracy: 0.959 \t Loss: 0.612\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.984 \t Test accuracy: 0.960 \t Loss: 0.613\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.984 \t Test accuracy: 0.957 \t Loss: 0.617\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.985 \t Test accuracy: 0.957 \t Loss: 0.619\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.985 \t Test accuracy: 0.958 \t Loss: 0.620\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.985 \t Test accuracy: 0.959 \t Loss: 0.619\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.985 \t Test accuracy: 0.959 \t Loss: 0.620\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.986 \t Test accuracy: 0.959 \t Loss: 0.624\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.986 \t Test accuracy: 0.958 \t Loss: 0.628\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.986 \t Test accuracy: 0.958 \t Loss: 0.626\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.986 \t Test accuracy: 0.956 \t Loss: 0.630\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.987 \t Test accuracy: 0.958 \t Loss: 0.629\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.630\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.987 \t Test accuracy: 0.958 \t Loss: 0.631\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.987 \t Test accuracy: 0.957 \t Loss: 0.634\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.987 \t Test accuracy: 0.956 \t Loss: 0.637\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.987 \t Test accuracy: 0.955 \t Loss: 0.642\n",
      "\n",
      "Using cnn data, learning rate = 0.050\n",
      "Epoch: 0 \t Train accuracy: 0.946 \t Test accuracy: 0.936 \t Loss: 0.708\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.960 \t Test accuracy: 0.945 \t Loss: 0.648\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.967 \t Test accuracy: 0.951 \t Loss: 0.633\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.971 \t Test accuracy: 0.953 \t Loss: 0.622\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.973 \t Test accuracy: 0.953 \t Loss: 0.620\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.976 \t Test accuracy: 0.955 \t Loss: 0.620\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.624\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.977 \t Test accuracy: 0.954 \t Loss: 0.624\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.980 \t Test accuracy: 0.954 \t Loss: 0.621\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.981 \t Test accuracy: 0.952 \t Loss: 0.629\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.981 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.982 \t Test accuracy: 0.956 \t Loss: 0.627\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.982 \t Test accuracy: 0.954 \t Loss: 0.628\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.983 \t Test accuracy: 0.956 \t Loss: 0.626\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.983 \t Test accuracy: 0.953 \t Loss: 0.637\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.983 \t Test accuracy: 0.955 \t Loss: 0.638\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.635\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \t Train accuracy: 0.984 \t Test accuracy: 0.955 \t Loss: 0.641\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.985 \t Test accuracy: 0.955 \t Loss: 0.639\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.985 \t Test accuracy: 0.952 \t Loss: 0.650\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.985 \t Test accuracy: 0.953 \t Loss: 0.650\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.985 \t Test accuracy: 0.951 \t Loss: 0.653\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.985 \t Test accuracy: 0.954 \t Loss: 0.646\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.985 \t Test accuracy: 0.952 \t Loss: 0.660\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.986 \t Test accuracy: 0.953 \t Loss: 0.660\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.986 \t Test accuracy: 0.951 \t Loss: 0.656\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.986 \t Test accuracy: 0.954 \t Loss: 0.651\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.986 \t Test accuracy: 0.952 \t Loss: 0.657\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.986 \t Test accuracy: 0.952 \t Loss: 0.655\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.987 \t Test accuracy: 0.953 \t Loss: 0.656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try various learning rates\n",
    "\n",
    "for rate in [0.001, 0.005, 0.01, 0.02, 0.05]:\n",
    "    print(\"Using cnn data, learning rate = %.3f\" % (rate))\n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    tree = SoftDecisionTree(max_depth = 8,\n",
    "                            inverse_temperature = 0.1,\n",
    "                            reg_fn = lambda d: 4 ** -d,\n",
    "                            learning_rate = rate)\n",
    "    \n",
    "    tree.build_graph()\n",
    "    \n",
    "    n_epochs = 75 if rate < 0.01 else 30\n",
    "    for epoch in range(n_epochs):\n",
    "        tree.train(data_train, cnn_probs, batch_size=256)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        prediction = tree.predict(data_train, method=2)\n",
    "        train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        prediction = tree.predict(data_test, method=2)\n",
    "        test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "        print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "              (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.874 \t Test accuracy: 0.869 \t Loss: 1.405\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.920 \t Test accuracy: 0.921 \t Loss: 1.099\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.937 \t Test accuracy: 0.934 \t Loss: 0.978\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.947 \t Test accuracy: 0.943 \t Loss: 0.910\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.954 \t Test accuracy: 0.949 \t Loss: 0.866\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.959 \t Test accuracy: 0.953 \t Loss: 0.833\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.963 \t Test accuracy: 0.956 \t Loss: 0.809\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.966 \t Test accuracy: 0.958 \t Loss: 0.789\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.968 \t Test accuracy: 0.961 \t Loss: 0.774\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.971 \t Test accuracy: 0.961 \t Loss: 0.762\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.973 \t Test accuracy: 0.962 \t Loss: 0.752\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.974 \t Test accuracy: 0.962 \t Loss: 0.743\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.976 \t Test accuracy: 0.963 \t Loss: 0.735\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.964 \t Loss: 0.728\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.979 \t Test accuracy: 0.964 \t Loss: 0.722\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.980 \t Test accuracy: 0.965 \t Loss: 0.717\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.981 \t Test accuracy: 0.964 \t Loss: 0.713\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.982 \t Test accuracy: 0.965 \t Loss: 0.709\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.983 \t Test accuracy: 0.966 \t Loss: 0.706\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.984 \t Test accuracy: 0.966 \t Loss: 0.703\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.984 \t Test accuracy: 0.967 \t Loss: 0.701\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.985 \t Test accuracy: 0.967 \t Loss: 0.699\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.985 \t Test accuracy: 0.969 \t Loss: 0.697\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.986 \t Test accuracy: 0.969 \t Loss: 0.695\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.986 \t Test accuracy: 0.968 \t Loss: 0.694\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.987 \t Test accuracy: 0.969 \t Loss: 0.693\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.987 \t Test accuracy: 0.969 \t Loss: 0.692\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.988 \t Test accuracy: 0.969 \t Loss: 0.691\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.988 \t Test accuracy: 0.969 \t Loss: 0.690\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.988 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.989 \t Test accuracy: 0.969 \t Loss: 0.690\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.989 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.989 \t Test accuracy: 0.970 \t Loss: 0.689\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.990 \t Test accuracy: 0.970 \t Loss: 0.690\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.990 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.990 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.990 \t Test accuracy: 0.970 \t Loss: 0.689\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.990 \t Test accuracy: 0.970 \t Loss: 0.689\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.991 \t Test accuracy: 0.970 \t Loss: 0.689\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.991 \t Test accuracy: 0.970 \t Loss: 0.689\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.991 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.991 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.991 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.992 \t Test accuracy: 0.969 \t Loss: 0.689\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.992 \t Test accuracy: 0.969 \t Loss: 0.690\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.992 \t Test accuracy: 0.969 \t Loss: 0.690\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.690\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.992 \t Test accuracy: 0.969 \t Loss: 0.690\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.992 \t Test accuracy: 0.969 \t Loss: 0.690\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.690\n",
      "\n",
      "Epoch: 50 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.690\n",
      "\n",
      "Epoch: 51 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 52 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 53 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.691\n",
      "\n",
      "Epoch: 54 \t Train accuracy: 0.993 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 55 \t Train accuracy: 0.993 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 56 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.692\n",
      "\n",
      "Epoch: 57 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.693\n",
      "\n",
      "Epoch: 58 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.692\n",
      "\n",
      "Epoch: 59 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.694\n",
      "\n",
      "Epoch: 60 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.694\n",
      "\n",
      "Epoch: 61 \t Train accuracy: 0.993 \t Test accuracy: 0.970 \t Loss: 0.694\n",
      "\n",
      "Epoch: 62 \t Train accuracy: 0.993 \t Test accuracy: 0.971 \t Loss: 0.695\n",
      "\n",
      "Epoch: 63 \t Train accuracy: 0.993 \t Test accuracy: 0.970 \t Loss: 0.695\n",
      "\n",
      "Epoch: 64 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.695\n",
      "\n",
      "Epoch: 65 \t Train accuracy: 0.994 \t Test accuracy: 0.971 \t Loss: 0.695\n",
      "\n",
      "Epoch: 66 \t Train accuracy: 0.994 \t Test accuracy: 0.971 \t Loss: 0.696\n",
      "\n",
      "Epoch: 67 \t Train accuracy: 0.994 \t Test accuracy: 0.971 \t Loss: 0.697\n",
      "\n",
      "Epoch: 68 \t Train accuracy: 0.994 \t Test accuracy: 0.971 \t Loss: 0.697\n",
      "\n",
      "Epoch: 69 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.698\n",
      "\n",
      "Epoch: 70 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.698\n",
      "\n",
      "Epoch: 71 \t Train accuracy: 0.994 \t Test accuracy: 0.971 \t Loss: 0.699\n",
      "\n",
      "Epoch: 72 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.699\n",
      "\n",
      "Epoch: 73 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.699\n",
      "\n",
      "Epoch: 74 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.700\n",
      "\n",
      "Epoch: 75 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.700\n",
      "\n",
      "Epoch: 76 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.701\n",
      "\n",
      "Epoch: 77 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.701\n",
      "\n",
      "Epoch: 78 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.702\n",
      "\n",
      "Epoch: 79 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.702\n",
      "\n",
      "Epoch: 80 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.702\n",
      "\n",
      "Epoch: 81 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.703\n",
      "\n",
      "Epoch: 82 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.703\n",
      "\n",
      "Epoch: 83 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.705\n",
      "\n",
      "Epoch: 84 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.705\n",
      "\n",
      "Epoch: 85 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.705\n",
      "\n",
      "Epoch: 86 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.706\n",
      "\n",
      "Epoch: 87 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.706\n",
      "\n",
      "Epoch: 88 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.708\n",
      "\n",
      "Epoch: 89 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.707\n",
      "\n",
      "Epoch: 90 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.707\n",
      "\n",
      "Epoch: 91 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.707\n",
      "\n",
      "Epoch: 92 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.709\n",
      "\n",
      "Epoch: 93 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.709\n",
      "\n",
      "Epoch: 94 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.709\n",
      "\n",
      "Epoch: 95 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.709\n",
      "\n",
      "Epoch: 96 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.710\n",
      "\n",
      "Epoch: 97 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.710\n",
      "\n",
      "Epoch: 98 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.710\n",
      "\n",
      "Epoch: 99 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 10,\n",
    "                        inverse_temperature = 0.05,\n",
    "                        reg_fn = lambda d: 3.5 ** -d,\n",
    "                        learning_rate = 0.005)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, cnn_probs, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=2)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=2)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train accuracy: 0.874 \t Test accuracy: 0.868 \t Loss: 1.406\n",
      "\n",
      "Epoch: 1 \t Train accuracy: 0.920 \t Test accuracy: 0.920 \t Loss: 1.101\n",
      "\n",
      "Epoch: 2 \t Train accuracy: 0.937 \t Test accuracy: 0.934 \t Loss: 0.979\n",
      "\n",
      "Epoch: 3 \t Train accuracy: 0.947 \t Test accuracy: 0.944 \t Loss: 0.911\n",
      "\n",
      "Epoch: 4 \t Train accuracy: 0.954 \t Test accuracy: 0.949 \t Loss: 0.867\n",
      "\n",
      "Epoch: 5 \t Train accuracy: 0.959 \t Test accuracy: 0.954 \t Loss: 0.835\n",
      "\n",
      "Epoch: 6 \t Train accuracy: 0.963 \t Test accuracy: 0.957 \t Loss: 0.810\n",
      "\n",
      "Epoch: 7 \t Train accuracy: 0.966 \t Test accuracy: 0.958 \t Loss: 0.791\n",
      "\n",
      "Epoch: 8 \t Train accuracy: 0.969 \t Test accuracy: 0.960 \t Loss: 0.775\n",
      "\n",
      "Epoch: 9 \t Train accuracy: 0.971 \t Test accuracy: 0.962 \t Loss: 0.763\n",
      "\n",
      "Epoch: 10 \t Train accuracy: 0.973 \t Test accuracy: 0.962 \t Loss: 0.752\n",
      "\n",
      "Epoch: 11 \t Train accuracy: 0.975 \t Test accuracy: 0.963 \t Loss: 0.744\n",
      "\n",
      "Epoch: 12 \t Train accuracy: 0.977 \t Test accuracy: 0.963 \t Loss: 0.737\n",
      "\n",
      "Epoch: 13 \t Train accuracy: 0.978 \t Test accuracy: 0.963 \t Loss: 0.730\n",
      "\n",
      "Epoch: 14 \t Train accuracy: 0.980 \t Test accuracy: 0.963 \t Loss: 0.724\n",
      "\n",
      "Epoch: 15 \t Train accuracy: 0.981 \t Test accuracy: 0.964 \t Loss: 0.719\n",
      "\n",
      "Epoch: 16 \t Train accuracy: 0.982 \t Test accuracy: 0.965 \t Loss: 0.714\n",
      "\n",
      "Epoch: 17 \t Train accuracy: 0.983 \t Test accuracy: 0.966 \t Loss: 0.710\n",
      "\n",
      "Epoch: 18 \t Train accuracy: 0.984 \t Test accuracy: 0.966 \t Loss: 0.707\n",
      "\n",
      "Epoch: 19 \t Train accuracy: 0.985 \t Test accuracy: 0.968 \t Loss: 0.704\n",
      "\n",
      "Epoch: 20 \t Train accuracy: 0.985 \t Test accuracy: 0.968 \t Loss: 0.702\n",
      "\n",
      "Epoch: 21 \t Train accuracy: 0.986 \t Test accuracy: 0.968 \t Loss: 0.700\n",
      "\n",
      "Epoch: 22 \t Train accuracy: 0.987 \t Test accuracy: 0.968 \t Loss: 0.698\n",
      "\n",
      "Epoch: 23 \t Train accuracy: 0.987 \t Test accuracy: 0.969 \t Loss: 0.696\n",
      "\n",
      "Epoch: 24 \t Train accuracy: 0.988 \t Test accuracy: 0.969 \t Loss: 0.695\n",
      "\n",
      "Epoch: 25 \t Train accuracy: 0.988 \t Test accuracy: 0.969 \t Loss: 0.694\n",
      "\n",
      "Epoch: 26 \t Train accuracy: 0.989 \t Test accuracy: 0.969 \t Loss: 0.693\n",
      "\n",
      "Epoch: 27 \t Train accuracy: 0.989 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 28 \t Train accuracy: 0.990 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 29 \t Train accuracy: 0.990 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 30 \t Train accuracy: 0.990 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 31 \t Train accuracy: 0.991 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 32 \t Train accuracy: 0.991 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 33 \t Train accuracy: 0.991 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 34 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.691\n",
      "\n",
      "Epoch: 35 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 36 \t Train accuracy: 0.992 \t Test accuracy: 0.969 \t Loss: 0.692\n",
      "\n",
      "Epoch: 37 \t Train accuracy: 0.992 \t Test accuracy: 0.970 \t Loss: 0.692\n",
      "\n",
      "Epoch: 38 \t Train accuracy: 0.993 \t Test accuracy: 0.969 \t Loss: 0.693\n",
      "\n",
      "Epoch: 39 \t Train accuracy: 0.993 \t Test accuracy: 0.969 \t Loss: 0.693\n",
      "\n",
      "Epoch: 40 \t Train accuracy: 0.993 \t Test accuracy: 0.969 \t Loss: 0.695\n",
      "\n",
      "Epoch: 41 \t Train accuracy: 0.993 \t Test accuracy: 0.969 \t Loss: 0.695\n",
      "\n",
      "Epoch: 42 \t Train accuracy: 0.993 \t Test accuracy: 0.969 \t Loss: 0.696\n",
      "\n",
      "Epoch: 43 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.697\n",
      "\n",
      "Epoch: 44 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.698\n",
      "\n",
      "Epoch: 45 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.698\n",
      "\n",
      "Epoch: 46 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.698\n",
      "\n",
      "Epoch: 47 \t Train accuracy: 0.994 \t Test accuracy: 0.969 \t Loss: 0.699\n",
      "\n",
      "Epoch: 48 \t Train accuracy: 0.994 \t Test accuracy: 0.970 \t Loss: 0.700\n",
      "\n",
      "Epoch: 49 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.701\n",
      "\n",
      "Epoch: 50 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.702\n",
      "\n",
      "Epoch: 51 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.703\n",
      "\n",
      "Epoch: 52 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.704\n",
      "\n",
      "Epoch: 53 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.704\n",
      "\n",
      "Epoch: 54 \t Train accuracy: 0.995 \t Test accuracy: 0.970 \t Loss: 0.706\n",
      "\n",
      "Epoch: 55 \t Train accuracy: 0.995 \t Test accuracy: 0.970 \t Loss: 0.707\n",
      "\n",
      "Epoch: 56 \t Train accuracy: 0.995 \t Test accuracy: 0.969 \t Loss: 0.708\n",
      "\n",
      "Epoch: 57 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.708\n",
      "\n",
      "Epoch: 58 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.709\n",
      "\n",
      "Epoch: 59 \t Train accuracy: 0.996 \t Test accuracy: 0.969 \t Loss: 0.710\n",
      "\n",
      "Epoch: 60 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.710\n",
      "\n",
      "Epoch: 61 \t Train accuracy: 0.996 \t Test accuracy: 0.969 \t Loss: 0.711\n",
      "\n",
      "Epoch: 62 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.712\n",
      "\n",
      "Epoch: 63 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.713\n",
      "\n",
      "Epoch: 64 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.714\n",
      "\n",
      "Epoch: 65 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.714\n",
      "\n",
      "Epoch: 66 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.715\n",
      "\n",
      "Epoch: 67 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.716\n",
      "\n",
      "Epoch: 68 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.718\n",
      "\n",
      "Epoch: 69 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.718\n",
      "\n",
      "Epoch: 70 \t Train accuracy: 0.996 \t Test accuracy: 0.970 \t Loss: 0.719\n",
      "\n",
      "Epoch: 71 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.720\n",
      "\n",
      "Epoch: 72 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.721\n",
      "\n",
      "Epoch: 73 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.722\n",
      "\n",
      "Epoch: 74 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.723\n",
      "\n",
      "Epoch: 75 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.723\n",
      "\n",
      "Epoch: 76 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.724\n",
      "\n",
      "Epoch: 77 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.725\n",
      "\n",
      "Epoch: 78 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.727\n",
      "\n",
      "Epoch: 79 \t Train accuracy: 0.997 \t Test accuracy: 0.970 \t Loss: 0.727\n",
      "\n",
      "Epoch: 80 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.728\n",
      "\n",
      "Epoch: 81 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.729\n",
      "\n",
      "Epoch: 82 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.730\n",
      "\n",
      "Epoch: 83 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.731\n",
      "\n",
      "Epoch: 84 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.732\n",
      "\n",
      "Epoch: 85 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.732\n",
      "\n",
      "Epoch: 86 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.733\n",
      "\n",
      "Epoch: 87 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.734\n",
      "\n",
      "Epoch: 88 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.736\n",
      "\n",
      "Epoch: 89 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.737\n",
      "\n",
      "Epoch: 90 \t Train accuracy: 0.997 \t Test accuracy: 0.968 \t Loss: 0.738\n",
      "\n",
      "Epoch: 91 \t Train accuracy: 0.997 \t Test accuracy: 0.969 \t Loss: 0.739\n",
      "\n",
      "Epoch: 92 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.741\n",
      "\n",
      "Epoch: 93 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.741\n",
      "\n",
      "Epoch: 94 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.742\n",
      "\n",
      "Epoch: 95 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.743\n",
      "\n",
      "Epoch: 96 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.744\n",
      "\n",
      "Epoch: 97 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.744\n",
      "\n",
      "Epoch: 98 \t Train accuracy: 0.998 \t Test accuracy: 0.968 \t Loss: 0.745\n",
      "\n",
      "Epoch: 99 \t Train accuracy: 0.998 \t Test accuracy: 0.969 \t Loss: 0.746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "tree = SoftDecisionTree(max_depth = 10,\n",
    "                        inverse_temperature = 0.05,\n",
    "                        reg_fn = lambda d: 3.5 ** -d,\n",
    "                        learning_rate = 0.005)\n",
    "\n",
    "tree.build_graph()\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    tree.train(data_train, labels_train_one_hot, batch_size=256)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    prediction = tree.predict(data_train, method=2)\n",
    "    train_accuracy = sum(prediction == labels_train) / len(prediction)\n",
    "\n",
    "    # Compute test accuracy\n",
    "    prediction = tree.predict(data_test, method=2)\n",
    "    test_accuracy = sum(prediction == labels_test) / len(prediction)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = tree.compute_loss(data_test, labels_test_one_hot)\n",
    "\n",
    "    print(\"Epoch: %d \\t Train accuracy: %.3f \\t Test accuracy: %.3f \\t Loss: %.3f\\n\" % \n",
    "          (epoch, train_accuracy, test_accuracy, loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
